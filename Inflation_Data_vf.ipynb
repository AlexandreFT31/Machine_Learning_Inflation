{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R Path - required by rpy2 prior to importing libraries\n",
    "import os\n",
    "os.environ[\"R_HOME\"] = \"D:\\R-4.0.2\"\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy.stats import jarque_bera\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "\n",
    "# Data\n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "# R\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r, DataFrame, FloatVector\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "# Kalman Filter\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# Wavelets\n",
    "import pywt\n",
    "\n",
    "# Machine Learning - tensorflow, keras, and sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, LSTM, ConvLSTM2D, Flatten, BatchNormalization, Lambda \n",
    "from keras.layers import MaxPooling2D, MaxPooling3D, Conv3D, RepeatVector, TimeDistributed\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# SKLearn Models\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller \n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX \n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox \n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.vector_ar.svar_model import SVAR\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "from statsmodels.tsa.vector_ar.vecm import select_coint_rank\n",
    "from statsmodels.tsa.vector_ar.output import VARSummary\n",
    "\n",
    "# Univariate GARCH\n",
    "from arch import arch_model\n",
    "\n",
    "# Plots\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import dates as md\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# Misc\n",
    "import pydot\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Constants and Parameters                                                                                          #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# Folders and worksheet names\n",
    "str_Dir_Plan_FRED = 'D:/Estudos/2 Mestrado/Insper/Dissertação/Dados/'\n",
    "str_Dir_Plan_Data = 'C:/Users/alext/Desktop/Inflation Temp/'\n",
    "str_Dir_Plan_PC = 'D:/Estudos/2 Mestrado/Insper/Dissertação/Dados/PC/'\n",
    "str_Nome_Plan_FRED_MD = 'FRED_MD_2020_04'\n",
    "str_Nome_Plan_FRED_QD = 'FRED_QD_2020_04'\n",
    "str_Nome_Plan_FRED_MD_Desc = 'Data_Description_MD'\n",
    "str_Nome_Plan_FRED_QD_Desc = 'Data_Description_QD'\n",
    "\n",
    "# How to display plots\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.dpi'] = 200 # Plot resolution (dpi)\n",
    "\n",
    "# Required to convert datatypes from Python to R and vice-versa\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Remove warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Color style (plots)\n",
    "sns.set(color_codes = True)\n",
    "\n",
    "# Statistical significance for hypothesis testing\n",
    "# Using 1% due to the high number of tests carried out\n",
    "alfa = 0.01\n",
    "\n",
    "# Test size (share of observations used to build the test sample)\n",
    "share_test_size = 0.20\n",
    "\n",
    "# Validation sample size (share of observations used to build the validation sample)\n",
    "share_validation_size = 0.20\n",
    "\n",
    "# Number of lags considered when splitting the data - see LSTM models\n",
    "n_lags_lstm = 4\n",
    "\n",
    "# Number of lags considered when splitting the data - see ConvLSTM models\n",
    "n_lags_conv = 4\n",
    "\n",
    "# Number of sequences into which sample are broken when fitting ConvLSTM\n",
    "# Note: n_lags = n_seq * n_steps\n",
    "n_seq_conv = 1\n",
    "\n",
    "# Size of each sequence into which sample are broken when fitting ConvLSTM\n",
    "# Note: n_lags = n_seq * n_steps\n",
    "n_steps_conv = int(n_lags_conv / n_seq_conv)\n",
    "\n",
    "# Activation function\n",
    "act_fun = 'selu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Auxiliary Functions                                                                                               #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# Split a univariate sequence into samples\n",
    "def split_sequence_uni(sequence, n_steps, per_ahead, cum = False):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix + per_ahead - 1 > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        if cum == False:\n",
    "            seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + per_ahead - 1]\n",
    "        else:\n",
    "            seq_x, seq_y = sequence[i:end_ix], np.sum(sequence[end_ix:(end_ix + per_ahead)])\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Split a multivariate sequence into samples\n",
    "def split_sequence_mult(sequences, n_steps, per_ahead, cum = False):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix + per_ahead - 1 > len(sequences)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        if cum == False:\n",
    "            seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix + per_ahead - 1, -1]\n",
    "        else:\n",
    "            seq_x, seq_y = sequences[i:end_ix, :-1], np.sum(sequences[end_ix:(end_ix + per_ahead), -1])\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Kalman filter regression\n",
    "# If EM = True, then EM algorithm is used for estimation\n",
    "# delta is related to the variance of the betas. Delta -> 1 makes betas more volatile, which may lead to overfitting.\n",
    "# However, delta -> 0 may increase the MSE.\n",
    "\n",
    "def KFReg(X, y, delta, obs_cov, init_mean, init_cov, EM = False):\n",
    "    n_features = X.shape[1]\n",
    "    obs_mat = X[:, np.newaxis, :]\n",
    "    if EM == False:\n",
    "        trans_cov = (delta/(1 - delta))*np.eye(n_features)\n",
    "        kf = KalmanFilter(n_dim_obs = 1, n_dim_state = n_features, \n",
    "                          initial_state_mean = init_mean,\n",
    "                          initial_state_covariance = init_cov,\n",
    "                          transition_matrices = np.eye(n_features),\n",
    "                          observation_matrices = obs_mat,\n",
    "                          observation_covariance = obs_cov,\n",
    "                          transition_covariance = trans_cov)\n",
    "        state_means, state_covs = kf.filter(y)\n",
    "    else:\n",
    "        kf = KalmanFilter(n_dim_obs = 1, n_dim_state = n_features, \n",
    "                          initial_state_mean = init_mean, \n",
    "                          initial_state_covariance = init_cov,\n",
    "                          observation_matrices = obs_mat)\n",
    "    state_means, state_covs = kf.em(y).filter(y)\n",
    "    return state_means, state_covs, kf\n",
    "\n",
    "# Mean Absolute Error\n",
    "def MAE(y_obs, y_hat):\n",
    "    return np.mean(np.abs(y_obs - y_hat))\n",
    "\n",
    "# Mean Squared Error\n",
    "def MSE(y_obs, y_hat):\n",
    "    return np.mean((y_obs - y_hat)**2)\n",
    "\n",
    "# RMSE\n",
    "def RMSE(y_obs, y_hat):\n",
    "    return np.sqrt(MSE(y_obs, y_hat))\n",
    "\n",
    "def MAPE(y_obs, y_hat):\n",
    "    return np.mean(np.abs(y_obs - y_hat)/y_obs)\n",
    "\n",
    "def cos_sim(y_obs, y_hat):\n",
    "    return np.dot(y_obs, y_hat)/(np.linalg.norm(y_obs)*np.linalg.norm(y_hat))\n",
    "\n",
    "def R2(y_obs, y_hat):\n",
    "    SSR = np.sum((y_obs - y_hat)**2)\n",
    "    SST = np.sum((y_obs - np.mean(y_obs))**2)\n",
    "    return (1 - SSR/SST)\n",
    "\n",
    "# Variational autoencoder\n",
    "# Use those parameters to sample new points from the latent space:\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Variational autoencoder\n",
    "# As in the Keras tutorial, we define a custom loss function:\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = losses.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "# Variational autoencoder\n",
    "def vae(X_train, X_test, intermediate_dim, latent_dim, batch_size, epochs, verbose, plot_name):\n",
    "    \n",
    "    original_dim = X_train.shape[1]\n",
    "    input_shape = (original_dim, )\n",
    "    \n",
    "    # Map inputs to the latent distribution parameters:\n",
    "    # VAE model = encoder + decoder\n",
    "    # build encoder model\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = Dense(intermediate_dim, activation='relu', name='intermediate_encoding')(inputs)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    # z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    z = Lambda(sampling, name='z')([z_mean, z_log_var])\n",
    "\n",
    "    # Instantiate the encoder model:\n",
    "    # encoder = Model(inputs, z_mean)\n",
    "    encoder = Model(inputs=inputs, outputs=[z_mean, z_log_var, z], name='encoder')\n",
    "    #encoder = Model(inputs=inputs, outputs=z, name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    # Build the decoder model:\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(intermediate_dim, activation='relu', name='intermediate_decoding')(latent_inputs)\n",
    "    outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "    # Instantiate the decoder model:\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    decoder.summary()\n",
    "\n",
    "    # Instantiate the VAE model:\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    # outputs = decoder(encoder(inputs))\n",
    "    vae = Model(inputs, outputs, name='vae_mlp')\n",
    "    \n",
    "    # Loss function\n",
    "    reconstruction_loss = losses.binary_crossentropy(inputs, outputs)\n",
    "    reconstruction_loss *= original_dim\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "\n",
    "    # We compile the model:\n",
    "    # vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "    vae.compile(optimizer='rmsprop', loss=None)\n",
    "    \n",
    "    # Finally, we train the model:\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    results = vae.fit(X_train, X_train,\n",
    "            shuffle=True,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1,\n",
    "            verbose=verbose)\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    \n",
    "    '''\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(results.history['loss'])\n",
    "    plt.plot(results.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.savefig(plot_name + '_loss')\n",
    "    plt.close()\n",
    "    '''\n",
    "\n",
    "    # Use the encoded layer to encode the training input\n",
    "    encoded_data = encoder.predict(X_train)[2]\n",
    "    print(encoded_data)\n",
    "    encoded_data = pd.DataFrame(data = encoded_data, \n",
    "                                index = X_train.index, \n",
    "                                columns = [\"PC_vae\" + str(i) for i in np.arange(0, latent_dim)])\n",
    "    \n",
    "    '''\n",
    "    # Plots encoded data\n",
    "    sns.lineplot(data = encoded_data)\n",
    "    plt.savefig(plot_name + '_PC')\n",
    "    plt.close()\n",
    "    '''\n",
    "    \n",
    "    # Correlation matrix\n",
    "    print(encoded_data.corr())\n",
    "    \n",
    "    # Encoded data\n",
    "    X_train_encoded_vae = encoded_data\n",
    "    encoded_data = encoder.predict(X_test)[2]\n",
    "    encoded_data = pd.DataFrame(data = encoded_data, \n",
    "                                index = X_test.index, \n",
    "                                columns = [\"PC_vae\" + str(i) for i in np.arange(0, latent_dim)])\n",
    "    X_test_encoded_vae = encoded_data\n",
    "    \n",
    "    X_train_encoded_vae.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_train.csv')\n",
    "    X_test_encoded_vae.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_test.csv')\n",
    "\n",
    "    return X_train_encoded_vae, X_test_encoded_vae, vae\n",
    "\n",
    "def deep_ae(X_train, X_test, intermediate_dim, latent_dim, batch_size, epochs, verbose, plot_name):\n",
    "    \n",
    "    # Number of time series\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Dimension of encoding units (roughly equivalent to principal components)\n",
    "    encoding_dim1 = intermediate_dim\n",
    "    encoding_dim2 = latent_dim\n",
    "    \n",
    "    # Autoencoder architecture\n",
    "    input_img = Input(shape=(input_dim,), name = 'encoder_input')\n",
    "    encoded_partial = Dense(encoding_dim1, activation = \"selu\", name = 'intermediate_encoding')(input_img)\n",
    "    encoded = Dense(encoding_dim2, activation=\"selu\", name = 'encoding_layer')(encoded_partial)\n",
    "    decoded_partial = Dense(encoding_dim1, activation=\"selu\", name = 'intermediate_decoding')(encoded)\n",
    "    decoded = Dense(input_dim, activation=\"selu\", name = 'decoding_layer')(decoded_partial)\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    print(autoencoder.summary())\n",
    "    \n",
    "    # Fits the autoencoder\n",
    "    hist_autoencoder = autoencoder.fit(X_train, X_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_split=0.1,\n",
    "                verbose=verbose)\n",
    "    \n",
    "    # Use the encoded layer to encode the training input\n",
    "    encoder = Model(input_img, encoded)\n",
    "    encoded_input = Input(shape=(encoding_dim1,))\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "    encoded_data = encoder.predict(X_train)\n",
    "    \n",
    "    '''\n",
    "    # Plots loss function\n",
    "    plt.plot(hist_autoencoder.history['loss'])\n",
    "    plt.plot(hist_autoencoder.history['val_loss'])\n",
    "    plt.title('Model Train vs. Validation Loss (MSE)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.savefig(plot_name + '_loss')\n",
    "    plt.close()\n",
    "    '''\n",
    "    \n",
    "    # Converts encoded data to a labeled dataframe\n",
    "    encoded_data = pd.DataFrame(data = encoded_data, \n",
    "                                index = X_train.index, \n",
    "                                columns = [\"PC_ae\" + str(i) for i in np.arange(0, encoding_dim2)])\n",
    "    \n",
    "    '''\n",
    "    # Plots encoded data\n",
    "    sns.lineplot(data = encoded_data)\n",
    "    plt.savefig(plot_name + '_PC')\n",
    "    plt.close()\n",
    "    '''\n",
    "\n",
    "    # Correlation matrix\n",
    "    encoded_data.corr()\n",
    "\n",
    "    # Stores the encoded data\n",
    "    X_train_encoded_ae = encoded_data\n",
    "    encoded_data = encoder.predict(X_test)\n",
    "    encoded_data = pd.DataFrame(data = encoded_data, \n",
    "                                index = X_test.index, \n",
    "                                columns = [\"PC_ae\" + str(i) for i in np.arange(0, encoding_dim2)])\n",
    "    X_test_encoded_ae = encoded_data\n",
    "    \n",
    "    X_train_encoded_ae.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_train.csv')\n",
    "    X_test_encoded_ae.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_test.csv')\n",
    "\n",
    "    return X_train_encoded_ae, X_test_encoded_ae\n",
    "\n",
    "def pca_decomp(X_train, X_test, threshold, plot_name):\n",
    "    \n",
    "    # Runs PCA for the maximum number of components possible\n",
    "    n_series = X_train.shape[1]\n",
    "    pca = PCA(n_components = n_series, svd_solver = 'full')\n",
    "    pca.fit(X_train)\n",
    "    \n",
    "    # Selects the number of PCs required for explained variance > threshold\n",
    "    total_var = 0\n",
    "    n_comp = 0\n",
    "    for var in pca.explained_variance_ratio_:\n",
    "        total_var = var + total_var\n",
    "        n_comp = n_comp + 1\n",
    "        if total_var > threshold:\n",
    "            break\n",
    "    \n",
    "    # Runs PCA for the number of components selected\n",
    "    pca = PCA(n_components = n_comp, svd_solver = 'full')\n",
    "    pca.fit(X_train)\n",
    "    \n",
    "    # Applies transformation to training data\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_train_pca = X_train_pca.reshape(X_train_pca.shape[0], n_comp)\n",
    "    X_train_pca = pd.DataFrame(data = X_train_pca, \n",
    "                           index = X_train.index, \n",
    "                           columns = [\"PC\" + str(i) for i in np.arange(0,n_comp)])\n",
    "    \n",
    "    # Applies transformation to test data\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    X_test_pca = X_test_pca.reshape(X_test_pca.shape[0], n_comp)\n",
    "    X_test_pca = pd.DataFrame(data = X_test_pca, \n",
    "                           index = X_test.index, \n",
    "                           columns = [\"PC\" + str(i) for i in np.arange(0,n_comp)])\n",
    "    \n",
    "    '''\n",
    "    # Plots training data\n",
    "    sns.lineplot(data = X_train_pca)\n",
    "    plt.savefig(plot_name + '_PC')\n",
    "    plt.close()\n",
    "    '''\n",
    "\n",
    "    # Correlation matrix\n",
    "    X_train_pca.corr()\n",
    "    \n",
    "    # Saves results\n",
    "    X_train_pca.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_train.csv')\n",
    "    X_test_pca.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_test.csv')\n",
    "    \n",
    "    return X_train_pca, X_test_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Data                                                                                                              #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "usrec = DataReader('USREC', 'fred', start=datetime(1947, 1, 1), end=datetime(2019, 4, 1))\n",
    "\n",
    "df_FRED_MD = pd.read_csv(filepath_or_buffer = str_Dir_Plan_FRED + str_Nome_Plan_FRED_MD + '.csv', sep = ';')\n",
    "df_FRED_MD.index = pd.to_datetime(df_FRED_MD.iloc[:,0])\n",
    "df_FRED_MD = df_FRED_MD.drop(columns = 'Date')\n",
    "df_FRED_MD.head()\n",
    "\n",
    "df_FRED_MD.tail()\n",
    "\n",
    "df_FRED_QD = pd.read_csv(filepath_or_buffer = str_Dir_Plan_FRED + str_Nome_Plan_FRED_QD + '.csv', sep = ';')\n",
    "df_FRED_QD.index = pd.to_datetime(df_FRED_QD.iloc[:,0])\n",
    "df_FRED_QD = df_FRED_QD.drop(columns = 'Date')\n",
    "df_FRED_QD.head()\n",
    "\n",
    "df_FRED_QD.tail()\n",
    "\n",
    "df_FRED_Desc_MD = pd.read_csv(filepath_or_buffer = str_Dir_Plan_FRED + str_Nome_Plan_FRED_MD_Desc + '.csv', sep = ';')\n",
    "df_FRED_Desc_MD = df_FRED_Desc_MD.drop(columns = 'Index')\n",
    "df_FRED_Desc_MD.head()\n",
    "\n",
    "df_FRED_Desc_MD.tail()\n",
    "\n",
    "df_FRED_Desc_QD = pd.read_csv(filepath_or_buffer = str_Dir_Plan_FRED + str_Nome_Plan_FRED_QD_Desc + '.csv', sep = ';')\n",
    "df_FRED_Desc_QD = df_FRED_Desc_QD.drop(columns = 'Index')\n",
    "df_FRED_Desc_QD.head()\n",
    "\n",
    "df_FRED_Desc_QD.tail()\n",
    "\n",
    "df_FRED_MD.describe()\n",
    "df_FRED_QD.describe()\n",
    "\n",
    "# Data transformation according to McCraken and Ng (2016)\n",
    "# Monthly database\n",
    "\n",
    "qty_series = df_FRED_MD.shape[1]\n",
    "df_FRED_MD_t = df_FRED_MD.copy()\n",
    "\n",
    "for i in range(0,qty_series):\n",
    "    str_transf = df_FRED_Desc_MD.iloc[i,2]\n",
    "    str_ticker = df_FRED_Desc_MD.iloc[i,3]\n",
    "    col_ticker = np.where(df_FRED_MD_t.columns == str_ticker)\n",
    "    if len(col_ticker[0]) > 0:\n",
    "        col_ticker = col_ticker[0][0]\n",
    "        df_series = df_FRED_MD_t.iloc[:,col_ticker]\n",
    "        if str_transf == \"First difference of log\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = np.log(df_series).diff()\n",
    "        elif str_transf == \"First difference\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = df_series.diff()\n",
    "        elif str_transf == \"Log\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = np.log(df_series)\n",
    "        elif str_transf == \"Second difference of log\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = np.log(df_series).diff().diff()\n",
    "        elif str_transf == \"Second difference\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = df_series.diff().diff()\n",
    "        elif str_transf == \"First difference of (ratio - 1)\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = df_series.pct_change().diff()\n",
    "\n",
    "df_FRED_MD_t = df_FRED_MD_t.iloc[2:,:] # Removes the first 2 rows due to differencing\n",
    "df_FRED_MD_t.head()\n",
    "\n",
    "# Normalization (mean = 0 and std = 1)\n",
    "df_FRED_MD_t_norm = pd.DataFrame(data = scale(df_FRED_MD_t), \n",
    "                                 index = df_FRED_MD_t.index, \n",
    "                                 columns = df_FRED_MD_t.columns)\n",
    "df_FRED_MD_t_norm.head()\n",
    "\n",
    "# Data preparation using normalized data\n",
    "\n",
    "# Exclude nan from the transformed time series\n",
    "df_FRED_MD_t_norm_ex_nan = df_FRED_MD_t_norm.dropna()\n",
    "\n",
    "# Dataframe containing inflation time series only\n",
    "# CPIAUCSL - Consumer Price Index for All Urban Consumers: All Items\n",
    "df_cpi_t_norm = df_FRED_MD_t_norm_ex_nan[\"CPIAUCSL\"]\n",
    "\n",
    "# Dataframe excluding inflation time series\n",
    "df_FRED_MD_t_norm_ex_nan_inf = df_FRED_MD_t_norm_ex_nan.drop(columns = [\"CPIAUCSL\",\"CPIAPPSL\",\"CPITRNSL\",\n",
    "                                        \"CPIMEDSL\",\"CUSR0000SAC\",\"CUSR0000SAD\",\"CUSR0000SAS\",\"CPIULFSL\"])\n",
    "\n",
    "# Dataframes containing shifted time series\n",
    "df_FRED_MD_t_norm_ex_nan_L1 = df_FRED_MD_t_norm_ex_nan.shift(1).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L2 = df_FRED_MD_t_norm_ex_nan.shift(2).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L3 = df_FRED_MD_t_norm_ex_nan.shift(3).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L4 = df_FRED_MD_t_norm_ex_nan.shift(4).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L5 = df_FRED_MD_t_norm_ex_nan.shift(5).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L6 = df_FRED_MD_t_norm_ex_nan.shift(6).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L7 = df_FRED_MD_t_norm_ex_nan.shift(7).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L8 = df_FRED_MD_t_norm_ex_nan.shift(8).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L9 = df_FRED_MD_t_norm_ex_nan.shift(9).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L10 = df_FRED_MD_t_norm_ex_nan.shift(10).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L11 = df_FRED_MD_t_norm_ex_nan.shift(11).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_L12 = df_FRED_MD_t_norm_ex_nan.shift(12).dropna()\n",
    "\n",
    "# Dataframes containing shifted time series - excluding inflation time series\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L1 = df_FRED_MD_t_norm_ex_nan_inf.shift(1).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L2 = df_FRED_MD_t_norm_ex_nan_inf.shift(2).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L3 = df_FRED_MD_t_norm_ex_nan_inf.shift(3).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L4 = df_FRED_MD_t_norm_ex_nan_inf.shift(4).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L5 = df_FRED_MD_t_norm_ex_nan_inf.shift(5).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L6 = df_FRED_MD_t_norm_ex_nan_inf.shift(6).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L7 = df_FRED_MD_t_norm_ex_nan_inf.shift(7).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L8 = df_FRED_MD_t_norm_ex_nan_inf.shift(8).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L9 = df_FRED_MD_t_norm_ex_nan_inf.shift(9).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L10 = df_FRED_MD_t_norm_ex_nan_inf.shift(10).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L11 = df_FRED_MD_t_norm_ex_nan_inf.shift(11).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_inf_L12 = df_FRED_MD_t_norm_ex_nan_inf.shift(12).dropna()\n",
    "\n",
    "# Grouping series according to their classification\n",
    "\n",
    "# Output and income\n",
    "df_FRED_MD_t_norm_ex_nan_OI = pd.DataFrame()\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L1 = pd.DataFrame()\n",
    "\n",
    "# Labor market\n",
    "df_FRED_MD_t_norm_ex_nan_LM = pd.DataFrame()\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L1 = pd.DataFrame()\n",
    "\n",
    "# Housing\n",
    "df_FRED_MD_t_norm_ex_nan_H = pd.DataFrame()\n",
    "df_FRED_MD_t_norm_ex_nan_H_L1 = pd.DataFrame()\n",
    "\n",
    "# Consumption, Orders, and Inventories\n",
    "df_FRED_MD_t_norm_ex_nan_COI = pd.DataFrame()\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L1 = pd.DataFrame()\n",
    "\n",
    "# Money and Credit\n",
    "df_FRED_MD_t_norm_ex_nan_MC = pd.DataFrame()\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L1 = pd.DataFrame()\n",
    "\n",
    "# Interest and Exchange Rates\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX = pd.DataFrame()\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L1 = pd.DataFrame()\n",
    "\n",
    "# Prices\n",
    "df_FRED_MD_t_norm_ex_nan_P = pd.DataFrame()\n",
    "df_FRED_MD_t_norm_ex_nan_P_L1 = pd.DataFrame()\n",
    "\n",
    "# Stock Market\n",
    "df_FRED_MD_t_norm_ex_nan_S = pd.DataFrame()\n",
    "df_FRED_MD_t_norm_ex_nan_S_L1 = pd.DataFrame()\n",
    "\n",
    "for i in range(0,qty_series):\n",
    "    str_group = df_FRED_Desc_MD.iloc[i,0]\n",
    "    str_ticker = df_FRED_Desc_MD.iloc[i,3]\n",
    "    col_ticker = np.where(df_FRED_MD_t_norm_ex_nan.columns == str_ticker)\n",
    "    if len(col_ticker[0]) > 0:\n",
    "        col_ticker = col_ticker[0][0]\n",
    "        df_series = df_FRED_MD_t_norm_ex_nan.iloc[:,col_ticker]\n",
    "        if str_group == \"Output and Income\":\n",
    "            df_FRED_MD_t_norm_ex_nan_OI = pd.concat([df_FRED_MD_t_norm_ex_nan_OI, df_series], axis = 1)\n",
    "            #df_FRED_MD_t_norm_ex_nan_OI_L1 = pd.concat([df_FRED_MD_t_norm_ex_nan_OI_L1, \n",
    "            #                                           df_series.shift(1).dropna()], axis = 1)\n",
    "        elif str_group == \"Labor Market\":\n",
    "            df_FRED_MD_t_norm_ex_nan_LM = pd.concat([df_FRED_MD_t_norm_ex_nan_LM, df_series], axis = 1)\n",
    "            #df_FRED_MD_t_norm_ex_nan_LM_L1 = pd.concat([df_FRED_MD_t_norm_ex_nan_LM_L1, \n",
    "            #                                            df_series.shift(1).dropna()], axis = 1)\n",
    "        elif str_group == \"Housing\":\n",
    "            df_FRED_MD_t_norm_ex_nan_H = pd.concat([df_FRED_MD_t_norm_ex_nan_H, df_series], axis = 1)\n",
    "            #df_FRED_MD_t_norm_ex_nan_H_L1 = pd.concat([df_FRED_MD_t_norm_ex_nan_H_L1, \n",
    "            #                                            df_series.shift(1).dropna()], axis = 1)\n",
    "        elif str_group == \"Consumption, Orders and Inventories\":\n",
    "            df_FRED_MD_t_norm_ex_nan_COI = pd.concat([df_FRED_MD_t_norm_ex_nan_COI, df_series], axis = 1)\n",
    "            #df_FRED_MD_t_norm_ex_nan_COI_L1 = pd.concat([df_FRED_MD_t_norm_ex_nan_COI_L1, \n",
    "            #                                            df_series.shift(1).dropna()], axis = 1)\n",
    "        elif str_group == \"Money and Credit\":\n",
    "            df_FRED_MD_t_norm_ex_nan_MC = pd.concat([df_FRED_MD_t_norm_ex_nan_MC, df_series], axis = 1)\n",
    "            #df_FRED_MD_t_norm_ex_nan_MC_L1 = pd.concat([df_FRED_MD_t_norm_ex_nan_MC_L1, \n",
    "            #                                            df_series.shift(1).dropna()], axis = 1)\n",
    "        elif str_group == \"Interest and Exchange Rates\":\n",
    "            df_FRED_MD_t_norm_ex_nan_INTFX = pd.concat([df_FRED_MD_t_norm_ex_nan_INTFX, df_series], axis = 1)\n",
    "            #df_FRED_MD_t_norm_ex_nan_INTFX_L1 = pd.concat([df_FRED_MD_t_norm_ex_nan_INTFX_L1, \n",
    "            #                                            df_series.shift(1).dropna()], axis = 1)\n",
    "        elif str_group == \"Prices\":\n",
    "            df_FRED_MD_t_norm_ex_nan_P = pd.concat([df_FRED_MD_t_norm_ex_nan_P, df_series], axis = 1)\n",
    "            #df_FRED_MD_t_norm_ex_nan_P_L1 = pd.concat([df_FRED_MD_t_norm_ex_nan_P_L1, \n",
    "            #                                            df_series.shift(1).dropna()], axis = 1)\n",
    "        elif str_group == \"Stock Market\":\n",
    "            df_FRED_MD_t_norm_ex_nan_S = pd.concat([df_FRED_MD_t_norm_ex_nan_S, df_series], axis = 1)\n",
    "            #df_FRED_MD_t_norm_ex_nan_S_L1 = pd.concat([df_FRED_MD_t_norm_ex_nan_S_L1, \n",
    "            #                                            df_series.shift(1).dropna()], axis = 1)\n",
    "\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L1 = df_FRED_MD_t_norm_ex_nan_OI.shift(1).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L1 = df_FRED_MD_t_norm_ex_nan_LM.shift(1).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_H_L1 = df_FRED_MD_t_norm_ex_nan_H.shift(1).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L1 = df_FRED_MD_t_norm_ex_nan_COI.shift(1).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L1 = df_FRED_MD_t_norm_ex_nan_MC.shift(1).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L1 = df_FRED_MD_t_norm_ex_nan_INTFX.shift(1).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_P_L1 = df_FRED_MD_t_norm_ex_nan_P.shift(1).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_S_L1 = df_FRED_MD_t_norm_ex_nan_S.shift(1).dropna()\n",
    "\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L2 = df_FRED_MD_t_norm_ex_nan_OI.shift(2).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L2 = df_FRED_MD_t_norm_ex_nan_LM.shift(2).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_H_L2 = df_FRED_MD_t_norm_ex_nan_H.shift(2).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L2 = df_FRED_MD_t_norm_ex_nan_COI.shift(2).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L2 = df_FRED_MD_t_norm_ex_nan_MC.shift(2).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L2 = df_FRED_MD_t_norm_ex_nan_INTFX.shift(2).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_P_L2 = df_FRED_MD_t_norm_ex_nan_P.shift(2).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_S_L2 = df_FRED_MD_t_norm_ex_nan_S.shift(2).dropna()\n",
    "\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L3 = df_FRED_MD_t_norm_ex_nan_OI.shift(3).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L3 = df_FRED_MD_t_norm_ex_nan_LM.shift(3).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_H_L3 = df_FRED_MD_t_norm_ex_nan_H.shift(3).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L3 = df_FRED_MD_t_norm_ex_nan_COI.shift(3).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L3 = df_FRED_MD_t_norm_ex_nan_MC.shift(3).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L3 = df_FRED_MD_t_norm_ex_nan_INTFX.shift(3).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_P_L3 = df_FRED_MD_t_norm_ex_nan_P.shift(3).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_S_L3 = df_FRED_MD_t_norm_ex_nan_S.shift(3).dropna()\n",
    "\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L4 = df_FRED_MD_t_norm_ex_nan_OI.shift(4).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L4 = df_FRED_MD_t_norm_ex_nan_LM.shift(4).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_H_L4 = df_FRED_MD_t_norm_ex_nan_H.shift(4).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L4 = df_FRED_MD_t_norm_ex_nan_COI.shift(4).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L4 = df_FRED_MD_t_norm_ex_nan_MC.shift(4).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L4 = df_FRED_MD_t_norm_ex_nan_INTFX.shift(4).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_P_L4 = df_FRED_MD_t_norm_ex_nan_P.shift(4).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_S_L4 = df_FRED_MD_t_norm_ex_nan_S.shift(4).dropna()\n",
    "\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L12 = df_FRED_MD_t_norm_ex_nan_OI.shift(12).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L12 = df_FRED_MD_t_norm_ex_nan_LM.shift(12).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_H_L12 = df_FRED_MD_t_norm_ex_nan_H.shift(12).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L12 = df_FRED_MD_t_norm_ex_nan_COI.shift(12).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L12 = df_FRED_MD_t_norm_ex_nan_MC.shift(12).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L12 = df_FRED_MD_t_norm_ex_nan_INTFX.shift(12).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_P_L12 = df_FRED_MD_t_norm_ex_nan_P.shift(12).dropna()\n",
    "df_FRED_MD_t_norm_ex_nan_S_L12 = df_FRED_MD_t_norm_ex_nan_S.shift(12).dropna()\n",
    "\n",
    "# Index adjustment\n",
    "index_ref = df_FRED_MD_t_norm_ex_nan_L12.index\n",
    "\n",
    "# index_refnflatindex_refon\n",
    "df_cpi_t_norm = df_cpi_t_norm.loc[index_ref]\n",
    "\n",
    "# Full database\n",
    "df_FRED_MD_t_norm_ex_nan = df_FRED_MD_t_norm_ex_nan.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L1 = df_FRED_MD_t_norm_ex_nan_L1.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L2 = df_FRED_MD_t_norm_ex_nan_L2.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L3 = df_FRED_MD_t_norm_ex_nan_L3.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L4 = df_FRED_MD_t_norm_ex_nan_L4.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L5 = df_FRED_MD_t_norm_ex_nan_L5.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L6 = df_FRED_MD_t_norm_ex_nan_L6.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L7 = df_FRED_MD_t_norm_ex_nan_L7.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L8 = df_FRED_MD_t_norm_ex_nan_L8.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L9 = df_FRED_MD_t_norm_ex_nan_L9.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L10 = df_FRED_MD_t_norm_ex_nan_L10.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L11 = df_FRED_MD_t_norm_ex_nan_L11.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_L12 = df_FRED_MD_t_norm_ex_nan_L12.loc[index_ref]\n",
    "\n",
    "# Output and index_refncome\n",
    "df_FRED_MD_t_norm_ex_nan_OI = df_FRED_MD_t_norm_ex_nan_OI.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L1 = df_FRED_MD_t_norm_ex_nan_OI_L1.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L2 = df_FRED_MD_t_norm_ex_nan_OI_L2.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L3 = df_FRED_MD_t_norm_ex_nan_OI_L3.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L4 = df_FRED_MD_t_norm_ex_nan_OI_L4.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_OI_L12 = df_FRED_MD_t_norm_ex_nan_OI_L12.loc[index_ref]\n",
    "\n",
    "# Labor market\n",
    "df_FRED_MD_t_norm_ex_nan_LM = df_FRED_MD_t_norm_ex_nan_LM.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L1 = df_FRED_MD_t_norm_ex_nan_LM_L1.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L2 = df_FRED_MD_t_norm_ex_nan_LM_L2.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L3 = df_FRED_MD_t_norm_ex_nan_LM_L3.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L4 = df_FRED_MD_t_norm_ex_nan_LM_L4.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_LM_L12 = df_FRED_MD_t_norm_ex_nan_LM_L12.loc[index_ref]\n",
    "\n",
    "# Housindex_refng\n",
    "df_FRED_MD_t_norm_ex_nan_H = df_FRED_MD_t_norm_ex_nan_H.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_H_L1 = df_FRED_MD_t_norm_ex_nan_H_L1.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_H_L2 = df_FRED_MD_t_norm_ex_nan_H_L2.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_H_L3 = df_FRED_MD_t_norm_ex_nan_H_L3.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_H_L4 = df_FRED_MD_t_norm_ex_nan_H_L4.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_H_L12 = df_FRED_MD_t_norm_ex_nan_H_L12.loc[index_ref]\n",
    "\n",
    "# Consumptindex_refon, Orders, and index_refnventorindex_refes\n",
    "df_FRED_MD_t_norm_ex_nan_COI = df_FRED_MD_t_norm_ex_nan_COI.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L1 = df_FRED_MD_t_norm_ex_nan_COI_L1.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L2 = df_FRED_MD_t_norm_ex_nan_COI_L2.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L3 = df_FRED_MD_t_norm_ex_nan_COI_L3.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L4 = df_FRED_MD_t_norm_ex_nan_COI_L4.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_COI_L12 = df_FRED_MD_t_norm_ex_nan_COI_L12.loc[index_ref]\n",
    "\n",
    "# Money and Credindex_reft\n",
    "df_FRED_MD_t_norm_ex_nan_MC = df_FRED_MD_t_norm_ex_nan_MC.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L1 = df_FRED_MD_t_norm_ex_nan_MC_L1.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L2 = df_FRED_MD_t_norm_ex_nan_MC_L2.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L3 = df_FRED_MD_t_norm_ex_nan_MC_L3.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L4 = df_FRED_MD_t_norm_ex_nan_MC_L4.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_MC_L12 = df_FRED_MD_t_norm_ex_nan_MC_L12.loc[index_ref]\n",
    "\n",
    "# index_refnterest and Exchange Rates\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX = df_FRED_MD_t_norm_ex_nan_INTFX.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L1 = df_FRED_MD_t_norm_ex_nan_INTFX_L1.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L2 = df_FRED_MD_t_norm_ex_nan_INTFX_L2.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L3 = df_FRED_MD_t_norm_ex_nan_INTFX_L3.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L4 = df_FRED_MD_t_norm_ex_nan_INTFX_L4.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_INTFX_L12 = df_FRED_MD_t_norm_ex_nan_INTFX_L12.loc[index_ref]\n",
    "\n",
    "# Prindex_refces\n",
    "df_FRED_MD_t_norm_ex_nan_P = df_FRED_MD_t_norm_ex_nan_P.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_P_L1 = df_FRED_MD_t_norm_ex_nan_P_L1.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_P_L2 = df_FRED_MD_t_norm_ex_nan_P_L2.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_P_L3 = df_FRED_MD_t_norm_ex_nan_P_L3.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_P_L4 = df_FRED_MD_t_norm_ex_nan_P_L4.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_P_L12 = df_FRED_MD_t_norm_ex_nan_P_L12.loc[index_ref]\n",
    "\n",
    "# Stock Market\n",
    "df_FRED_MD_t_norm_ex_nan_S = df_FRED_MD_t_norm_ex_nan_S.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_S_L1 = df_FRED_MD_t_norm_ex_nan_S_L1.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_S_L2 = df_FRED_MD_t_norm_ex_nan_S_L2.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_S_L3 = df_FRED_MD_t_norm_ex_nan_S_L3.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_S_L4 = df_FRED_MD_t_norm_ex_nan_S_L4.loc[index_ref]\n",
    "df_FRED_MD_t_norm_ex_nan_S_L12 = df_FRED_MD_t_norm_ex_nan_S_L12.loc[index_ref]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Training and Test Samples                                                                                         #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# Complete database\n",
    "\n",
    "X = df_FRED_MD_t_norm_ex_nan\n",
    "X_L1 = df_FRED_MD_t_norm_ex_nan_L1\n",
    "X_L2 = df_FRED_MD_t_norm_ex_nan_L2\n",
    "X_L3 = df_FRED_MD_t_norm_ex_nan_L3\n",
    "X_L4 = df_FRED_MD_t_norm_ex_nan_L4\n",
    "X_L5 = df_FRED_MD_t_norm_ex_nan_L5\n",
    "X_L6 = df_FRED_MD_t_norm_ex_nan_L6\n",
    "X_L7 = df_FRED_MD_t_norm_ex_nan_L7\n",
    "X_L8 = df_FRED_MD_t_norm_ex_nan_L8\n",
    "X_L9 = df_FRED_MD_t_norm_ex_nan_L9\n",
    "X_L10 = df_FRED_MD_t_norm_ex_nan_L10\n",
    "X_L11 = df_FRED_MD_t_norm_ex_nan_L11\n",
    "X_L12 = df_FRED_MD_t_norm_ex_nan_L12\n",
    "y = df_cpi_t_norm\n",
    "\n",
    "# Indices for splitting samples\n",
    "\n",
    "for rnd_state in range(0,100):\n",
    "    \n",
    "    index_train, index_test = train_test_split(index_ref, test_size = share_test_size, random_state = rnd_state)\n",
    "    \n",
    "    # Save raw database\n",
    "\n",
    "    X.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X.csv')\n",
    "    X_L1.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L1.csv')\n",
    "    X_L2.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L2.csv')\n",
    "    X_L3.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L3.csv')\n",
    "    X_L4.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L4.csv')\n",
    "    X_L5.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L5.csv')\n",
    "    X_L6.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L6.csv')\n",
    "    X_L7.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L7.csv')\n",
    "    X_L8.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L8.csv')\n",
    "    X_L9.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L9.csv')\n",
    "    X_L10.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L10.csv')\n",
    "    X_L11.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L11.csv')\n",
    "    X_L12.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L12.csv')\n",
    "    y.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'y.csv')\n",
    "    \n",
    "    # Split samples\n",
    "    \n",
    "    y_train, y_test = y.loc[index_train], y.loc[index_test]\n",
    "    X_train, X_test = X.loc[index_train], X.loc[index_test]\n",
    "    X_L1_train, X_L1_test = X_L1.loc[index_train], X_L1.loc[index_test]\n",
    "    X_L2_train, X_L2_test = X_L2.loc[index_train], X_L2.loc[index_test]\n",
    "    X_L3_train, X_L3_test = X_L3.loc[index_train], X_L3.loc[index_test]\n",
    "    X_L4_train, X_L4_test = X_L4.loc[index_train], X_L4.loc[index_test]\n",
    "    X_L5_train, X_L5_test = X_L5.loc[index_train], X_L5.loc[index_test]\n",
    "    X_L6_train, X_L6_test = X_L6.loc[index_train], X_L6.loc[index_test]\n",
    "    X_L7_train, X_L7_test = X_L7.loc[index_train], X_L7.loc[index_test]\n",
    "    X_L8_train, X_L8_test = X_L8.loc[index_train], X_L8.loc[index_test]\n",
    "    X_L9_train, X_L9_test = X_L9.loc[index_train], X_L9.loc[index_test]\n",
    "    X_L10_train, X_L10_test = X_L10.loc[index_train], X_L10.loc[index_test]\n",
    "    X_L11_train, X_L11_test = X_L11.loc[index_train], X_L11.loc[index_test]\n",
    "    X_L12_train, X_L12_test = X_L12.loc[index_train], X_L12.loc[index_test]\n",
    "       \n",
    "    # Normalization\n",
    "    \n",
    "    y_train = pd.DataFrame(data = scale(y_train), index = y_train.index)\n",
    "    y_test = pd.DataFrame(data = scale(y_test), index = y_test.index)\n",
    "    \n",
    "    X_L1_train = pd.DataFrame(data = scale(X_L1_train), index = X_L1_train.index, columns = X_L1_train.columns)\n",
    "    X_L2_train = pd.DataFrame(data = scale(X_L2_train), index = X_L2_train.index, columns = X_L2_train.columns)\n",
    "    X_L3_train = pd.DataFrame(data = scale(X_L3_train), index = X_L3_train.index, columns = X_L3_train.columns)\n",
    "    X_L4_train = pd.DataFrame(data = scale(X_L4_train), index = X_L4_train.index, columns = X_L4_train.columns)\n",
    "    X_L5_train = pd.DataFrame(data = scale(X_L5_train), index = X_L5_train.index, columns = X_L5_train.columns)\n",
    "    X_L6_train = pd.DataFrame(data = scale(X_L6_train), index = X_L6_train.index, columns = X_L6_train.columns)\n",
    "    X_L7_train = pd.DataFrame(data = scale(X_L7_train), index = X_L7_train.index, columns = X_L7_train.columns)\n",
    "    X_L8_train = pd.DataFrame(data = scale(X_L8_train), index = X_L8_train.index, columns = X_L8_train.columns)\n",
    "    X_L9_train = pd.DataFrame(data = scale(X_L9_train), index = X_L9_train.index, columns = X_L9_train.columns)\n",
    "    X_L10_train = pd.DataFrame(data = scale(X_L10_train), index = X_L10_train.index, columns = X_L10_train.columns)\n",
    "    X_L11_train = pd.DataFrame(data = scale(X_L11_train), index = X_L11_train.index, columns = X_L11_train.columns)\n",
    "    X_L12_train = pd.DataFrame(data = scale(X_L12_train), index = X_L12_train.index, columns = X_L12_train.columns)\n",
    "    \n",
    "    X_L1_test = pd.DataFrame(data = scale(X_L1_test), index = X_L1_test.index, columns = X_L1_test.columns)\n",
    "    X_L2_test = pd.DataFrame(data = scale(X_L2_test), index = X_L2_test.index, columns = X_L2_test.columns)\n",
    "    X_L3_test = pd.DataFrame(data = scale(X_L3_test), index = X_L3_test.index, columns = X_L3_test.columns)\n",
    "    X_L4_test = pd.DataFrame(data = scale(X_L4_test), index = X_L4_test.index, columns = X_L4_test.columns)\n",
    "    X_L5_test = pd.DataFrame(data = scale(X_L5_test), index = X_L5_test.index, columns = X_L5_test.columns)\n",
    "    X_L6_test = pd.DataFrame(data = scale(X_L6_test), index = X_L6_test.index, columns = X_L6_test.columns)\n",
    "    X_L7_test = pd.DataFrame(data = scale(X_L7_test), index = X_L7_test.index, columns = X_L7_test.columns)\n",
    "    X_L8_test = pd.DataFrame(data = scale(X_L8_test), index = X_L8_test.index, columns = X_L8_test.columns)\n",
    "    X_L9_test = pd.DataFrame(data = scale(X_L9_test), index = X_L9_test.index, columns = X_L9_test.columns)\n",
    "    X_L10_test = pd.DataFrame(data = scale(X_L10_test), index = X_L10_test.index, columns = X_L10_test.columns)\n",
    "    X_L11_test = pd.DataFrame(data = scale(X_L11_test), index = X_L11_test.index, columns = X_L11_test.columns)\n",
    "    X_L12_test = pd.DataFrame(data = scale(X_L12_test), index = X_L12_test.index, columns = X_L12_test.columns)\n",
    "    \n",
    "    # Save raw database (split)\n",
    "    \n",
    "    y_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'y_train.csv')\n",
    "    y_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'y_test.csv')\n",
    "    X_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_train.csv')\n",
    "    X_test.to_csv(str_Dir_Plan_PC+ str(rnd_state) + ' ' + 'X_test.csv')\n",
    "    X_L1_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L1_train.csv')\n",
    "    X_L1_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L1_test.csv')\n",
    "    X_L2_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L2_train.csv')\n",
    "    X_L2_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L2_test.csv')\n",
    "    X_L3_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L3_train.csv')\n",
    "    X_L3_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L3_test.csv')\n",
    "    X_L4_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L4_train.csv')\n",
    "    X_L4_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L4_test.csv')\n",
    "    X_L5_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L5_train.csv')\n",
    "    X_L5_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L5_test.csv')\n",
    "    X_L6_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L6_train.csv')\n",
    "    X_L6_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L6_test.csv')\n",
    "    X_L7_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L7_train.csv')\n",
    "    X_L7_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L7_test.csv')\n",
    "    X_L8_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L8_train.csv')\n",
    "    X_L8_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L8_test.csv')\n",
    "    X_L9_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L9_train.csv')\n",
    "    X_L9_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L9_test.csv')\n",
    "    X_L10_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L10_train.csv')\n",
    "    X_L10_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L10_test.csv')\n",
    "    X_L11_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L11_train.csv')\n",
    "    X_L11_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L11_test.csv')\n",
    "    X_L12_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L12_train.csv')\n",
    "    X_L12_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_L12_test.csv')\n",
    "    \n",
    "    \n",
    "    # Grouped series\n",
    "    \n",
    "    X_OI = df_FRED_MD_t_norm_ex_nan_OI\n",
    "    X_LM = df_FRED_MD_t_norm_ex_nan_LM\n",
    "    X_H = df_FRED_MD_t_norm_ex_nan_H\n",
    "    X_COI = df_FRED_MD_t_norm_ex_nan_COI\n",
    "    X_MC = df_FRED_MD_t_norm_ex_nan_MC\n",
    "    X_INTFX = df_FRED_MD_t_norm_ex_nan_INTFX\n",
    "    X_P = df_FRED_MD_t_norm_ex_nan_P\n",
    "    X_S = df_FRED_MD_t_norm_ex_nan_S\n",
    "    \n",
    "    X_OI_L1 = df_FRED_MD_t_norm_ex_nan_OI_L1\n",
    "    X_LM_L1 = df_FRED_MD_t_norm_ex_nan_LM_L1\n",
    "    X_H_L1 = df_FRED_MD_t_norm_ex_nan_H_L1\n",
    "    X_COI_L1 = df_FRED_MD_t_norm_ex_nan_COI_L1\n",
    "    X_MC_L1 = df_FRED_MD_t_norm_ex_nan_MC_L1\n",
    "    X_INTFX_L1 = df_FRED_MD_t_norm_ex_nan_INTFX_L1\n",
    "    X_P_L1 = df_FRED_MD_t_norm_ex_nan_P_L1\n",
    "    X_S_L1 = df_FRED_MD_t_norm_ex_nan_S_L1\n",
    "    \n",
    "    X_OI_L2 = df_FRED_MD_t_norm_ex_nan_OI_L2\n",
    "    X_LM_L2 = df_FRED_MD_t_norm_ex_nan_LM_L2\n",
    "    X_H_L2 = df_FRED_MD_t_norm_ex_nan_H_L2\n",
    "    X_COI_L2 = df_FRED_MD_t_norm_ex_nan_COI_L2\n",
    "    X_MC_L2 = df_FRED_MD_t_norm_ex_nan_MC_L2\n",
    "    X_INTFX_L2 = df_FRED_MD_t_norm_ex_nan_INTFX_L2\n",
    "    X_P_L2 = df_FRED_MD_t_norm_ex_nan_P_L2\n",
    "    X_S_L2 = df_FRED_MD_t_norm_ex_nan_S_L2\n",
    "    \n",
    "    X_OI_L3 = df_FRED_MD_t_norm_ex_nan_OI_L3\n",
    "    X_LM_L3 = df_FRED_MD_t_norm_ex_nan_LM_L3\n",
    "    X_H_L3 = df_FRED_MD_t_norm_ex_nan_H_L3\n",
    "    X_COI_L3 = df_FRED_MD_t_norm_ex_nan_COI_L3\n",
    "    X_MC_L3 = df_FRED_MD_t_norm_ex_nan_MC_L3\n",
    "    X_INTFX_L3 = df_FRED_MD_t_norm_ex_nan_INTFX_L3\n",
    "    X_P_L3 = df_FRED_MD_t_norm_ex_nan_P_L3\n",
    "    X_S_L3 = df_FRED_MD_t_norm_ex_nan_S_L3\n",
    "    \n",
    "    X_OI_L4 = df_FRED_MD_t_norm_ex_nan_OI_L4\n",
    "    X_LM_L4 = df_FRED_MD_t_norm_ex_nan_LM_L4\n",
    "    X_H_L4 = df_FRED_MD_t_norm_ex_nan_H_L4\n",
    "    X_COI_L4 = df_FRED_MD_t_norm_ex_nan_COI_L4\n",
    "    X_MC_L4 = df_FRED_MD_t_norm_ex_nan_MC_L4\n",
    "    X_INTFX_L4 = df_FRED_MD_t_norm_ex_nan_INTFX_L4\n",
    "    X_P_L4 = df_FRED_MD_t_norm_ex_nan_P_L4\n",
    "    X_S_L4 = df_FRED_MD_t_norm_ex_nan_S_L4\n",
    "    \n",
    "    X_OI_L12 = df_FRED_MD_t_norm_ex_nan_OI_L12\n",
    "    X_LM_L12 = df_FRED_MD_t_norm_ex_nan_LM_L12\n",
    "    X_H_L12 = df_FRED_MD_t_norm_ex_nan_H_L12\n",
    "    X_COI_L12 = df_FRED_MD_t_norm_ex_nan_COI_L12\n",
    "    X_MC_L12 = df_FRED_MD_t_norm_ex_nan_MC_L12\n",
    "    X_INTFX_L12 = df_FRED_MD_t_norm_ex_nan_INTFX_L12\n",
    "    X_P_L12 = df_FRED_MD_t_norm_ex_nan_P_L12\n",
    "    X_S_L12 = df_FRED_MD_t_norm_ex_nan_S_L12\n",
    "    \n",
    "    # Split grouped series\n",
    "    \n",
    "    X_OI_train, X_OI_test = X_OI.loc[index_train], X_OI.loc[index_test]\n",
    "    X_LM_train, X_LM_test = X_LM.loc[index_train], X_LM.loc[index_test]\n",
    "    X_H_train, X_H_test = X_H.loc[index_train], X_H.loc[index_test]\n",
    "    X_COI_train, X_COI_test = X_COI.loc[index_train], X_COI.loc[index_test]\n",
    "    X_MC_train, X_MC_test = X_MC.loc[index_train], X_MC.loc[index_test]\n",
    "    X_INTFX_train, X_INTFX_test = X_INTFX.loc[index_train], X_INTFX.loc[index_test]\n",
    "    X_P_train, X_P_test = X_P.loc[index_train], X_P.loc[index_test]\n",
    "    X_S_train, X_S_test = X_S.loc[index_train], X_S.loc[index_test]\n",
    "    \n",
    "    X_OI_L1_train, X_OI_L1_test = X_OI_L1.loc[index_train], X_OI_L1.loc[index_test]\n",
    "    X_LM_L1_train, X_LM_L1_test = X_LM_L1.loc[index_train], X_LM_L1.loc[index_test]\n",
    "    X_H_L1_train, X_H_L1_test = X_H_L1.loc[index_train], X_H_L1.loc[index_test]\n",
    "    X_COI_L1_train, X_COI_L1_test = X_COI_L1.loc[index_train], X_COI_L1.loc[index_test]\n",
    "    X_MC_L1_train, X_MC_L1_test = X_MC_L1.loc[index_train], X_MC_L1.loc[index_test]\n",
    "    X_INTFX_L1_train, X_INTFX_L1_test = X_INTFX_L1.loc[index_train], X_INTFX_L1.loc[index_test]\n",
    "    X_P_L1_train, X_P_L1_test = X_P_L1.loc[index_train], X_P_L1.loc[index_test]\n",
    "    X_S_L1_train, X_S_L1_test = X_S_L1.loc[index_train], X_S_L1.loc[index_test]\n",
    "    \n",
    "    X_OI_L2_train, X_OI_L2_test = X_OI_L2.loc[index_train], X_OI_L2.loc[index_test]\n",
    "    X_LM_L2_train, X_LM_L2_test = X_LM_L2.loc[index_train], X_LM_L2.loc[index_test]\n",
    "    X_H_L2_train, X_H_L2_test = X_H_L2.loc[index_train], X_H_L2.loc[index_test]\n",
    "    X_COI_L2_train, X_COI_L2_test = X_COI_L2.loc[index_train], X_COI_L2.loc[index_test]\n",
    "    X_MC_L2_train, X_MC_L2_test = X_MC_L2.loc[index_train], X_MC_L2.loc[index_test]\n",
    "    X_INTFX_L2_train, X_INTFX_L2_test = X_INTFX_L2.loc[index_train], X_INTFX_L2.loc[index_test]\n",
    "    X_P_L2_train, X_P_L2_test = X_P_L2.loc[index_train], X_P_L2.loc[index_test]\n",
    "    X_S_L2_train, X_S_L2_test = X_S_L2.loc[index_train], X_S_L2.loc[index_test]\n",
    "    \n",
    "    X_OI_L3_train, X_OI_L3_test = X_OI_L3.loc[index_train], X_OI_L3.loc[index_test]\n",
    "    X_LM_L3_train, X_LM_L3_test = X_LM_L3.loc[index_train], X_LM_L3.loc[index_test]\n",
    "    X_H_L3_train, X_H_L3_test = X_H_L3.loc[index_train], X_H_L3.loc[index_test]\n",
    "    X_COI_L3_train, X_COI_L3_test = X_COI_L3.loc[index_train], X_COI_L3.loc[index_test]\n",
    "    X_MC_L3_train, X_MC_L3_test = X_MC_L3.loc[index_train], X_MC_L3.loc[index_test]\n",
    "    X_INTFX_L3_train, X_INTFX_L3_test = X_INTFX_L3.loc[index_train], X_INTFX_L3.loc[index_test]\n",
    "    X_P_L3_train, X_P_L3_test = X_P_L3.loc[index_train], X_P_L3.loc[index_test]\n",
    "    X_S_L3_train, X_S_L3_test = X_S_L3.loc[index_train], X_S_L3.loc[index_test]\n",
    "    \n",
    "    X_OI_L4_train, X_OI_L4_test = X_OI_L4.loc[index_train], X_OI_L4.loc[index_test]\n",
    "    X_LM_L4_train, X_LM_L4_test = X_LM_L4.loc[index_train], X_LM_L4.loc[index_test]\n",
    "    X_H_L4_train, X_H_L4_test = X_H_L4.loc[index_train], X_H_L4.loc[index_test]\n",
    "    X_COI_L4_train, X_COI_L4_test = X_COI_L4.loc[index_train], X_COI_L4.loc[index_test]\n",
    "    X_MC_L4_train, X_MC_L4_test = X_MC_L4.loc[index_train], X_MC_L4.loc[index_test]\n",
    "    X_INTFX_L4_train, X_INTFX_L4_test = X_INTFX_L4.loc[index_train], X_INTFX_L4.loc[index_test]\n",
    "    X_P_L4_train, X_P_L4_test = X_P_L4.loc[index_train], X_P_L4.loc[index_test]\n",
    "    X_S_L4_train, X_S_L4_test = X_S_L4.loc[index_train], X_S_L4.loc[index_test]\n",
    "    \n",
    "    X_OI_L12_train, X_OI_L12_test = X_OI_L12.loc[index_train], X_OI_L12.loc[index_test]\n",
    "    X_LM_L12_train, X_LM_L12_test = X_LM_L12.loc[index_train], X_LM_L12.loc[index_test]\n",
    "    X_H_L12_train, X_H_L12_test = X_H_L12.loc[index_train], X_H_L12.loc[index_test]\n",
    "    X_COI_L12_train, X_COI_L12_test = X_COI_L12.loc[index_train], X_COI_L12.loc[index_test]\n",
    "    X_MC_L12_train, X_MC_L12_test = X_MC_L12.loc[index_train], X_MC_L12.loc[index_test]\n",
    "    X_INTFX_L12_train, X_INTFX_L12_test = X_INTFX_L12.loc[index_train], X_INTFX_L12.loc[index_test]\n",
    "    X_P_L12_train, X_P_L12_test = X_P_L12.loc[index_train], X_P_L12.loc[index_test]\n",
    "    X_S_L12_train, X_S_L12_test = X_S_L12.loc[index_train], X_S_L12.loc[index_test]\n",
    "    \n",
    "    # Normalization\n",
    "    \n",
    "    X_OI_train = pd.DataFrame(data = scale(X_OI_train), \n",
    "                              index = X_OI_train.index, columns = X_OI_train.columns)\n",
    "    X_LM_train = pd.DataFrame(data = scale(X_LM_train), \n",
    "                              index = X_LM_train.index, columns = X_LM_train.columns)\n",
    "    X_H_train = pd.DataFrame(data = scale(X_H_train), \n",
    "                             index = X_H_train.index, columns = X_H_train.columns)\n",
    "    X_COI_train = pd.DataFrame(data = scale(X_COI_train), \n",
    "                               index = X_COI_train.index, columns = X_COI_train.columns)\n",
    "    X_MC_train = pd.DataFrame(data = scale(X_MC_train), \n",
    "                              index = X_MC_train.index, columns = X_MC_train.columns)\n",
    "    X_INTFX_train = pd.DataFrame(data = scale(X_INTFX_train), \n",
    "                                 index = X_INTFX_train.index, columns = X_INTFX_train.columns)\n",
    "    X_P_train = pd.DataFrame(data = scale(X_P_train), \n",
    "                             index = X_P_train.index, columns = X_P_train.columns)\n",
    "    X_S_train = pd.DataFrame(data = scale(X_S_train), \n",
    "                             index = X_S_train.index, columns = X_S_train.columns)\n",
    "    \n",
    "    X_OI_L1_train = pd.DataFrame(data = scale(X_OI_L1_train), \n",
    "                                 index = X_OI_L1_train.index, columns = X_OI_L1_train.columns)\n",
    "    X_LM_L1_train = pd.DataFrame(data = scale(X_LM_L1_train), \n",
    "                                 index = X_LM_L1_train.index, columns = X_LM_L1_train.columns)\n",
    "    X_H_L1_train = pd.DataFrame(data = scale(X_H_L1_train), \n",
    "                                index = X_H_L1_train.index, columns = X_H_L1_train.columns)\n",
    "    X_COI_L1_train = pd.DataFrame(data = scale(X_COI_L1_train), \n",
    "                                  index = X_COI_L1_train.index, columns = X_COI_L1_train.columns)\n",
    "    X_MC_L1_train = pd.DataFrame(data = scale(X_MC_L1_train), \n",
    "                                 index = X_MC_L1_train.index, columns = X_MC_L1_train.columns)\n",
    "    X_INTFX_L1_train = pd.DataFrame(data = scale(X_INTFX_L1_train), \n",
    "                                    index = X_INTFX_L1_train.index, columns = X_INTFX_L1_train.columns)\n",
    "    X_P_L1_train = pd.DataFrame(data = scale(X_P_L1_train), \n",
    "                                index = X_P_L1_train.index, columns = X_P_L1_train.columns)\n",
    "    X_S_L1_train = pd.DataFrame(data = scale(X_S_L1_train), \n",
    "                                index = X_S_L1_train.index, columns = X_S_L1_train.columns)\n",
    "    \n",
    "    X_OI_L2_train = pd.DataFrame(data = scale(X_OI_L2_train), \n",
    "                                 index = X_OI_L2_train.index, columns = X_OI_L2_train.columns)\n",
    "    X_LM_L2_train = pd.DataFrame(data = scale(X_LM_L2_train), \n",
    "                                 index = X_LM_L2_train.index, columns = X_LM_L2_train.columns)\n",
    "    X_H_L2_train = pd.DataFrame(data = scale(X_H_L2_train), \n",
    "                                index = X_H_L2_train.index, columns = X_H_L2_train.columns)\n",
    "    X_COI_L2_train = pd.DataFrame(data = scale(X_COI_L2_train), \n",
    "                                  index = X_COI_L2_train.index, columns = X_COI_L2_train.columns)\n",
    "    X_MC_L2_train = pd.DataFrame(data = scale(X_MC_L2_train), \n",
    "                                 index = X_MC_L2_train.index, columns = X_MC_L2_train.columns)\n",
    "    X_INTFX_L2_train = pd.DataFrame(data = scale(X_INTFX_L2_train), \n",
    "                                    index = X_INTFX_L2_train.index, columns = X_INTFX_L2_train.columns)\n",
    "    X_P_L2_train = pd.DataFrame(data = scale(X_P_L2_train), \n",
    "                                index = X_P_L2_train.index, columns = X_P_L2_train.columns)\n",
    "    X_S_L2_train = pd.DataFrame(data = scale(X_S_L2_train), \n",
    "                                index = X_S_L2_train.index, columns = X_S_L2_train.columns)\n",
    "    \n",
    "    X_OI_L3_train = pd.DataFrame(data = scale(X_OI_L3_train), \n",
    "                                 index = X_OI_L3_train.index, columns = X_OI_L3_train.columns)\n",
    "    X_LM_L3_train = pd.DataFrame(data = scale(X_LM_L3_train), \n",
    "                                 index = X_LM_L3_train.index, columns = X_LM_L3_train.columns)\n",
    "    X_H_L3_train = pd.DataFrame(data = scale(X_H_L3_train), \n",
    "                                index = X_H_L3_train.index, columns = X_H_L3_train.columns)\n",
    "    X_COI_L3_train = pd.DataFrame(data = scale(X_COI_L3_train), \n",
    "                                  index = X_COI_L3_train.index, columns = X_COI_L3_train.columns)\n",
    "    X_MC_L3_train = pd.DataFrame(data = scale(X_MC_L3_train), \n",
    "                                 index = X_MC_L3_train.index, columns = X_MC_L3_train.columns)\n",
    "    X_INTFX_L3_train = pd.DataFrame(data = scale(X_INTFX_L3_train), \n",
    "                                    index = X_INTFX_L3_train.index, columns = X_INTFX_L3_train.columns)\n",
    "    X_P_L3_train = pd.DataFrame(data = scale(X_P_L3_train), \n",
    "                                index = X_P_L3_train.index, columns = X_P_L3_train.columns)\n",
    "    X_S_L3_train = pd.DataFrame(data = scale(X_S_L3_train), \n",
    "                                index = X_S_L3_train.index, columns = X_S_L3_train.columns)\n",
    "    \n",
    "    X_OI_L4_train = pd.DataFrame(data = scale(X_OI_L4_train), \n",
    "                                 index = X_OI_L4_train.index, columns = X_OI_L4_train.columns)\n",
    "    X_LM_L4_train = pd.DataFrame(data = scale(X_LM_L4_train), \n",
    "                                 index = X_LM_L4_train.index, columns = X_LM_L4_train.columns)\n",
    "    X_H_L4_train = pd.DataFrame(data = scale(X_H_L4_train), \n",
    "                                index = X_H_L4_train.index, columns = X_H_L4_train.columns)\n",
    "    X_COI_L4_train = pd.DataFrame(data = scale(X_COI_L4_train), \n",
    "                                  index = X_COI_L4_train.index, columns = X_COI_L4_train.columns)\n",
    "    X_MC_L4_train = pd.DataFrame(data = scale(X_MC_L4_train), \n",
    "                                 index = X_MC_L4_train.index, columns = X_MC_L4_train.columns)\n",
    "    X_INTFX_L4_train = pd.DataFrame(data = scale(X_INTFX_L4_train), \n",
    "                                    index = X_INTFX_L4_train.index, columns = X_INTFX_L4_train.columns)\n",
    "    X_P_L4_train = pd.DataFrame(data = scale(X_P_L4_train), \n",
    "                                index = X_P_L4_train.index, columns = X_P_L4_train.columns)\n",
    "    X_S_L4_train = pd.DataFrame(data = scale(X_S_L4_train), \n",
    "                                index = X_S_L4_train.index, columns = X_S_L4_train.columns)\n",
    "    \n",
    "    X_OI_L12_train = pd.DataFrame(data = scale(X_OI_L12_train), \n",
    "                                  index = X_OI_L12_train.index, columns = X_OI_L12_train.columns)\n",
    "    X_LM_L12_train = pd.DataFrame(data = scale(X_LM_L12_train), \n",
    "                                  index = X_LM_L12_train.index, columns = X_LM_L12_train.columns)\n",
    "    X_H_L12_train = pd.DataFrame(data = scale(X_H_L12_train), \n",
    "                                 index = X_H_L12_train.index, columns = X_H_L12_train.columns)\n",
    "    X_COI_L12_train = pd.DataFrame(data = scale(X_COI_L12_train), \n",
    "                                   index = X_COI_L12_train.index, columns = X_COI_L12_train.columns)\n",
    "    X_MC_L12_train = pd.DataFrame(data = scale(X_MC_L12_train), \n",
    "                                  index = X_MC_L12_train.index, columns = X_MC_L12_train.columns)\n",
    "    X_INTFX_L12_train = pd.DataFrame(data = scale(X_INTFX_L12_train), \n",
    "                                     index = X_INTFX_L12_train.index, columns = X_INTFX_L12_train.columns)\n",
    "    X_P_L12_train = pd.DataFrame(data = scale(X_P_L12_train), \n",
    "                                 index = X_P_L12_train.index, columns = X_P_L12_train.columns)\n",
    "    X_S_L12_train = pd.DataFrame(data = scale(X_S_L12_train), \n",
    "                                 index = X_S_L12_train.index, columns = X_S_L12_train.columns)\n",
    "    \n",
    "    # Normalization\n",
    "    \n",
    "    X_OI_test = pd.DataFrame(data = scale(X_OI_test), \n",
    "                              index = X_OI_test.index, columns = X_OI_test.columns)\n",
    "    X_LM_test = pd.DataFrame(data = scale(X_LM_test), \n",
    "                              index = X_LM_test.index, columns = X_LM_test.columns)\n",
    "    X_H_test = pd.DataFrame(data = scale(X_H_test), \n",
    "                             index = X_H_test.index, columns = X_H_test.columns)\n",
    "    X_COI_test = pd.DataFrame(data = scale(X_COI_test), \n",
    "                               index = X_COI_test.index, columns = X_COI_test.columns)\n",
    "    X_MC_test = pd.DataFrame(data = scale(X_MC_test), \n",
    "                              index = X_MC_test.index, columns = X_MC_test.columns)\n",
    "    X_INTFX_test = pd.DataFrame(data = scale(X_INTFX_test), \n",
    "                                 index = X_INTFX_test.index, columns = X_INTFX_test.columns)\n",
    "    X_P_test = pd.DataFrame(data = scale(X_P_test), \n",
    "                             index = X_P_test.index, columns = X_P_test.columns)\n",
    "    X_S_test = pd.DataFrame(data = scale(X_S_test), \n",
    "                             index = X_S_test.index, columns = X_S_test.columns)\n",
    "    \n",
    "    X_OI_L1_test = pd.DataFrame(data = scale(X_OI_L1_test), \n",
    "                                 index = X_OI_L1_test.index, columns = X_OI_L1_test.columns)\n",
    "    X_LM_L1_test = pd.DataFrame(data = scale(X_LM_L1_test), \n",
    "                                 index = X_LM_L1_test.index, columns = X_LM_L1_test.columns)\n",
    "    X_H_L1_test = pd.DataFrame(data = scale(X_H_L1_test), \n",
    "                                index = X_H_L1_test.index, columns = X_H_L1_test.columns)\n",
    "    X_COI_L1_test = pd.DataFrame(data = scale(X_COI_L1_test), \n",
    "                                  index = X_COI_L1_test.index, columns = X_COI_L1_test.columns)\n",
    "    X_MC_L1_test = pd.DataFrame(data = scale(X_MC_L1_test), \n",
    "                                 index = X_MC_L1_test.index, columns = X_MC_L1_test.columns)\n",
    "    X_INTFX_L1_test = pd.DataFrame(data = scale(X_INTFX_L1_test), \n",
    "                                    index = X_INTFX_L1_test.index, columns = X_INTFX_L1_test.columns)\n",
    "    X_P_L1_test = pd.DataFrame(data = scale(X_P_L1_test), \n",
    "                                index = X_P_L1_test.index, columns = X_P_L1_test.columns)\n",
    "    X_S_L1_test = pd.DataFrame(data = scale(X_S_L1_test), \n",
    "                                index = X_S_L1_test.index, columns = X_S_L1_test.columns)\n",
    "    \n",
    "    X_OI_L2_test = pd.DataFrame(data = scale(X_OI_L2_test), \n",
    "                                 index = X_OI_L2_test.index, columns = X_OI_L2_test.columns)\n",
    "    X_LM_L2_test = pd.DataFrame(data = scale(X_LM_L2_test), \n",
    "                                 index = X_LM_L2_test.index, columns = X_LM_L2_test.columns)\n",
    "    X_H_L2_test = pd.DataFrame(data = scale(X_H_L2_test), \n",
    "                                index = X_H_L2_test.index, columns = X_H_L2_test.columns)\n",
    "    X_COI_L2_test = pd.DataFrame(data = scale(X_COI_L2_test), \n",
    "                                  index = X_COI_L2_test.index, columns = X_COI_L2_test.columns)\n",
    "    X_MC_L2_test = pd.DataFrame(data = scale(X_MC_L2_test), \n",
    "                                 index = X_MC_L2_test.index, columns = X_MC_L2_test.columns)\n",
    "    X_INTFX_L2_test = pd.DataFrame(data = scale(X_INTFX_L2_test), \n",
    "                                    index = X_INTFX_L2_test.index, columns = X_INTFX_L2_test.columns)\n",
    "    X_P_L2_test = pd.DataFrame(data = scale(X_P_L2_test), \n",
    "                                index = X_P_L2_test.index, columns = X_P_L2_test.columns)\n",
    "    X_S_L2_test = pd.DataFrame(data = scale(X_S_L2_test), \n",
    "                                index = X_S_L2_test.index, columns = X_S_L2_test.columns)\n",
    "    \n",
    "    X_OI_L3_test = pd.DataFrame(data = scale(X_OI_L3_test), \n",
    "                                 index = X_OI_L3_test.index, columns = X_OI_L3_test.columns)\n",
    "    X_LM_L3_test = pd.DataFrame(data = scale(X_LM_L3_test), \n",
    "                                 index = X_LM_L3_test.index, columns = X_LM_L3_test.columns)\n",
    "    X_H_L3_test = pd.DataFrame(data = scale(X_H_L3_test), \n",
    "                                index = X_H_L3_test.index, columns = X_H_L3_test.columns)\n",
    "    X_COI_L3_test = pd.DataFrame(data = scale(X_COI_L3_test), \n",
    "                                  index = X_COI_L3_test.index, columns = X_COI_L3_test.columns)\n",
    "    X_MC_L3_test = pd.DataFrame(data = scale(X_MC_L3_test), \n",
    "                                 index = X_MC_L3_test.index, columns = X_MC_L3_test.columns)\n",
    "    X_INTFX_L3_test = pd.DataFrame(data = scale(X_INTFX_L3_test), \n",
    "                                    index = X_INTFX_L3_test.index, columns = X_INTFX_L3_test.columns)\n",
    "    X_P_L3_test = pd.DataFrame(data = scale(X_P_L3_test), \n",
    "                                index = X_P_L3_test.index, columns = X_P_L3_test.columns)\n",
    "    X_S_L3_test = pd.DataFrame(data = scale(X_S_L3_test), \n",
    "                                index = X_S_L3_test.index, columns = X_S_L3_test.columns)\n",
    "    \n",
    "    X_OI_L4_test = pd.DataFrame(data = scale(X_OI_L4_test), \n",
    "                                 index = X_OI_L4_test.index, columns = X_OI_L4_test.columns)\n",
    "    X_LM_L4_test = pd.DataFrame(data = scale(X_LM_L4_test), \n",
    "                                 index = X_LM_L4_test.index, columns = X_LM_L4_test.columns)\n",
    "    X_H_L4_test = pd.DataFrame(data = scale(X_H_L4_test), \n",
    "                                index = X_H_L4_test.index, columns = X_H_L4_test.columns)\n",
    "    X_COI_L4_test = pd.DataFrame(data = scale(X_COI_L4_test), \n",
    "                                  index = X_COI_L4_test.index, columns = X_COI_L4_test.columns)\n",
    "    X_MC_L4_test = pd.DataFrame(data = scale(X_MC_L4_test), \n",
    "                                 index = X_MC_L4_test.index, columns = X_MC_L4_test.columns)\n",
    "    X_INTFX_L4_test = pd.DataFrame(data = scale(X_INTFX_L4_test), \n",
    "                                    index = X_INTFX_L4_test.index, columns = X_INTFX_L4_test.columns)\n",
    "    X_P_L4_test = pd.DataFrame(data = scale(X_P_L4_test), \n",
    "                                index = X_P_L4_test.index, columns = X_P_L4_test.columns)\n",
    "    X_S_L4_test = pd.DataFrame(data = scale(X_S_L4_test), \n",
    "                                index = X_S_L4_test.index, columns = X_S_L4_test.columns)\n",
    "    \n",
    "    X_OI_L12_test = pd.DataFrame(data = scale(X_OI_L12_test), \n",
    "                                  index = X_OI_L12_test.index, columns = X_OI_L12_test.columns)\n",
    "    X_LM_L12_test = pd.DataFrame(data = scale(X_LM_L12_test), \n",
    "                                  index = X_LM_L12_test.index, columns = X_LM_L12_test.columns)\n",
    "    X_H_L12_test = pd.DataFrame(data = scale(X_H_L12_test), \n",
    "                                 index = X_H_L12_test.index, columns = X_H_L12_test.columns)\n",
    "    X_COI_L12_test = pd.DataFrame(data = scale(X_COI_L12_test), \n",
    "                                   index = X_COI_L12_test.index, columns = X_COI_L12_test.columns)\n",
    "    X_MC_L12_test = pd.DataFrame(data = scale(X_MC_L12_test), \n",
    "                                  index = X_MC_L12_test.index, columns = X_MC_L12_test.columns)\n",
    "    X_INTFX_L12_test = pd.DataFrame(data = scale(X_INTFX_L12_test), \n",
    "                                     index = X_INTFX_L12_test.index, columns = X_INTFX_L12_test.columns)\n",
    "    X_P_L12_test = pd.DataFrame(data = scale(X_P_L12_test), \n",
    "                                 index = X_P_L12_test.index, columns = X_P_L12_test.columns)\n",
    "    X_S_L12_test = pd.DataFrame(data = scale(X_S_L12_test), \n",
    "                                 index = X_S_L12_test.index, columns = X_S_L12_test.columns)\n",
    "\n",
    "    #####################################################################################################################\n",
    "    #                                                                                                                   #\n",
    "    # Denoising and Compression                                                                                         #\n",
    "    #                                                                                                                   #\n",
    "    #####################################################################################################################\n",
    "    \n",
    "    X_train_pca, X_test_pca = pca_decomp(X_train, X_test, threshold = 0.9, plot_name = 'X_full_pca')\n",
    "    X_OI_train_pca, X_OI_test_pca = pca_decomp(X_OI_train, X_OI_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_OI_pca')\n",
    "    X_LM_train_pca, X_LM_test_pca = pca_decomp(X_LM_train, X_LM_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_LM_pca')\n",
    "    X_H_train_pca, X_H_test_pca = pca_decomp(X_H_train, X_H_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_H_pca')\n",
    "    X_COI_train_pca, X_COI_test_pca = pca_decomp(X_COI_train, X_COI_test, \n",
    "                                                       threshold = 0.9, plot_name = 'X_full_COI_pca')\n",
    "    X_MC_train_pca, X_MC_test_pca = pca_decomp(X_MC_train, X_MC_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_MC_pca')\n",
    "    X_INTFX_train_pca, X_INTFX_test_pca = pca_decomp(X_INTFX_train, X_INTFX_test, \n",
    "                                                           threshold = 0.9, plot_name = 'X_full_INTFX_pca')\n",
    "    X_P_train_pca, X_P_test_pca = pca_decomp(X_P_train, X_P_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_P_pca')\n",
    "    X_S_train_pca, X_S_test_pca = pca_decomp(X_S_train, X_S_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_S_pca')\n",
    "    \n",
    "    X_L1_train_pca, X_L1_test_pca = pca_decomp(X_L1_train, X_L1_test, threshold = 0.9, plot_name = 'X_full_L1_pca')\n",
    "    X_OI_L1_train_pca, X_OI_L1_test_pca = pca_decomp(X_OI_L1_train, X_OI_L1_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_OI_L1_pca')\n",
    "    X_LM_L1_train_pca, X_LM_L1_test_pca = pca_decomp(X_LM_L1_train, X_LM_L1_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_LM_L1_pca')\n",
    "    X_H_L1_train_pca, X_H_L1_test_pca = pca_decomp(X_H_L1_train, X_H_L1_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_H_L1_pca')\n",
    "    X_COI_L1_train_pca, X_COI_L1_test_pca = pca_decomp(X_COI_L1_train, X_COI_L1_test, \n",
    "                                                       threshold = 0.9, plot_name = 'X_full_COI_L1_pca')\n",
    "    X_MC_L1_train_pca, X_MC_L1_test_pca = pca_decomp(X_MC_L1_train, X_MC_L1_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_MC_L1_pca')\n",
    "    X_INTFX_L1_train_pca, X_INTFX_L1_test_pca = pca_decomp(X_INTFX_L1_train, X_INTFX_L1_test, \n",
    "                                                           threshold = 0.9, plot_name = 'X_full_INTFX_L1_pca')\n",
    "    X_P_L1_train_pca, X_P_L1_test_pca = pca_decomp(X_P_L1_train, X_P_L1_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_P_L1_pca')\n",
    "    X_S_L1_train_pca, X_S_L1_test_pca = pca_decomp(X_S_L1_train, X_S_L1_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_S_L1_pca')\n",
    "    \n",
    "    X_L2_train_pca, X_L2_test_pca = pca_decomp(X_L2_train, X_L2_test, threshold = 0.9, plot_name = 'X_full_L2_pca')\n",
    "    X_OI_L2_train_pca, X_OI_L2_test_pca = pca_decomp(X_OI_L2_train, X_OI_L2_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_OI_L2_pca')\n",
    "    X_LM_L2_train_pca, X_LM_L2_test_pca = pca_decomp(X_LM_L2_train, X_LM_L2_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_LM_L2_pca')\n",
    "    X_H_L2_train_pca, X_H_L2_test_pca = pca_decomp(X_H_L2_train, X_H_L2_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_H_L2_pca')\n",
    "    X_COI_L2_train_pca, X_COI_L2_test_pca = pca_decomp(X_COI_L2_train, X_COI_L2_test, \n",
    "                                                       threshold = 0.9, plot_name = 'X_full_COI_L2_pca')\n",
    "    X_MC_L2_train_pca, X_MC_L2_test_pca = pca_decomp(X_MC_L2_train, X_MC_L2_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_MC_L2_pca')\n",
    "    X_INTFX_L2_train_pca, X_INTFX_L2_test_pca = pca_decomp(X_INTFX_L2_train, X_INTFX_L2_test, \n",
    "                                                           threshold = 0.9, plot_name = 'X_full_INTFX_L2_pca')\n",
    "    X_P_L2_train_pca, X_P_L2_test_pca = pca_decomp(X_P_L2_train, X_P_L2_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_P_L2_pca')\n",
    "    X_S_L2_train_pca, X_S_L2_test_pca = pca_decomp(X_S_L2_train, X_S_L2_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_S_L2_pca')\n",
    "    \n",
    "    X_L3_train_pca, X_L3_test_pca = pca_decomp(X_L3_train, X_L3_test, threshold = 0.9, plot_name = 'X_full_L3_pca')\n",
    "    X_OI_L3_train_pca, X_OI_L3_test_pca = pca_decomp(X_OI_L3_train, X_OI_L3_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_OI_L3_pca')\n",
    "    X_LM_L3_train_pca, X_LM_L3_test_pca = pca_decomp(X_LM_L3_train, X_LM_L3_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_LM_L3_pca')\n",
    "    X_H_L3_train_pca, X_H_L3_test_pca = pca_decomp(X_H_L3_train, X_H_L3_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_H_L3_pca')\n",
    "    X_COI_L3_train_pca, X_COI_L3_test_pca = pca_decomp(X_COI_L3_train, X_COI_L3_test, \n",
    "                                                       threshold = 0.9, plot_name = 'X_full_COI_L3_pca')\n",
    "    X_MC_L3_train_pca, X_MC_L3_test_pca = pca_decomp(X_MC_L3_train, X_MC_L3_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_MC_L3_pca')\n",
    "    X_INTFX_L3_train_pca, X_INTFX_L3_test_pca = pca_decomp(X_INTFX_L3_train, X_INTFX_L3_test, \n",
    "                                                           threshold = 0.9, plot_name = 'X_full_INTFX_L3_pca')\n",
    "    X_P_L3_train_pca, X_P_L3_test_pca = pca_decomp(X_P_L3_train, X_P_L3_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_P_L3_pca')\n",
    "    X_S_L3_train_pca, X_S_L3_test_pca = pca_decomp(X_S_L3_train, X_S_L3_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_S_L3_pca')\n",
    "    \n",
    "    X_L4_train_pca, X_L4_test_pca = pca_decomp(X_L4_train, X_L4_test, threshold = 0.9, plot_name = 'X_full_L4_pca')\n",
    "    X_OI_L4_train_pca, X_OI_L4_test_pca = pca_decomp(X_OI_L4_train, X_OI_L4_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_OI_L4_pca')\n",
    "    X_LM_L4_train_pca, X_LM_L4_test_pca = pca_decomp(X_LM_L4_train, X_LM_L4_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_LM_L4_pca')\n",
    "    X_H_L4_train_pca, X_H_L4_test_pca = pca_decomp(X_H_L4_train, X_H_L4_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_H_L4_pca')\n",
    "    X_COI_L4_train_pca, X_COI_L4_test_pca = pca_decomp(X_COI_L4_train, X_COI_L4_test, \n",
    "                                                       threshold = 0.9, plot_name = 'X_full_COI_L4_pca')\n",
    "    X_MC_L4_train_pca, X_MC_L4_test_pca = pca_decomp(X_MC_L4_train, X_MC_L4_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_MC_L4_pca')\n",
    "    X_INTFX_L4_train_pca, X_INTFX_L4_test_pca = pca_decomp(X_INTFX_L4_train, X_INTFX_L4_test, \n",
    "                                                           threshold = 0.9, plot_name = 'X_full_INTFX_L4_pca')\n",
    "    X_P_L4_train_pca, X_P_L4_test_pca = pca_decomp(X_P_L4_train, X_P_L4_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_P_L4_pca')\n",
    "    X_S_L4_train_pca, X_S_L4_test_pca = pca_decomp(X_S_L4_train, X_S_L4_test, \n",
    "                                                   threshold = 0.9, plot_name = 'X_full_S_L4_pca')\n",
    "    \n",
    "    X_L12_train_pca, X_L12_test_pca = pca_decomp(X_L12_train, X_L12_test, threshold = 0.9, plot_name = 'X_full_L12_pca')\n",
    "    X_OI_L12_train_pca, X_OI_L12_test_pca = pca_decomp(X_OI_L12_train, X_OI_L12_test, \n",
    "                                                       threshold = 0.9, plot_name = 'X_full_OI_L12_pca')\n",
    "    X_LM_L12_train_pca, X_LM_L12_test_pca = pca_decomp(X_LM_L12_train, X_LM_L12_test, \n",
    "                                                       threshold = 0.9, plot_name = 'X_full_LM_L12_pca')\n",
    "    X_H_L12_train_pca, X_H_L12_test_pca = pca_decomp(X_H_L12_train, X_H_L12_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_H_L12_pca')\n",
    "    X_COI_L12_train_pca, X_COI_L12_test_pca = pca_decomp(X_COI_L12_train, X_COI_L12_test, \n",
    "                                                         threshold = 0.9, plot_name = 'X_full_COI_L12_pca')\n",
    "    X_MC_L12_train_pca, X_MC_L12_test_pca = pca_decomp(X_MC_L12_train, X_MC_L12_test, \n",
    "                                                       threshold = 0.9, plot_name = 'X_full_MC_L12_pca')\n",
    "    X_INTFX_L12_train_pca, X_INTFX_L12_test_pca = pca_decomp(X_INTFX_L12_train, X_INTFX_L12_test, \n",
    "                                                             threshold = 0.9, plot_name = 'X_full_INTFX_L12_pca')\n",
    "    X_P_L12_train_pca, X_P_L12_test_pca = pca_decomp(X_P_L12_train, X_P_L12_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_P_L12_pca')\n",
    "    X_S_L12_train_pca, X_S_L12_test_pca = pca_decomp(X_S_L12_train, X_S_L12_test, \n",
    "                                                     threshold = 0.9, plot_name = 'X_full_S_L12_pca')\n",
    "    \n",
    "    # Reads dimensions from PCA\n",
    "    n = X_train.shape[1]\n",
    "    n_OI = X_OI_train.shape[1]\n",
    "    n_LM = X_LM_train.shape[1]\n",
    "    n_H = X_H_train.shape[1]\n",
    "    n_COI = X_COI_train.shape[1]\n",
    "    n_MC = X_MC_train.shape[1]\n",
    "    n_INTFX = X_INTFX_train.shape[1]\n",
    "    n_P = X_P_train.shape[1]\n",
    "    n_S = X_S_train.shape[1]\n",
    "    \n",
    "    n_pca = X_train_pca.shape[1]\n",
    "    n_OI_pca = X_OI_train_pca.shape[1]\n",
    "    n_LM_pca = X_LM_train_pca.shape[1]\n",
    "    n_H_pca = X_H_train_pca.shape[1]\n",
    "    n_COI_pca = X_COI_train_pca.shape[1]\n",
    "    n_MC_pca = X_MC_train_pca.shape[1]\n",
    "    n_INTFX_pca = X_INTFX_train_pca.shape[1]\n",
    "    n_P_pca = X_P_train_pca.shape[1]\n",
    "    n_S_pca = X_S_train_pca.shape[1]\n",
    "    \n",
    "    X_train_ae, X_test_ae = deep_ae(X_train, X_test, \n",
    "                                    intermediate_dim = int((n + n_pca)/2), latent_dim = n_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_full_ae')\n",
    "    \n",
    "    X_OI_train_ae, X_OI_test_ae = deep_ae(X_OI_train, X_OI_test, \n",
    "                                    intermediate_dim = int((n_OI + n_OI_pca)/2), latent_dim = n_OI_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_OI_ae')\n",
    "    \n",
    "    X_LM_train_ae, X_LM_test_ae = deep_ae(X_LM_train, X_LM_test, \n",
    "                                    intermediate_dim = int((n_LM + n_LM_pca)/2), latent_dim = n_LM_pca,  \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_LM_ae')\n",
    "    \n",
    "    X_H_train_ae, X_H_test_ae = deep_ae(X_H_train, X_H_test, \n",
    "                                    intermediate_dim = int((n_H + n_H_pca)/2), latent_dim = n_H_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_H_ae')\n",
    "    \n",
    "    X_COI_train_ae, X_COI_test_ae = deep_ae(X_COI_train, X_COI_test, \n",
    "                                    intermediate_dim = int((n_COI + n_COI_pca)/2), latent_dim = n_COI_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_COI_ae')\n",
    "    \n",
    "    X_MC_train_ae, X_MC_test_ae = deep_ae(X_MC_train, X_MC_test, \n",
    "                                    intermediate_dim = int((n_MC + n_MC_pca)/2), latent_dim = n_MC_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_MC_ae')\n",
    "    \n",
    "    X_INTFX_train_ae, X_INTFX_test_ae = deep_ae(X_INTFX_train, X_INTFX_test, \n",
    "                                    intermediate_dim = int((n_INTFX + n_INTFX_pca)/2), latent_dim = n_INTFX_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_INTFX_ae')\n",
    "    \n",
    "    X_P_train_ae, X_P_test_ae = deep_ae(X_P_train, X_P_test, \n",
    "                                    intermediate_dim = int((n_P + n_P_pca)/2), latent_dim = n_P_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_P_ae')\n",
    "    \n",
    "    X_S_train_ae, X_S_test_ae = deep_ae(X_S_train, X_S_test, \n",
    "                                    intermediate_dim = int((n_S + n_S_pca)/2), latent_dim = n_S_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_S_ae')\n",
    "    \n",
    "    X_train_vae, X_test_vae = vae(X_train, X_test, \n",
    "                                    intermediate_dim = int((n + n_pca)/2), latent_dim = n_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_full_vae')\n",
    "    \n",
    "    X_OI_train_vae, X_OI_test_vae = vae(X_OI_train, X_OI_test, \n",
    "                                    intermediate_dim = int((n_OI + n_OI_pca)/2), latent_dim = n_OI_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_OI_vae')\n",
    "    \n",
    "    X_LM_train_vae, X_LM_test_vae = vae(X_LM_train, X_LM_test, \n",
    "                                    intermediate_dim = int((n_LM + n_LM_pca)/2), latent_dim = n_LM_pca,  \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_LM_vae')\n",
    "    \n",
    "    X_H_train_vae, X_H_test_vae = vae(X_H_train, X_H_test, \n",
    "                                    intermediate_dim = int((n_H + n_H_pca)/2), latent_dim = n_H_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_H_vae')\n",
    "    \n",
    "    X_COI_train_vae, X_COI_test_vae = vae(X_COI_train, X_COI_test, \n",
    "                                    intermediate_dim = int((n_COI + n_COI_pca)/2), latent_dim = n_COI_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_COI_vae')\n",
    "    \n",
    "    X_MC_train_vae, X_MC_test_vae = vae(X_MC_train, X_MC_test, \n",
    "                                    intermediate_dim = int((n_MC + n_MC_pca)/2), latent_dim = n_MC_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_MC_vae')\n",
    "    \n",
    "    X_INTFX_train_vae, X_INTFX_test_vae = vae(X_INTFX_train, X_INTFX_test, \n",
    "                                    intermediate_dim = int((n_INTFX + n_INTFX_pca)/2), latent_dim = n_INTFX_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_INTFX_vae')\n",
    "    \n",
    "    X_P_train_vae, X_P_test_vae = vae(X_P_train, X_P_test, \n",
    "                                    intermediate_dim = int((n_P + n_P_pca)/2), latent_dim = n_P_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_P_vae')\n",
    "    \n",
    "    X_S_train_vae, X_S_test_vae = vae(X_S_train, X_S_test, \n",
    "                                    intermediate_dim = int((n_S + n_S_pca)/2), latent_dim = n_S_pca, \n",
    "                                    batch_size = 16, epochs = 100, \n",
    "                                    verbose = False, plot_name = 'X_S_vae')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
