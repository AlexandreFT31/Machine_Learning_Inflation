{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-070739e9a808>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;31m# Statsmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsaplots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_acf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_pacf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstattools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madfuller\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\statsmodels\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_version\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Libraries                                                                                                         #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# R Path - required by rpy2 prior to importing libraries\n",
    "import os\n",
    "os.environ[\"R_HOME\"] = r\"C:\\R\\R-4.0.3\"\n",
    "os.environ[\"PATH\"] = r\"C:\\R\\R-4.0.3\\bin\\x64\" + \";\" + os.environ[\"PATH\"]\n",
    "import rpy2\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy.stats import jarque_bera\n",
    "from datetime import datetime\n",
    "\n",
    "# R\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r, DataFrame, FloatVector\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "# Kalman Filter\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# Wavelets\n",
    "import pywt\n",
    "\n",
    "# Machine Learning - tensorflow, keras, and sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, LSTM, ConvLSTM2D, Flatten, BatchNormalization, Lambda \n",
    "from keras.layers import MaxPooling1D, MaxPooling2D, MaxPooling3D, Conv3D, RepeatVector, TimeDistributed, Bidirectional\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils import plot_model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# SKLearn Models\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import HuberRegressor, TheilSenRegressor, LinearRegression \n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller \n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX \n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox \n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.vector_ar.svar_model import SVAR\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "from statsmodels.tsa.vector_ar.vecm import select_coint_rank\n",
    "from statsmodels.tsa.vector_ar.output import VARSummary\n",
    "\n",
    "# Univariate GARCH\n",
    "from arch import arch_model\n",
    "\n",
    "# Plots\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import dates as md\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# Misc\n",
    "import pydot\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Constants and Parameters                                                                                          #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# Folders and worksheet names\n",
    "str_Dir_Plan_FRED = 'C:/Users/alext/Desktop/Modelo/FRED/'\n",
    "str_Dir_Plan_Data = 'C:/Users/alext/Desktop/Modelo/PC/'\n",
    "str_Dir_Plan_PC = 'C:/Users/alext/Desktop/Modelo/PC/'\n",
    "str_Dir_Results = 'C:/Users/alext/Desktop/Modelo/Results/'\n",
    "str_Nome_Plan_FRED_MD = 'FRED_MD_2020_04'\n",
    "str_Nome_Plan_FRED_QD = 'FRED_QD_2020_04'\n",
    "str_Nome_Plan_FRED_MD_Desc = 'Data_Description_MD'\n",
    "str_Nome_Plan_FRED_QD_Desc = 'Data_Description_QD'\n",
    "\n",
    "# How to display plots\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.dpi'] = 200 # Plot resolution (dpi)\n",
    "\n",
    "# Required to convert datatypes from Python to R and vice-versa\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Remove warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Color style (plots)\n",
    "sns.set(color_codes = True)\n",
    "\n",
    "# Statistical significance for hypothesis testing\n",
    "# Using 1% due to the high number of tests carried out\n",
    "alfa = 0.01\n",
    "\n",
    "# Test size (share of observations used to build the test sample)\n",
    "share_test_size = 0.20\n",
    "\n",
    "# Validation sample size (share of observations used to build the validation sample)\n",
    "share_validation_size = 0.20\n",
    "\n",
    "# Random state (used to split samples into training and test samples)\n",
    "rnd_state = 42\n",
    "\n",
    "# Number of lags considered when splitting the data - see LSTM models\n",
    "n_lags_lstm = 12\n",
    "\n",
    "# Number of lags considered when splitting the data - see ConvLSTM models\n",
    "n_lags_conv = 12\n",
    "\n",
    "# Number of sequences into which sample are broken when fitting ConvLSTM\n",
    "# Note: n_lags = n_seq * n_steps\n",
    "n_seq_conv = 1\n",
    "\n",
    "# Size of each sequence into which sample are broken when fitting ConvLSTM\n",
    "# Note: n_lags = n_seq * n_steps\n",
    "n_steps_conv = int(n_lags_conv / n_seq_conv)\n",
    "\n",
    "# Activation function\n",
    "act_fun = 'selu'\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Auxiliary Functions                                                                                               #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# Split a univariate sequence into samples\n",
    "def split_sequence_uni(sequence, n_steps, per_ahead, cum = False):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix + per_ahead - 1 > len(sequence) - 1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        if cum == False:\n",
    "            seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + per_ahead - 1]\n",
    "        else:\n",
    "            seq_x, seq_y = sequence[i:end_ix], np.sum(sequence[end_ix:(end_ix + per_ahead)])\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Split a multivariate sequence into samples\n",
    "def split_sequence_mult(sequences, n_steps, per_ahead, cum = False):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix + per_ahead - 1 > len(sequences) - 1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        if cum == False:\n",
    "            seq_x, seq_y = sequences[(i+1):(end_ix+1), :-1], sequences[end_ix + per_ahead - 1, -1]\n",
    "        else:\n",
    "            seq_x, seq_y = sequences[(i+1):(end_ix+1), :-1], np.sum(sequences[end_ix:(end_ix + per_ahead), -1])\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Kalman filter regression\n",
    "# If EM = True, then EM algorithm is used for estimation\n",
    "# delta is related to the variance of the betas. Delta -> 1 makes betas more volatile, which may lead to overfitting.\n",
    "# However, delta -> 0 may increase the MSE.\n",
    "\n",
    "def KFReg(X, y, delta, obs_cov, init_mean, init_cov, EM = False):\n",
    "    n_features = X.shape[1]\n",
    "    obs_mat = X[:, np.newaxis, :]\n",
    "    if EM == False:\n",
    "        trans_cov = (delta/(1 - delta))*np.eye(n_features)\n",
    "        kf = KalmanFilter(n_dim_obs = 1, n_dim_state = n_features, \n",
    "                          initial_state_mean = init_mean,\n",
    "                          initial_state_covariance = init_cov,\n",
    "                          transition_matrices = np.eye(n_features),\n",
    "                          observation_matrices = obs_mat,\n",
    "                          observation_covariance = obs_cov,\n",
    "                          transition_covariance = trans_cov)\n",
    "        state_means, state_covs = kf.filter(y)\n",
    "    else:\n",
    "        kf = KalmanFilter(n_dim_obs = 1, n_dim_state = n_features, \n",
    "                          initial_state_mean = init_mean, \n",
    "                          initial_state_covariance = init_cov,\n",
    "                          observation_matrices = obs_mat)\n",
    "    state_means, state_covs = kf.em(y).filter(y)\n",
    "    return state_means, state_covs, kf\n",
    "\n",
    "# Mean Absolute Error\n",
    "def MAE(y_obs, y_hat):\n",
    "    return np.mean(np.abs(y_obs - y_hat))\n",
    "\n",
    "# Mean Squared Error\n",
    "def MSE(y_obs, y_hat):\n",
    "    return np.mean((y_obs - y_hat)**2)\n",
    "\n",
    "# RMSE\n",
    "def RMSE(y_obs, y_hat):\n",
    "    return np.sqrt(MSE(y_obs, y_hat))\n",
    "\n",
    "def MAPE(y_obs, y_hat):\n",
    "    return np.mean(np.abs(y_obs - y_hat)/y_obs)\n",
    "\n",
    "def cos_sim(y_obs, y_hat):\n",
    "    return np.dot(y_obs, y_hat)/(np.linalg.norm(y_obs)*np.linalg.norm(y_hat))\n",
    "\n",
    "def cos_sim2(y_obs, y_hat):\n",
    "    return np.sum(y_obs*y_hat)/(np.linalg.norm(y_obs)*np.linalg.norm(y_hat))\n",
    "    \n",
    "def R2(y_obs, y_hat):\n",
    "    SSR = np.sum((y_obs - y_hat)**2)\n",
    "    SST = np.sum((y_obs - np.mean(y_obs))**2)\n",
    "    return (1 - SSR/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Data                                                                                                              #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# Raw data\n",
    "\n",
    "y = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'y.csv', index_col = 'Date', sep = ',')\n",
    "X = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X.csv', index_col = 'Date', sep = ',')\n",
    "\n",
    "X_L1 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L1.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L1.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L1'\n",
    "X_L1.columns = str_col\n",
    "\n",
    "X_L2 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L2.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L2.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L2'\n",
    "X_L2.columns = str_col\n",
    "\n",
    "X_L3 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L3.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L3.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L3'\n",
    "X_L3.columns = str_col\n",
    "\n",
    "X_L4 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L4.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L4.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L4'\n",
    "X_L4.columns = str_col\n",
    "\n",
    "X_L5 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L5.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L5.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L5'\n",
    "X_L5.columns = str_col\n",
    "\n",
    "X_L6 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L6.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L6.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L6'\n",
    "X_L6.columns = str_col\n",
    "\n",
    "X_L7 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L7.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L7.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L7'\n",
    "X_L7.columns = str_col\n",
    "\n",
    "X_L8 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L8.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L8.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L8'\n",
    "X_L8.columns = str_col\n",
    "\n",
    "X_L9 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L9.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L9.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L9'\n",
    "X_L9.columns = str_col\n",
    "\n",
    "X_L10 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L10.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L10.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L10'\n",
    "X_L10.columns = str_col\n",
    "\n",
    "X_L11 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L11.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L11.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L11'\n",
    "X_L11.columns = str_col\n",
    "\n",
    "X_L12 = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L12.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L12.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L12'\n",
    "X_L12.columns = str_col\n",
    "\n",
    "# Raw data - split samples - Train\n",
    "\n",
    "y_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'y_train.csv', index_col = 'Date', sep = ',')\n",
    "X_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_train.csv', index_col = 'Date', sep = ',')\n",
    "\n",
    "X_L1_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L1_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L1_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L1'\n",
    "X_L1_train.columns = str_col\n",
    "\n",
    "X_L2_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L2_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L2_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L2'\n",
    "X_L2_train.columns = str_col\n",
    "\n",
    "X_L3_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L3_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L3_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L3'\n",
    "X_L3_train.columns = str_col\n",
    "\n",
    "X_L4_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L4_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L4_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L4'\n",
    "X_L4_train.columns = str_col\n",
    "\n",
    "X_L5_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L5_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L5_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L5'\n",
    "X_L5_train.columns = str_col\n",
    "\n",
    "X_L6_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L6_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L6_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L6'\n",
    "X_L6_train.columns = str_col\n",
    "\n",
    "X_L7_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L7_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L7_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L7'\n",
    "X_L7_train.columns = str_col\n",
    "\n",
    "X_L8_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L8_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L8_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L8'\n",
    "X_L8_train.columns = str_col\n",
    "\n",
    "X_L9_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L9_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L9_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L9'\n",
    "X_L9_train.columns = str_col\n",
    "\n",
    "X_L10_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L10_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L10_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L10'\n",
    "X_L10_train.columns = str_col\n",
    "\n",
    "X_L11_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L11_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L11_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L11'\n",
    "X_L11_train.columns = str_col\n",
    "\n",
    "X_L12_train = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L12_train.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L12_train.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L12'\n",
    "X_L12_train.columns = str_col\n",
    "\n",
    "# Raw data - split samples - Test\n",
    "\n",
    "X_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_test.csv', index_col = 'Date', sep = ',')\n",
    "y_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'y_test.csv', index_col = 'Date', sep = ',')\n",
    "\n",
    "X_L1_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L1_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L1_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L1'\n",
    "X_L1_test.columns = str_col\n",
    "\n",
    "X_L2_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L2_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L2_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L2'\n",
    "X_L2_test.columns = str_col\n",
    "\n",
    "X_L3_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L3_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L3_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L3'\n",
    "X_L3_test.columns = str_col\n",
    "\n",
    "X_L4_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L4_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L4_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L4'\n",
    "X_L4_test.columns = str_col\n",
    "\n",
    "X_L5_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L5_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L5_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L5'\n",
    "X_L5_test.columns = str_col\n",
    "\n",
    "X_L6_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L6_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L6_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L6'\n",
    "X_L6_test.columns = str_col\n",
    "\n",
    "X_L7_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L7_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L7_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L7'\n",
    "X_L7_test.columns = str_col\n",
    "\n",
    "X_L8_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L8_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L8_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L8'\n",
    "X_L8_test.columns = str_col\n",
    "\n",
    "X_L9_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L9_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L9_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L9'\n",
    "X_L9_test.columns = str_col\n",
    "\n",
    "X_L10_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L10_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L10_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L10'\n",
    "X_L10_test.columns = str_col\n",
    "\n",
    "X_L11_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L11_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L11_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L11'\n",
    "X_L11_test.columns = str_col\n",
    "\n",
    "X_L12_test = pd.read_csv(filepath_or_buffer = str_Dir_Plan_Data + str(rnd_state) + ' ' + 'X_L12_test.csv', index_col = 'Date', sep = ',')\n",
    "str_col = X_L12_test.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L12'\n",
    "X_L12_test.columns = str_col\n",
    "\n",
    "# PCA samples - train\n",
    "\n",
    "X_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_pca_train' + '.csv', \n",
    "                          index_col = 'Date', sep = ',')\n",
    "\n",
    "X_OI_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_pca_train' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI'\n",
    "X_OI_train_pca.columns = str_col\n",
    "\n",
    "X_LM_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_pca_train' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM'\n",
    "X_LM_train_pca.columns = str_col\n",
    "\n",
    "X_H_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_pca_train' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_H_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H'\n",
    "X_H_train_pca.columns = str_col\n",
    "\n",
    "X_COI_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_pca_train' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI'\n",
    "X_COI_train_pca.columns = str_col\n",
    "\n",
    "X_MC_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_pca_train' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC'\n",
    "X_MC_train_pca.columns = str_col\n",
    "\n",
    "X_INTFX_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX'\n",
    "X_INTFX_train_pca.columns = str_col\n",
    "\n",
    "X_P_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_pca_train' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_P_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P'\n",
    "X_P_train_pca.columns = str_col\n",
    "\n",
    "X_S_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_pca_train' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_S_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S'\n",
    "X_S_train_pca.columns = str_col\n",
    "\n",
    "# PCA samples - test\n",
    "\n",
    "X_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_pca_test' + '.csv', \n",
    "                          index_col = 'Date', sep = ',')\n",
    "\n",
    "X_OI_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_pca_test' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI'\n",
    "X_OI_test_pca.columns = str_col\n",
    "\n",
    "X_LM_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_pca_test' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM'\n",
    "X_LM_test_pca.columns = str_col\n",
    "\n",
    "X_H_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_pca_test' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_H_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H'\n",
    "X_H_test_pca.columns = str_col\n",
    "\n",
    "X_COI_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_pca_test' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI'\n",
    "X_COI_test_pca.columns = str_col\n",
    "\n",
    "X_MC_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_pca_test' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC'\n",
    "X_MC_test_pca.columns = str_col\n",
    "\n",
    "X_INTFX_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX'\n",
    "X_INTFX_test_pca.columns = str_col\n",
    "\n",
    "X_P_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_pca_test' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_P_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P'\n",
    "X_P_test_pca.columns = str_col\n",
    "\n",
    "X_S_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_pca_test' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_S_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S'\n",
    "X_S_test_pca.columns = str_col\n",
    "\n",
    "# PCA samples - train L1\n",
    "\n",
    "X_L1_train_pca  = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_L1_pca_train' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_L1_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L1'\n",
    "X_L1_train_pca.columns = str_col\n",
    "\n",
    "X_OI_L1_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_L1_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_L1_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI_L1'\n",
    "X_OI_L1_train_pca.columns = str_col\n",
    "\n",
    "X_LM_L1_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_L1_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_L1_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM_L1'\n",
    "X_LM_L1_train_pca.columns = str_col\n",
    "\n",
    "X_H_L1_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_L1_pca_train' + '.csv',\n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_H_L1_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H_L1'\n",
    "X_H_L1_train_pca.columns = str_col\n",
    "\n",
    "X_COI_L1_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_L1_pca_train' + '.csv',\n",
    "                                 index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_L1_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI_L1'\n",
    "X_COI_L1_train_pca.columns = str_col\n",
    "\n",
    "X_MC_L1_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_L1_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_L1_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC_L1'\n",
    "X_MC_L1_train_pca.columns = str_col\n",
    "\n",
    "X_INTFX_L1_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_L1_pca_train' + '.csv', \n",
    "                                   index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_L1_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX_L1'\n",
    "X_INTFX_L1_train_pca.columns = str_col\n",
    "\n",
    "X_P_L1_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_L1_pca_train' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_P_L1_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P_L1'\n",
    "X_P_L1_train_pca.columns = str_col\n",
    "\n",
    "X_S_L1_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_L1_pca_train' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_S_L1_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S_L1'\n",
    "X_S_L1_train_pca.columns = str_col\n",
    "\n",
    "# PCA samples - test L1\n",
    "\n",
    "X_L1_test_pca  = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_L1_pca_test' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_L1_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L1'\n",
    "X_L1_test_pca.columns = str_col\n",
    "\n",
    "X_OI_L1_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_L1_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_L1_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI_L1'\n",
    "X_OI_L1_test_pca.columns = str_col\n",
    "\n",
    "X_LM_L1_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_L1_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_L1_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM_L1'\n",
    "X_LM_L1_test_pca.columns = str_col\n",
    "\n",
    "X_H_L1_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_L1_pca_test' + '.csv',\n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_H_L1_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H_L1'\n",
    "X_H_L1_test_pca.columns = str_col\n",
    "\n",
    "X_COI_L1_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_L1_pca_test' + '.csv',\n",
    "                                 index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_L1_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI_L1'\n",
    "X_COI_L1_test_pca.columns = str_col\n",
    "\n",
    "X_MC_L1_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_L1_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_L1_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC_L1'\n",
    "X_MC_L1_test_pca.columns = str_col\n",
    "\n",
    "X_INTFX_L1_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_L1_pca_test' + '.csv', \n",
    "                                   index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_L1_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX_L1'\n",
    "X_INTFX_L1_test_pca.columns = str_col\n",
    "\n",
    "X_P_L1_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_L1_pca_test' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_P_L1_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P_L1'\n",
    "X_P_L1_test_pca.columns = str_col\n",
    "\n",
    "X_S_L1_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_L1_pca_test' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_S_L1_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S_L1'\n",
    "X_S_L1_test_pca.columns = str_col\n",
    "\n",
    "# PCA samples - train L2\n",
    "\n",
    "X_L2_train_pca  = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_L2_pca_train' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_L2_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L2'\n",
    "X_L2_train_pca.columns = str_col\n",
    "\n",
    "X_OI_L2_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_L2_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_L2_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI_L2'\n",
    "X_OI_L2_train_pca.columns = str_col\n",
    "\n",
    "X_LM_L2_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_L2_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_L2_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM_L2'\n",
    "X_LM_L2_train_pca.columns = str_col\n",
    "\n",
    "X_H_L2_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_L2_pca_train' + '.csv',\n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_H_L2_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H_L2'\n",
    "X_H_L2_train_pca.columns = str_col\n",
    "\n",
    "X_COI_L2_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_L2_pca_train' + '.csv',\n",
    "                                 index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_L2_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI_L2'\n",
    "X_COI_L2_train_pca.columns = str_col\n",
    "\n",
    "X_MC_L2_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_L2_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_L2_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC_L2'\n",
    "X_MC_L2_train_pca.columns = str_col\n",
    "\n",
    "X_INTFX_L2_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_L2_pca_train' + '.csv', \n",
    "                                   index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_L2_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX_L2'\n",
    "X_INTFX_L2_train_pca.columns = str_col\n",
    "\n",
    "X_P_L2_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_L2_pca_train' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_P_L2_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P_L2'\n",
    "X_P_L2_train_pca.columns = str_col\n",
    "\n",
    "X_S_L2_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_L2_pca_train' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_S_L2_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S_L2'\n",
    "X_S_L2_train_pca.columns = str_col\n",
    "\n",
    "# PCA samples - test L2\n",
    "\n",
    "X_L2_test_pca  = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_L2_pca_test' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_L2_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L2'\n",
    "X_L2_test_pca.columns = str_col\n",
    "\n",
    "X_OI_L2_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_L2_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_L2_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI_L2'\n",
    "X_OI_L2_test_pca.columns = str_col\n",
    "\n",
    "X_LM_L2_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_L2_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_L2_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM_L2'\n",
    "X_LM_L2_test_pca.columns = str_col\n",
    "\n",
    "X_H_L2_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_L2_pca_test' + '.csv',\n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_H_L2_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H_L2'\n",
    "X_H_L2_test_pca.columns = str_col\n",
    "\n",
    "X_COI_L2_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_L2_pca_test' + '.csv',\n",
    "                                 index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_L2_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI_L2'\n",
    "X_COI_L2_test_pca.columns = str_col\n",
    "\n",
    "X_MC_L2_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_L2_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_L2_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC_L2'\n",
    "X_MC_L2_test_pca.columns = str_col\n",
    "\n",
    "X_INTFX_L2_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_L2_pca_test' + '.csv', \n",
    "                                   index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_L2_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX_L2'\n",
    "X_INTFX_L2_test_pca.columns = str_col\n",
    "\n",
    "X_P_L2_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_L2_pca_test' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_P_L2_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P_L2'\n",
    "X_P_L2_test_pca.columns = str_col\n",
    "\n",
    "X_S_L2_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_L2_pca_test' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_S_L2_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S_L2'\n",
    "X_S_L2_test_pca.columns = str_col\n",
    "\n",
    "# PCA samples - train L3\n",
    "\n",
    "X_L3_train_pca  = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_L3_pca_train' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_L3_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L3'\n",
    "X_L3_train_pca.columns = str_col\n",
    "\n",
    "X_OI_L3_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_L3_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_L3_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI_L3'\n",
    "X_OI_L3_train_pca.columns = str_col\n",
    "\n",
    "X_LM_L3_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_L3_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_L3_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM_L3'\n",
    "X_LM_L3_train_pca.columns = str_col\n",
    "\n",
    "X_H_L3_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_L3_pca_train' + '.csv',\n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_H_L3_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H_L3'\n",
    "X_H_L3_train_pca.columns = str_col\n",
    "\n",
    "X_COI_L3_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_L3_pca_train' + '.csv',\n",
    "                                 index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_L3_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI_L3'\n",
    "X_COI_L3_train_pca.columns = str_col\n",
    "\n",
    "X_MC_L3_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_L3_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_L3_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC_L3'\n",
    "X_MC_L3_train_pca.columns = str_col\n",
    "\n",
    "X_INTFX_L3_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_L3_pca_train' + '.csv', \n",
    "                                   index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_L3_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX_L3'\n",
    "X_INTFX_L3_train_pca.columns = str_col\n",
    "\n",
    "X_P_L3_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_L3_pca_train' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_P_L3_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P_L3'\n",
    "X_P_L3_train_pca.columns = str_col\n",
    "\n",
    "X_S_L3_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_L3_pca_train' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_S_L3_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S_L3'\n",
    "X_S_L3_train_pca.columns = str_col\n",
    "\n",
    "# PCA samples - test L3\n",
    "\n",
    "X_L3_test_pca  = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_L3_pca_test' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_L3_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L3'\n",
    "X_L3_test_pca.columns = str_col\n",
    "\n",
    "X_OI_L3_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_L3_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_L3_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI_L3'\n",
    "X_OI_L3_test_pca.columns = str_col\n",
    "\n",
    "X_LM_L3_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_L3_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_L3_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM_L3'\n",
    "X_LM_L3_test_pca.columns = str_col\n",
    "\n",
    "X_H_L3_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_L3_pca_test' + '.csv',\n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_H_L3_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H_L3'\n",
    "X_H_L3_test_pca.columns = str_col\n",
    "\n",
    "X_COI_L3_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_L3_pca_test' + '.csv',\n",
    "                                 index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_L3_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI_L3'\n",
    "X_COI_L3_test_pca.columns = str_col\n",
    "\n",
    "X_MC_L3_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_L3_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_L3_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC_L3'\n",
    "X_MC_L3_test_pca.columns = str_col\n",
    "\n",
    "X_INTFX_L3_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_L3_pca_test' + '.csv', \n",
    "                                   index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_L3_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX_L3'\n",
    "X_INTFX_L3_test_pca.columns = str_col\n",
    "\n",
    "X_P_L3_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_L3_pca_test' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_P_L3_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P_L3'\n",
    "X_P_L3_test_pca.columns = str_col\n",
    "\n",
    "X_S_L3_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_L3_pca_test' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_S_L3_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S_L3'\n",
    "X_S_L3_test_pca.columns = str_col\n",
    "\n",
    "# PCA samples - train L4\n",
    "\n",
    "X_L4_train_pca  = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_L4_pca_train' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_L4_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L4'\n",
    "X_L4_train_pca.columns = str_col\n",
    "\n",
    "X_OI_L4_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_L4_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_L4_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI_L4'\n",
    "X_OI_L4_train_pca.columns = str_col\n",
    "\n",
    "X_LM_L4_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_L4_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_L4_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM_L4'\n",
    "X_LM_L4_train_pca.columns = str_col\n",
    "\n",
    "X_H_L4_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_L4_pca_train' + '.csv',\n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_H_L4_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H_L4'\n",
    "X_H_L4_train_pca.columns = str_col\n",
    "\n",
    "X_COI_L4_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_L4_pca_train' + '.csv',\n",
    "                                 index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_L4_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI_L4'\n",
    "X_COI_L4_train_pca.columns = str_col\n",
    "\n",
    "X_MC_L4_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_L4_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_L4_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC_L4'\n",
    "X_MC_L4_train_pca.columns = str_col\n",
    "\n",
    "X_INTFX_L4_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_L4_pca_train' + '.csv', \n",
    "                                   index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_L4_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX_L4'\n",
    "X_INTFX_L4_train_pca.columns = str_col\n",
    "\n",
    "X_P_L4_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_L4_pca_train' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_P_L4_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P_L4'\n",
    "X_P_L4_train_pca.columns = str_col\n",
    "\n",
    "X_S_L4_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_L4_pca_train' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_S_L4_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S_L4'\n",
    "X_S_L4_train_pca.columns = str_col\n",
    "\n",
    "# PCA samples - test L4\n",
    "\n",
    "X_L4_test_pca  = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_L4_pca_test' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_L4_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L4'\n",
    "X_L4_test_pca.columns = str_col\n",
    "\n",
    "X_OI_L4_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_L4_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_L4_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI_L4'\n",
    "X_OI_L4_test_pca.columns = str_col\n",
    "\n",
    "X_LM_L4_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_L4_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_L4_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM_L4'\n",
    "X_LM_L4_test_pca.columns = str_col\n",
    "\n",
    "X_H_L4_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_L4_pca_test' + '.csv',\n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_H_L4_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H_L4'\n",
    "X_H_L4_test_pca.columns = str_col\n",
    "\n",
    "X_COI_L4_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_L4_pca_test' + '.csv',\n",
    "                                 index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_L4_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI_L4'\n",
    "X_COI_L4_test_pca.columns = str_col\n",
    "\n",
    "X_MC_L4_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_L4_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_L4_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC_L4'\n",
    "X_MC_L4_test_pca.columns = str_col\n",
    "\n",
    "X_INTFX_L4_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_L4_pca_test' + '.csv', \n",
    "                                   index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_L4_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX_L4'\n",
    "X_INTFX_L4_test_pca.columns = str_col\n",
    "\n",
    "X_P_L4_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_L4_pca_test' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_P_L4_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P_L4'\n",
    "X_P_L4_test_pca.columns = str_col\n",
    "\n",
    "X_S_L4_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_L4_pca_test' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_S_L4_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S_L4'\n",
    "X_S_L4_test_pca.columns = str_col\n",
    "\n",
    "# PCA samples - train L12\n",
    "\n",
    "X_L12_train_pca  = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_L12_pca_train' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_L12_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L12'\n",
    "X_L12_train_pca.columns = str_col\n",
    "\n",
    "X_OI_L12_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_L12_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_L12_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI_L12'\n",
    "X_OI_L12_train_pca.columns = str_col\n",
    "\n",
    "X_LM_L12_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_L12_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_L12_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM_L12'\n",
    "X_LM_L12_train_pca.columns = str_col\n",
    "\n",
    "X_H_L12_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_L12_pca_train' + '.csv',\n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_H_L12_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H_L12'\n",
    "X_H_L12_train_pca.columns = str_col\n",
    "\n",
    "X_COI_L12_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_L12_pca_train' + '.csv',\n",
    "                                 index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_L12_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI_L12'\n",
    "X_COI_L12_train_pca.columns = str_col\n",
    "\n",
    "X_MC_L12_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_L12_pca_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_L12_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC_L12'\n",
    "X_MC_L12_train_pca.columns = str_col\n",
    "\n",
    "X_INTFX_L12_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_L12_pca_train' + '.csv', \n",
    "                                   index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_L12_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX_L12'\n",
    "X_INTFX_L12_train_pca.columns = str_col\n",
    "\n",
    "X_P_L12_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_L12_pca_train' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_P_L12_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P_L12'\n",
    "X_P_L12_train_pca.columns = str_col\n",
    "\n",
    "X_S_L12_train_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_L12_pca_train' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_S_L12_train_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S_L12'\n",
    "X_S_L12_train_pca.columns = str_col\n",
    "\n",
    "# PCA samples - test L12\n",
    "\n",
    "X_L12_test_pca  = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_L12_pca_test' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_L12_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_L12'\n",
    "X_L12_test_pca.columns = str_col\n",
    "\n",
    "X_OI_L12_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_OI_L12_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_L12_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI_L12'\n",
    "X_OI_L12_test_pca.columns = str_col\n",
    "\n",
    "X_LM_L12_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_LM_L12_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_L12_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM_L12'\n",
    "X_LM_L12_test_pca.columns = str_col\n",
    "\n",
    "X_H_L12_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_H_L12_pca_test' + '.csv',\n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_H_L12_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H_L12'\n",
    "X_H_L12_test_pca.columns = str_col\n",
    "\n",
    "X_COI_L12_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_COI_L12_pca_test' + '.csv',\n",
    "                                 index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_L12_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI_L12'\n",
    "X_COI_L12_test_pca.columns = str_col\n",
    "\n",
    "X_MC_L12_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_MC_L12_pca_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_L12_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC_L12'\n",
    "X_MC_L12_test_pca.columns = str_col\n",
    "\n",
    "X_INTFX_L12_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_INTFX_L12_pca_test' + '.csv', \n",
    "                                   index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_L12_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX_L12'\n",
    "X_INTFX_L12_test_pca.columns = str_col\n",
    "\n",
    "X_P_L12_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_P_L12_pca_test' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_P_L12_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P_L12'\n",
    "X_P_L12_test_pca.columns = str_col\n",
    "\n",
    "X_S_L12_test_pca = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_S_L12_pca_test' + '.csv', \n",
    "                               index_col = 'Date', sep = ',')\n",
    "str_col = X_S_L12_test_pca.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S_L12'\n",
    "X_S_L12_test_pca.columns = str_col\n",
    "\n",
    "# Autoencoder samples - train\n",
    "\n",
    "X_train_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_ae_train' + '.csv', \n",
    "                          index_col = 'Date', sep = ',')\n",
    "\n",
    "X_OI_train_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_OI_ae_train' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_train_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI'\n",
    "X_OI_train_ae.columns = str_col\n",
    "\n",
    "X_LM_train_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_LM_ae_train' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_train_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM'\n",
    "X_LM_train_ae.columns = str_col\n",
    "\n",
    "X_H_train_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_H_ae_train' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_H_train_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H'\n",
    "X_H_train_ae.columns = str_col\n",
    "\n",
    "X_COI_train_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_COI_ae_train' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_train_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI'\n",
    "X_COI_train_ae.columns = str_col\n",
    "\n",
    "X_MC_train_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_MC_ae_train' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_train_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC'\n",
    "X_MC_train_ae.columns = str_col\n",
    "\n",
    "X_INTFX_train_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_INTFX_ae_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_train_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX'\n",
    "X_INTFX_train_ae.columns = str_col\n",
    "\n",
    "X_P_train_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_P_ae_train' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_P_train_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P'\n",
    "X_P_train_ae.columns = str_col\n",
    "\n",
    "X_S_train_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_S_ae_train' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_S_train_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S'\n",
    "X_S_train_ae.columns = str_col\n",
    "\n",
    "# Autoencoder samples - test\n",
    "\n",
    "X_test_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_ae_test' + '.csv', \n",
    "                          index_col = 'Date', sep = ',')\n",
    "\n",
    "X_OI_test_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_OI_ae_test' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_test_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI'\n",
    "X_OI_test_ae.columns = str_col\n",
    "\n",
    "X_LM_test_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_LM_ae_test' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_test_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM'\n",
    "X_LM_test_ae.columns = str_col\n",
    "\n",
    "X_H_test_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_H_ae_test' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_H_test_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H'\n",
    "X_H_test_ae.columns = str_col\n",
    "\n",
    "X_COI_test_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_COI_ae_test' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_test_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI'\n",
    "X_COI_test_ae.columns = str_col\n",
    "\n",
    "X_MC_test_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_MC_ae_test' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_test_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC'\n",
    "X_MC_test_ae.columns = str_col\n",
    "\n",
    "X_INTFX_test_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_INTFX_ae_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_test_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX'\n",
    "X_INTFX_test_ae.columns = str_col\n",
    "\n",
    "X_P_test_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_P_ae_test' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_P_test_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P'\n",
    "X_P_test_ae.columns = str_col\n",
    "\n",
    "X_S_test_ae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_S_ae_test' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_S_test_ae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S'\n",
    "X_S_test_ae.columns = str_col\n",
    "\n",
    "# Variational Autoencoder (VAE) samples - train\n",
    "\n",
    "X_train_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_vae_train' + '.csv', \n",
    "                          index_col = 'Date', sep = ',')\n",
    "\n",
    "X_OI_train_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_OI_vae_train' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_train_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI'\n",
    "X_OI_train_vae.columns = str_col\n",
    "\n",
    "X_LM_train_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_LM_vae_train' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_train_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM'\n",
    "X_LM_train_vae.columns = str_col\n",
    "\n",
    "X_H_train_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_H_vae_train' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_H_train_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H'\n",
    "X_H_train_vae.columns = str_col\n",
    "\n",
    "X_COI_train_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_COI_vae_train' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_train_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI'\n",
    "X_COI_train_vae.columns = str_col\n",
    "\n",
    "X_MC_train_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_MC_vae_train' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_train_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC'\n",
    "X_MC_train_vae.columns = str_col\n",
    "\n",
    "X_INTFX_train_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_INTFX_vae_train' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_train_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX'\n",
    "X_INTFX_train_vae.columns = str_col\n",
    "\n",
    "X_P_train_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_P_vae_train' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_P_train_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P'\n",
    "X_P_train_vae.columns = str_col\n",
    "\n",
    "X_S_train_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_S_vae_train' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_S_train_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S'\n",
    "X_S_train_vae.columns = str_col\n",
    "\n",
    "# Variational Autoencoder (VAE) samples - test\n",
    "\n",
    "X_test_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_full_vae_test' + '.csv', \n",
    "                          index_col = 'Date', sep = ',')\n",
    "\n",
    "X_OI_test_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_OI_vae_test' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_OI_test_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_OI'\n",
    "X_OI_test_vae.columns = str_col\n",
    "\n",
    "X_LM_test_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_LM_vae_test' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_LM_test_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_LM'\n",
    "X_LM_test_vae.columns = str_col\n",
    "\n",
    "X_H_test_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_H_vae_test' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_H_test_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_H'\n",
    "X_H_test_vae.columns = str_col\n",
    "\n",
    "X_COI_test_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_COI_vae_test' + '.csv', \n",
    "                              index_col = 'Date', sep = ',')\n",
    "str_col = X_COI_test_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_COI'\n",
    "X_COI_test_vae.columns = str_col\n",
    "\n",
    "X_MC_test_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_MC_vae_test' + '.csv', \n",
    "                             index_col = 'Date', sep = ',')\n",
    "str_col = X_MC_test_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_MC'\n",
    "X_MC_test_vae.columns = str_col\n",
    "\n",
    "X_INTFX_test_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_INTFX_vae_test' + '.csv', \n",
    "                                index_col = 'Date', sep = ',')\n",
    "str_col = X_INTFX_test_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_INTFX'\n",
    "X_INTFX_test_vae.columns = str_col\n",
    "\n",
    "X_P_test_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_P_vae_test' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_P_test_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_P'\n",
    "X_P_test_vae.columns = str_col\n",
    "\n",
    "X_S_test_vae = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_S_vae_test' + '.csv', \n",
    "                            index_col = 'Date', sep = ',')\n",
    "str_col = X_S_test_vae.columns.values\n",
    "for i in range(0,len(str_col)):\n",
    "    str_col[i] = str_col[i] + '_S'\n",
    "X_S_test_vae.columns = str_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Datasets for ML Models                                                                                            #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# Saves index\n",
    "index_full = y.index\n",
    "index_test = y_test.index\n",
    "index_train = y_train.index\n",
    "\n",
    "df_index = pd.DataFrame(data = range(0,len(index_full)), index = index_full)\n",
    "num_index_test = df_index.loc[index_test].iloc[:,0]\n",
    "num_index_train = df_index.loc[index_train].iloc[:,0]\n",
    "\n",
    "###########\n",
    "# LSTM v1 #\n",
    "###########\n",
    "\n",
    "# Data\n",
    "data_lstm_m1 = y.iloc[:,0]\n",
    "data_lstm_m1 = np.array(data_lstm_m1)\n",
    "\n",
    "# Split data into samples (input, output)\n",
    "X_lstm_m1, y_lstm_m1 = split_sequence_uni(data_lstm_m1, n_lags_lstm, per_ahead = 1, cum = False)\n",
    "\n",
    "# Indexation\n",
    "len1 = len(y_lstm_m1)\n",
    "len2 = len(y.index)\n",
    "i = y.index[(len2-len1):len2]\n",
    "\n",
    "X_lstm_m1 = pd.DataFrame(data = X_lstm_m1, index = i)\n",
    "X_lstm_m1_train = X_lstm_m1.loc[X_lstm_m1.index.intersection(index_train)]\n",
    "X_lstm_m1_test = X_lstm_m1.loc[X_lstm_m1.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m1 = pd.DataFrame(data = y_lstm_m1, index = i)\n",
    "y_lstm_m1_train = y_lstm_m1.loc[y_lstm_m1.index.intersection(index_train)]\n",
    "y_lstm_m1_test = y_lstm_m1.loc[y_lstm_m1.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m1_3m = y_lstm_m1.rolling(3).sum().dropna()\n",
    "y_lstm_m1_3m_train = y_lstm_m1_3m.loc[y_lstm_m1_3m.index.intersection(index_train)]\n",
    "y_lstm_m1_3m_test = y_lstm_m1_3m.loc[y_lstm_m1_3m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m1_6m = y_lstm_m1.rolling(6).sum().dropna()\n",
    "y_lstm_m1_6m_train = y_lstm_m1_6m.loc[y_lstm_m1_6m.index.intersection(index_train)]\n",
    "y_lstm_m1_6m_test = y_lstm_m1_6m.loc[y_lstm_m1_6m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m1_12m = y_lstm_m1.rolling(12).sum().dropna()\n",
    "y_lstm_m1_12m_train = y_lstm_m1_12m.loc[y_lstm_m1_12m.index.intersection(index_train)]\n",
    "y_lstm_m1_12m_test = y_lstm_m1_12m.loc[y_lstm_m1_12m.index.intersection(index_test)]\n",
    "\n",
    "# Converts into array\n",
    "X_lstm_m1 = np.array(X_lstm_m1)\n",
    "X_lstm_m1_train = np.array(X_lstm_m1_train)\n",
    "X_lstm_m1_test = np.array(X_lstm_m1_test)\n",
    "\n",
    "y_lstm_m1 = np.array(y_lstm_m1)[:,0]\n",
    "y_lstm_m1_train = np.array(y_lstm_m1_train)[:,0]\n",
    "y_lstm_m1_test = np.array(y_lstm_m1_test)[:,0]\n",
    "\n",
    "y_lstm_m1_3m = scale(np.array(y_lstm_m1_3m))[:,0]\n",
    "y_lstm_m1_3m_train = scale(np.array(y_lstm_m1_3m_train))[:,0]\n",
    "y_lstm_m1_3m_test = scale(np.array(y_lstm_m1_3m_test))[:,0]\n",
    "\n",
    "y_lstm_m1_6m = scale(np.array(y_lstm_m1_6m))[:,0]\n",
    "y_lstm_m1_6m_train = scale(np.array(y_lstm_m1_6m_train))[:,0]\n",
    "y_lstm_m1_6m_test = scale(np.array(y_lstm_m1_6m_test))[:,0]\n",
    "\n",
    "y_lstm_m1_12m = scale(np.array(y_lstm_m1_12m))[:,0]\n",
    "y_lstm_m1_12m_train = scale(np.array(y_lstm_m1_12m_train))[:,0]\n",
    "y_lstm_m1_12m_test = scale(np.array(y_lstm_m1_12m_test))[:,0]\n",
    "\n",
    "# Reshape\n",
    "n_features = 1\n",
    "X_lstm_m1 = X_lstm_m1.reshape(X_lstm_m1.shape[0], X_lstm_m1.shape[1], n_features)\n",
    "X_lstm_m1_train = X_lstm_m1_train.reshape(X_lstm_m1_train.shape[0], X_lstm_m1_train.shape[1], n_features)\n",
    "X_lstm_m1_test = X_lstm_m1_test.reshape(X_lstm_m1_test.shape[0], X_lstm_m1_test.shape[1], n_features)\n",
    "\n",
    "###########\n",
    "# LSTM v2 #\n",
    "###########\n",
    "\n",
    "# Data\n",
    "X_OI_ae = pd.concat([X_OI_train_ae, X_OI_test_ae], axis = 0)\n",
    "X_OI_ae = X_OI_ae.sort_values(by='Date')\n",
    "X_LM_ae = pd.concat([X_LM_train_ae, X_LM_test_ae], axis = 0)\n",
    "X_LM_ae = X_LM_ae.sort_values(by='Date')\n",
    "X_H_ae = pd.concat([X_H_train_ae, X_H_test_ae], axis = 0)\n",
    "X_H_ae = X_H_ae.sort_values(by='Date')\n",
    "X_COI_ae = pd.concat([X_COI_train_ae, X_COI_test_ae], axis = 0)\n",
    "X_COI_ae = X_COI_ae.sort_values(by='Date')\n",
    "X_MC_ae = pd.concat([X_MC_train_ae, X_MC_test_ae], axis = 0)\n",
    "X_MC_ae = X_MC_ae.sort_values(by='Date')\n",
    "X_INTFX_ae = pd.concat([X_INTFX_train_ae, X_INTFX_test_ae], axis = 0)\n",
    "X_INTFX_ae = X_INTFX_ae.sort_values(by='Date')\n",
    "X_P_ae = pd.concat([X_P_train_ae, X_P_test_ae], axis = 0)\n",
    "X_P_ae = X_P_ae.sort_values(by='Date')\n",
    "X_S_ae = pd.concat([X_S_train_ae, X_S_test_ae], axis = 0)\n",
    "X_S_ae = X_S_ae.sort_values(by='Date')\n",
    "\n",
    "# Concatenate\n",
    "data_lstm_m2 = pd.concat([X_OI_ae, X_LM_ae, X_H_ae, X_COI_ae, X_MC_ae, X_INTFX_ae, X_P_ae, X_S_ae, y], axis = 1)\n",
    "data_lstm_m2 = np.array(data_lstm_m2)\n",
    "\n",
    "# Split data into samples (input, output)\n",
    "X_lstm_m2, y_lstm_m2 = split_sequence_mult(data_lstm_m2, n_lags_lstm, per_ahead = 1, cum = False)\n",
    "\n",
    "# Indexation\n",
    "len1 = len(y_lstm_m2)\n",
    "len2 = len(y.index)\n",
    "i = y.index[(len2-len1):len2]\n",
    "\n",
    "y_lstm_m2 = pd.DataFrame(data = y_lstm_m2, index = i)\n",
    "df_index_adj = pd.DataFrame(data = range(0,len(y_lstm_m2)), index = y_lstm_m2.index)\n",
    "y_lstm_m2_train = y_lstm_m2.loc[y_lstm_m2.index.intersection(index_train)]\n",
    "y_lstm_m2_test = y_lstm_m2.loc[y_lstm_m2.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m2_3m = y_lstm_m2.rolling(3).sum().dropna()\n",
    "y_lstm_m2_3m_train = y_lstm_m2_3m.loc[y_lstm_m2_3m.index.intersection(index_train)]\n",
    "y_lstm_m2_3m_test = y_lstm_m2_3m.loc[y_lstm_m2_3m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m2_6m = y_lstm_m2.rolling(6).sum().dropna()\n",
    "y_lstm_m2_6m_train = y_lstm_m2_6m.loc[y_lstm_m2_6m.index.intersection(index_train)]\n",
    "y_lstm_m2_6m_test = y_lstm_m2_6m.loc[y_lstm_m2_6m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m2_12m = y_lstm_m2.rolling(12).sum().dropna()\n",
    "y_lstm_m2_12m_train = y_lstm_m2_12m.loc[y_lstm_m2_12m.index.intersection(index_train)]\n",
    "y_lstm_m2_12m_test = y_lstm_m2_12m.loc[y_lstm_m2_12m.index.intersection(index_test)]\n",
    "\n",
    "# Converts into array\n",
    "X_lstm_m2 = np.array(X_lstm_m2)\n",
    "X_lstm_m2_train = X_lstm_m2[df_index_adj.loc[y_lstm_m2_train.index][0], :, :]\n",
    "X_lstm_m2_test = X_lstm_m2[df_index_adj.loc[y_lstm_m2_test.index][0], :, :]\n",
    "\n",
    "y_lstm_m2 = np.array(y_lstm_m2)[:,0]\n",
    "y_lstm_m2_train = np.array(y_lstm_m2_train)[:,0]\n",
    "y_lstm_m2_test = np.array(y_lstm_m2_test)[:,0]\n",
    "\n",
    "y_lstm_m2_3m = scale(np.array(y_lstm_m2_3m))[:,0]\n",
    "y_lstm_m2_3m_train = scale(np.array(y_lstm_m2_3m_train))[:,0]\n",
    "y_lstm_m2_3m_test = scale(np.array(y_lstm_m2_3m_test))[:,0]\n",
    "\n",
    "y_lstm_m2_6m = scale(np.array(y_lstm_m2_6m))[:,0]\n",
    "y_lstm_m2_6m_train = scale(np.array(y_lstm_m2_6m_train))[:,0]\n",
    "y_lstm_m2_6m_test = scale(np.array(y_lstm_m2_6m_test))[:,0]\n",
    "\n",
    "y_lstm_m2_12m = scale(np.array(y_lstm_m2_12m))[:,0]\n",
    "y_lstm_m2_12m_train = scale(np.array(y_lstm_m2_12m_train))[:,0]\n",
    "y_lstm_m2_12m_test = scale(np.array(y_lstm_m2_12m_test))[:,0]\n",
    "\n",
    "###########\n",
    "# LSTM v3 #\n",
    "###########\n",
    "\n",
    "# Data\n",
    "\n",
    "data_lstm_m3 = pd.concat([X, y], axis = 1)\n",
    "data_lstm_m3 = np.array(data_lstm_m3)\n",
    "\n",
    "# Split data into samples (input, output)\n",
    "X_lstm_m3, y_lstm_m3 = split_sequence_mult(data_lstm_m3, n_lags_lstm, per_ahead = 1, cum = False)\n",
    "\n",
    "# Indexation\n",
    "len1 = len(y_lstm_m3)\n",
    "len2 = len(y.index)\n",
    "i = y.index[(len2-len1):len2]\n",
    "\n",
    "y_lstm_m3 = pd.DataFrame(data = y_lstm_m3, index = i)\n",
    "df_index_adj = pd.DataFrame(data = range(0,len(y_lstm_m3)), index = y_lstm_m3.index)\n",
    "y_lstm_m3_train = y_lstm_m3.loc[y_lstm_m3.index.intersection(index_train)]\n",
    "y_lstm_m3_test = y_lstm_m3.loc[y_lstm_m3.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m3_3m = y_lstm_m3.rolling(3).sum().dropna()\n",
    "y_lstm_m3_3m_train = y_lstm_m3_3m.loc[y_lstm_m3_3m.index.intersection(index_train)]\n",
    "y_lstm_m3_3m_test = y_lstm_m3_3m.loc[y_lstm_m3_3m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m3_6m = y_lstm_m3.rolling(6).sum().dropna()\n",
    "y_lstm_m3_6m_train = y_lstm_m3_6m.loc[y_lstm_m3_6m.index.intersection(index_train)]\n",
    "y_lstm_m3_6m_test = y_lstm_m3_6m.loc[y_lstm_m3_6m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m3_12m = y_lstm_m3.rolling(12).sum().dropna()\n",
    "y_lstm_m3_12m_train = y_lstm_m3_12m.loc[y_lstm_m3_12m.index.intersection(index_train)]\n",
    "y_lstm_m3_12m_test = y_lstm_m3_12m.loc[y_lstm_m3_12m.index.intersection(index_test)]\n",
    "\n",
    "# Converts into array\n",
    "X_lstm_m3 = np.array(X_lstm_m3)\n",
    "X_lstm_m3_train = X_lstm_m3[df_index_adj.loc[y_lstm_m3_train.index][0], :, :]\n",
    "X_lstm_m3_test = X_lstm_m3[df_index_adj.loc[y_lstm_m3_test.index][0], :, :]\n",
    "\n",
    "y_lstm_m3 = np.array(y_lstm_m3)[:,0]\n",
    "y_lstm_m3_train = np.array(y_lstm_m3_train)[:,0]\n",
    "y_lstm_m3_test = np.array(y_lstm_m3_test)[:,0]\n",
    "\n",
    "y_lstm_m3_3m = scale(np.array(y_lstm_m3_3m))[:,0]\n",
    "y_lstm_m3_3m_train = scale(np.array(y_lstm_m3_3m_train))[:,0]\n",
    "y_lstm_m3_3m_test = scale(np.array(y_lstm_m3_3m_test))[:,0]\n",
    "\n",
    "y_lstm_m3_6m = scale(np.array(y_lstm_m3_6m))[:,0]\n",
    "y_lstm_m3_6m_train = scale(np.array(y_lstm_m3_6m_train))[:,0]\n",
    "y_lstm_m3_6m_test = scale(np.array(y_lstm_m3_6m_test))[:,0]\n",
    "\n",
    "y_lstm_m3_12m = scale(np.array(y_lstm_m3_12m))[:,0]\n",
    "y_lstm_m3_12m_train = scale(np.array(y_lstm_m3_12m_train))[:,0]\n",
    "y_lstm_m3_12m_test = scale(np.array(y_lstm_m3_12m_test))[:,0]\n",
    "\n",
    "###########\n",
    "# LSTM v4 #\n",
    "###########\n",
    "\n",
    "# Data\n",
    "X_OI_ae = pd.concat([X_OI_train_ae, X_OI_test_ae], axis = 0)\n",
    "X_OI_ae = X_OI_ae.sort_values(by='Date')\n",
    "X_LM_ae = pd.concat([X_LM_train_ae, X_LM_test_ae], axis = 0)\n",
    "X_LM_ae = X_LM_ae.sort_values(by='Date')\n",
    "X_H_ae = pd.concat([X_H_train_ae, X_H_test_ae], axis = 0)\n",
    "X_H_ae = X_H_ae.sort_values(by='Date')\n",
    "X_COI_ae = pd.concat([X_COI_train_ae, X_COI_test_ae], axis = 0)\n",
    "X_COI_ae = X_COI_ae.sort_values(by='Date')\n",
    "X_MC_ae = pd.concat([X_MC_train_ae, X_MC_test_ae], axis = 0)\n",
    "X_MC_ae = X_MC_ae.sort_values(by='Date')\n",
    "X_INTFX_ae = pd.concat([X_INTFX_train_ae, X_INTFX_test_ae], axis = 0)\n",
    "X_INTFX_ae = X_INTFX_ae.sort_values(by='Date')\n",
    "X_P_ae = pd.concat([X_P_train_ae, X_P_test_ae], axis = 0)\n",
    "X_P_ae = X_P_ae.sort_values(by='Date')\n",
    "X_S_ae = pd.concat([X_S_train_ae, X_S_test_ae], axis = 0)\n",
    "X_S_ae = X_S_ae.sort_values(by='Date')\n",
    "\n",
    "data_lstm_m4 = pd.concat([X_OI_ae, X_LM_ae, X_H_ae, X_COI_ae, X_MC_ae, X_INTFX_ae, X_P_ae, X_S_ae, y], axis = 1)\n",
    "data_lstm_m4 = np.array(data_lstm_m4)\n",
    "\n",
    "# Split data into samples (input, output)\n",
    "X_lstm_m4, y_lstm_m4 = split_sequence_mult(data_lstm_m4, n_lags_lstm, per_ahead = 1, cum = False)\n",
    "\n",
    "# Indexation\n",
    "len1 = len(y_lstm_m4)\n",
    "len2 = len(y.index)\n",
    "i = y.index[(len2-len1):len2]\n",
    "\n",
    "y_lstm_m4 = pd.DataFrame(data = y_lstm_m4, index = i)\n",
    "df_index_adj = pd.DataFrame(data = range(0,len(y_lstm_m4)), index = y_lstm_m4.index)\n",
    "y_lstm_m4_train = y_lstm_m4.loc[y_lstm_m4.index.intersection(index_train)]\n",
    "y_lstm_m4_test = y_lstm_m4.loc[y_lstm_m4.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m4_3m = y_lstm_m4.rolling(3).sum().dropna()\n",
    "y_lstm_m4_3m_train = y_lstm_m4_3m.loc[y_lstm_m4_3m.index.intersection(index_train)]\n",
    "y_lstm_m4_3m_test = y_lstm_m4_3m.loc[y_lstm_m4_3m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m4_6m = y_lstm_m4.rolling(6).sum().dropna()\n",
    "y_lstm_m4_6m_train = y_lstm_m4_6m.loc[y_lstm_m4_6m.index.intersection(index_train)]\n",
    "y_lstm_m4_6m_test = y_lstm_m4_6m.loc[y_lstm_m4_6m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m4_12m = y_lstm_m4.rolling(12).sum().dropna()\n",
    "y_lstm_m4_12m_train = y_lstm_m4_12m.loc[y_lstm_m4_12m.index.intersection(index_train)]\n",
    "y_lstm_m4_12m_test = y_lstm_m4_12m.loc[y_lstm_m4_12m.index.intersection(index_test)]\n",
    "\n",
    "# Reshapes and converts into array\n",
    "X_lstm_m4 = np.array(X_lstm_m4)\n",
    "X_lstm_m4 = X_lstm_m4.reshape(X_lstm_m4.shape[0], n_seq_conv, 1, n_steps_conv, X_lstm_m4.shape[2])\n",
    "X_lstm_m4_train = X_lstm_m4[df_index_adj.loc[y_lstm_m4_train.index][0], :, :, :, :]\n",
    "X_lstm_m4_test = X_lstm_m4[df_index_adj.loc[y_lstm_m4_test.index][0], :, :, :, :]\n",
    "\n",
    "y_lstm_m4 = np.array(y_lstm_m4)[:,0]\n",
    "y_lstm_m4_train = np.array(y_lstm_m4_train)[:,0]\n",
    "y_lstm_m4_test = np.array(y_lstm_m4_test)[:,0]\n",
    "\n",
    "y_lstm_m4_3m = scale(np.array(y_lstm_m4_3m))[:,0]\n",
    "y_lstm_m4_3m_train = scale(np.array(y_lstm_m4_3m_train))[:,0]\n",
    "y_lstm_m4_3m_test = scale(np.array(y_lstm_m4_3m_test))[:,0]\n",
    "\n",
    "y_lstm_m4_6m = scale(np.array(y_lstm_m4_6m))[:,0]\n",
    "y_lstm_m4_6m_train = scale(np.array(y_lstm_m4_6m_train))[:,0]\n",
    "y_lstm_m4_6m_test = scale(np.array(y_lstm_m4_6m_test))[:,0]\n",
    "\n",
    "y_lstm_m4_12m = scale(np.array(y_lstm_m4_12m))[:,0]\n",
    "y_lstm_m4_12m_train = scale(np.array(y_lstm_m4_12m_train))[:,0]\n",
    "y_lstm_m4_12m_test = scale(np.array(y_lstm_m4_12m_test))[:,0]\n",
    "\n",
    "###########\n",
    "# LSTM v5 #\n",
    "###########\n",
    "\n",
    "# Data\n",
    "X_OI_vae = pd.concat([X_OI_train_vae, X_OI_test_vae], axis = 0)\n",
    "X_OI_vae = X_OI_vae.sort_values(by='Date')\n",
    "X_LM_vae = pd.concat([X_LM_train_vae, X_LM_test_vae], axis = 0)\n",
    "X_LM_vae = X_LM_vae.sort_values(by='Date')\n",
    "X_H_vae = pd.concat([X_H_train_vae, X_H_test_vae], axis = 0)\n",
    "X_H_vae = X_H_vae.sort_values(by='Date')\n",
    "X_COI_vae = pd.concat([X_COI_train_vae, X_COI_test_vae], axis = 0)\n",
    "X_COI_vae = X_COI_vae.sort_values(by='Date')\n",
    "X_MC_vae = pd.concat([X_MC_train_vae, X_MC_test_vae], axis = 0)\n",
    "X_MC_vae = X_MC_vae.sort_values(by='Date')\n",
    "X_INTFX_vae = pd.concat([X_INTFX_train_vae, X_INTFX_test_vae], axis = 0)\n",
    "X_INTFX_vae = X_INTFX_vae.sort_values(by='Date')\n",
    "X_P_vae = pd.concat([X_P_train_vae, X_P_test_vae], axis = 0)\n",
    "X_P_vae = X_P_vae.sort_values(by='Date')\n",
    "X_S_vae = pd.concat([X_S_train_vae, X_S_test_vae], axis = 0)\n",
    "X_S_vae = X_S_vae.sort_values(by='Date')\n",
    "\n",
    "data_lstm_m5 = pd.concat([X_OI_vae, X_LM_vae, X_H_vae, X_COI_vae, X_MC_vae, X_INTFX_vae, X_P_vae, X_S_vae, y], axis = 1)\n",
    "data_lstm_m5 = np.array(data_lstm_m5)\n",
    "\n",
    "# Split data into samples (input, output)\n",
    "X_lstm_m5, y_lstm_m5 = split_sequence_mult(data_lstm_m5, n_lags_lstm, per_ahead = 1, cum = False)\n",
    "\n",
    "# Indexation\n",
    "len1 = len(y_lstm_m5)\n",
    "len2 = len(y.index)\n",
    "i = y.index[(len2-len1):len2]\n",
    "\n",
    "y_lstm_m5 = pd.DataFrame(data = y_lstm_m5, index = i)\n",
    "df_index_adj = pd.DataFrame(data = range(0,len(y_lstm_m5)), index = y_lstm_m5.index)\n",
    "y_lstm_m5_train = y_lstm_m5.loc[y_lstm_m5.index.intersection(index_train)]\n",
    "y_lstm_m5_test = y_lstm_m5.loc[y_lstm_m5.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m5_3m = y_lstm_m5.rolling(3).mean().dropna()\n",
    "y_lstm_m5_3m_train = y_lstm_m5_3m.loc[y_lstm_m5_3m.index.intersection(index_train)]\n",
    "y_lstm_m5_3m_test = y_lstm_m5_3m.loc[y_lstm_m5_3m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m5_6m = y_lstm_m5.rolling(6).mean().dropna()\n",
    "y_lstm_m5_6m_train = y_lstm_m5_6m.loc[y_lstm_m5_6m.index.intersection(index_train)]\n",
    "y_lstm_m5_6m_test = y_lstm_m5_6m.loc[y_lstm_m5_6m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m5_12m = y_lstm_m5.rolling(12).mean().dropna()\n",
    "y_lstm_m5_12m_train = y_lstm_m5_12m.loc[y_lstm_m5_12m.index.intersection(index_train)]\n",
    "y_lstm_m5_12m_test = y_lstm_m5_12m.loc[y_lstm_m5_12m.index.intersection(index_test)]\n",
    "\n",
    "# Reshapes and converts into array\n",
    "X_lstm_m5 = np.array(X_lstm_m5)\n",
    "X_lstm_m5 = X_lstm_m5.reshape(X_lstm_m5.shape[0], n_seq_conv, 1, n_steps_conv, X_lstm_m5.shape[2])\n",
    "X_lstm_m5_train = X_lstm_m5[df_index_adj.loc[y_lstm_m5_train.index][0], :, :, :, :]\n",
    "X_lstm_m5_test = X_lstm_m5[df_index_adj.loc[y_lstm_m5_test.index][0], :, :, :, :]\n",
    "\n",
    "y_lstm_m5 = np.array(y_lstm_m5)[:,0]\n",
    "y_lstm_m5_train = np.array(y_lstm_m5_train)[:,0]\n",
    "y_lstm_m5_test = np.array(y_lstm_m5_test)[:,0]\n",
    "\n",
    "y_lstm_m5_3m = scale(np.array(y_lstm_m5_3m))[:,0]\n",
    "y_lstm_m5_3m_train = scale(np.array(y_lstm_m5_3m_train))[:,0]\n",
    "y_lstm_m5_3m_test = scale(np.array(y_lstm_m5_3m_test))[:,0]\n",
    "\n",
    "y_lstm_m5_6m = scale(np.array(y_lstm_m5_6m))[:,0]\n",
    "y_lstm_m5_6m_train = scale(np.array(y_lstm_m5_6m_train))[:,0]\n",
    "y_lstm_m5_6m_test = scale(np.array(y_lstm_m5_6m_test))[:,0]\n",
    "\n",
    "y_lstm_m5_12m = scale(np.array(y_lstm_m5_12m))[:,0]\n",
    "y_lstm_m5_12m_train = scale(np.array(y_lstm_m5_12m_train))[:,0]\n",
    "y_lstm_m5_12m_test = scale(np.array(y_lstm_m5_12m_test))[:,0]\n",
    "\n",
    "###########\n",
    "# LSTM v6 #\n",
    "###########\n",
    "\n",
    "# Data\n",
    "data_lstm_m6 = y.iloc[:,0]\n",
    "data_lstm_m6 = np.array(data_lstm_m1)\n",
    "\n",
    "# Split data into samples (input, output)\n",
    "X_lstm_m6, y_lstm_m6 = split_sequence_uni(data_lstm_m6, n_lags_lstm, per_ahead = 1, cum = False)\n",
    "\n",
    "# Indexation\n",
    "len1 = len(y_lstm_m6)\n",
    "len2 = len(y.index)\n",
    "i = y.index[(len2-len1):len2]\n",
    "\n",
    "y_lstm_m6 = pd.DataFrame(data = y_lstm_m6, index = i)\n",
    "df_index_adj = pd.DataFrame(data = range(0,len(y_lstm_m6)), index = y_lstm_m6.index)\n",
    "y_lstm_m6_train = y_lstm_m6.loc[y_lstm_m6.index.intersection(index_train)]\n",
    "y_lstm_m6_test = y_lstm_m6.loc[y_lstm_m6.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m6_3m = y_lstm_m6.rolling(3).sum().dropna()\n",
    "y_lstm_m6_3m_train = y_lstm_m6_3m.loc[y_lstm_m6_3m.index.intersection(index_train)]\n",
    "y_lstm_m6_3m_test = y_lstm_m6_3m.loc[y_lstm_m6_3m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m6_6m = y_lstm_m6.rolling(6).sum().dropna()\n",
    "y_lstm_m6_6m_train = y_lstm_m6_6m.loc[y_lstm_m6_6m.index.intersection(index_train)]\n",
    "y_lstm_m6_6m_test = y_lstm_m6_6m.loc[y_lstm_m6_6m.index.intersection(index_test)]\n",
    "\n",
    "y_lstm_m6_12m = y_lstm_m6.rolling(12).sum().dropna()\n",
    "y_lstm_m6_12m_train = y_lstm_m6_12m.loc[y_lstm_m6_12m.index.intersection(index_train)]\n",
    "y_lstm_m6_12m_test = y_lstm_m6_12m.loc[y_lstm_m6_12m.index.intersection(index_test)]\n",
    "\n",
    "# Reshapes and converts into array\n",
    "X_lstm_m6 = np.array(X_lstm_m6)\n",
    "X_lstm_m6 = X_lstm_m6.reshape(X_lstm_m6.shape[0], n_seq_conv, 1, n_steps_conv, 1)\n",
    "X_lstm_m6_train = X_lstm_m6[df_index_adj.loc[y_lstm_m6_train.index][0], :, :, :, :]\n",
    "X_lstm_m6_test = X_lstm_m6[df_index_adj.loc[y_lstm_m6_test.index][0], :, :, :, :]\n",
    "\n",
    "y_lstm_m6 = np.array(y_lstm_m6)[:,0]\n",
    "y_lstm_m6_train = np.array(y_lstm_m6_train)[:,0]\n",
    "y_lstm_m6_test = np.array(y_lstm_m6_test)[:,0]\n",
    "\n",
    "y_lstm_m6_3m = scale(np.array(y_lstm_m6_3m))[:,0]\n",
    "y_lstm_m6_3m_train = scale(np.array(y_lstm_m6_3m_train))[:,0]\n",
    "y_lstm_m6_3m_test = scale(np.array(y_lstm_m6_3m_test))[:,0]\n",
    "\n",
    "y_lstm_m6_6m = scale(np.array(y_lstm_m6_6m))[:,0]\n",
    "y_lstm_m6_6m_train = scale(np.array(y_lstm_m6_6m_train))[:,0]\n",
    "y_lstm_m6_6m_test = scale(np.array(y_lstm_m6_6m_test))[:,0]\n",
    "\n",
    "y_lstm_m6_12m = scale(np.array(y_lstm_m6_12m))[:,0]\n",
    "y_lstm_m6_12m_train = scale(np.array(y_lstm_m6_12m_train))[:,0]\n",
    "y_lstm_m6_12m_test = scale(np.array(y_lstm_m6_12m_test))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Models                                                                                                            #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "#################################\n",
    "# Storage variables             #\n",
    "#################################\n",
    "\n",
    "# Column names - periods ahead\n",
    "col_names = ['p0', 'p1', 'p2', 'p3', 'p4', 'p5', 'p12', '3m', '6m', '12m']\n",
    "periods = [0, 1, 2, 3, 4, 5, 12, 2, 5, 11]\n",
    "models_names = ['LSTM_M1', 'LSTM_M2', 'LSTM_M3', 'ConvLSTM_M1', 'ConvLSTM_M2', 'ConvLSTM_M3', 'MLP', 'MLP2', 'RW', \n",
    "                'Ridge', 'Ridge_CV', 'Bayesian_Ridge', 'Lasso', 'Lasso_CV', 'Bayesian_Lasso', 'ENet', 'SVR', \n",
    "                'Random_Forest', 'BART', 'BAGGING', 'kNN', 'Huber', 'Theil_Sen', 'Factor', 'GARCH', 'VECM', 'SETAR',\n",
    "                'MA', 'SARIMA', 'ARFIMA', 'GradBoost', 'AdaBoost', 'BayesRegression']\n",
    "\n",
    "# Matrices that store fitted values for each model (training and test samples)\n",
    "df_y_fit_lstm_m1 = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_lstm_m1 = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_lstm_m2 = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_lstm_m2 = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_lstm_m3 = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_lstm_m3 = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_lstm_m4 = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_lstm_m4 = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_lstm_m5 = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_lstm_m5 = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_lstm_m6 = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_lstm_m6 = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_mlp = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_mlp = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_mlp2 = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_mlp2 = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_rw = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_rw = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_ridge = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_ridge = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_ridge_cv = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_ridge_cv = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_bridge = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_bridge = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_lasso = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_lasso = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_lasso_cv = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_lasso_cv = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_blasso = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_blasso = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_enet = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_enet = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_svr = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_svr = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_rf = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_rf = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_bart = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_bart = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_bagging = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_bagging = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_var = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_var = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_ma = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_ma = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_sarima = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_sarima = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_huber = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_huber = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_ts = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_ts = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_factor = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_factor = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_gradboost = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_gradboost = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_adaboost = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_adaboost = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_garch = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_garch = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_vecm = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_vecm = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_setar = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_setar = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_arfima = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_arfima = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_bayesreg = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_bayesreg = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "df_y_fit_knn = pd.DataFrame(data = np.zeros([len(index_train), len(col_names)]).fill(np.nan), \n",
    "                             index = index_train, columns = col_names)\n",
    "df_y_hat_knn = pd.DataFrame(data = np.zeros([len(index_test), len(col_names)]).fill(np.nan), \n",
    "                             index = index_test, columns = col_names)\n",
    "\n",
    "# Matrices that save performance metrics\n",
    "df_MSE = pd.DataFrame(data = np.zeros([len(models_names), len(col_names)]), index = models_names, columns = col_names)\n",
    "df_MAE = pd.DataFrame(data = np.zeros([len(models_names), len(col_names)]), index = models_names, columns = col_names)\n",
    "df_RMSE = pd.DataFrame(data = np.zeros([len(models_names), len(col_names)]), index = models_names, columns = col_names)\n",
    "df_MAPE = pd.DataFrame(data = np.zeros([len(models_names), len(col_names)]), index = models_names, columns = col_names)\n",
    "df_CS = pd.DataFrame(data = np.zeros([len(models_names), len(col_names)]), index = models_names, columns = col_names)\n",
    "\n",
    "#################################\n",
    "# LSTM with past inflation only #\n",
    "#################################\n",
    "\n",
    "# Number of features\n",
    "n_features = 1\n",
    "\n",
    "# Define the model\n",
    "\n",
    "def lstm_m1_gen():\n",
    "    lstm_m1 = Sequential()\n",
    "    lstm_m1.add(LSTM(50, activation = act_fun, \n",
    "                     return_sequences=True, \n",
    "                     kernel_initializer = tf.keras.initializers.LecunNormal,\n",
    "                     input_shape=(n_lags_lstm, n_features)))\n",
    "    lstm_m1.add(LSTM(50, activation = act_fun,\n",
    "                     kernel_initializer = tf.keras.initializers.LecunNormal))\n",
    "    lstm_m1.add(Dense(1))\n",
    "    lstm_m1.compile(optimizer='adam', loss='mse',\n",
    "                    metrics = ['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), \n",
    "                               tf.keras.metrics.MeanAbsolutePercentageError(), 'cosine_similarity'])\n",
    "    return lstm_m1\n",
    "\n",
    "lstm_m1 = lstm_m1_gen()\n",
    "\n",
    "# Fit model\n",
    "hist_lstm_m1 = lstm_m1.fit(X_lstm_m1_train, y_lstm_m1_train, \n",
    "                           epochs=num_epochs,\n",
    "                           # validation_data = (X_lstm_m1_test, y_lstm_m1_test),\n",
    "                           validation_split = 0.1,\n",
    "                           verbose=False)\n",
    "\n",
    "# Fitted values\n",
    "y_fit_lstm_m1 = lstm_m1.predict(X_lstm_m1_train)[:,0]\n",
    "y_hat_lstm_m1 = lstm_m1.predict(X_lstm_m1_test)[:,0]\n",
    "df_y_fit_lstm_m1['p0'][-len(y_lstm_m1_train):] = y_fit_lstm_m1\n",
    "df_y_hat_lstm_m1['p0'][-len(y_lstm_m1_test):] = y_hat_lstm_m1\n",
    "\n",
    "# Plots fitted values x observed values - training sample\n",
    "y_fit_lstm_m1 = y_fit_lstm_m1.reshape(y_fit_lstm_m1.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m1_train, y_fit_lstm_m1])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plots fitted values x observed values - test sample\n",
    "y_hat_lstm_m1 = y_hat_lstm_m1.reshape(y_hat_lstm_m1.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m1_test, y_hat_lstm_m1])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plots loss function\n",
    "plt.plot(hist_lstm_m1.history['loss'])\n",
    "plt.plot(hist_lstm_m1.history['val_loss'])\n",
    "plt.title('LSTM - Loss (MSE)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Saves metrics\n",
    "df_MSE.loc['LSTM_M1','p0'] = np.min(hist_lstm_m1.history['val_mse'])\n",
    "df_MAE.loc['LSTM_M1','p0'] = np.min(hist_lstm_m1.history['val_mae'])\n",
    "df_RMSE.loc['LSTM_M1','p0'] = np.min(hist_lstm_m1.history['val_root_mean_squared_error'])\n",
    "df_MAPE.loc['LSTM_M1','p0'] = np.min(hist_lstm_m1.history['val_mean_absolute_percentage_error'])\n",
    "df_CS.loc['LSTM_M1','p0'] = np.min(hist_lstm_m1.history['val_cosine_similarity'])\n",
    "\n",
    "# Fits models for every period\n",
    "k = 0\n",
    "str_model = 'LSTM_M1'\n",
    "for p in col_names:\n",
    "    \n",
    "    if k > 0: # starts at p1 because p0 has been already executed\n",
    "        \n",
    "        shift = periods[k]\n",
    "        lstm_m1 = lstm_m1_gen()\n",
    "        \n",
    "        if p == '3m':\n",
    "            hist_lstm_m1 = lstm_m1.fit(X_lstm_m1_train[:-shift,:,:], y_lstm_m1_3m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '6m':\n",
    "            hist_lstm_m1 = lstm_m1.fit(X_lstm_m1_train[:-shift,:,:], y_lstm_m1_6m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '12m':\n",
    "            hist_lstm_m1 = lstm_m1.fit(X_lstm_m1_train[:-shift,:,:], y_lstm_m1_12m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        else:\n",
    "            hist_lstm_m1 = lstm_m1.fit(X_lstm_m1_train[:-shift,:,:], y_lstm_m1_train[shift:], \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        \n",
    "        df_y_fit_lstm_m1[p][(-len(y_lstm_m1_train)+shift):] = lstm_m1.predict(X_lstm_m1_train[:-shift,:,:])[:,0]\n",
    "        df_y_hat_lstm_m1[p][(-len(y_lstm_m1_test)+shift):] = lstm_m1.predict(X_lstm_m1_test[:-shift,:,:])[:,0]\n",
    "        \n",
    "        df_MSE.loc[str_model, p] = np.min(hist_lstm_m1.history['val_mse'])\n",
    "        df_MAE.loc[str_model, p] = np.min(hist_lstm_m1.history['val_mae'])\n",
    "        df_RMSE.loc[str_model, p] = np.min(hist_lstm_m1.history['val_root_mean_squared_error'])\n",
    "        df_MAPE.loc[str_model, p] = np.min(hist_lstm_m1.history['val_mean_absolute_percentage_error'])\n",
    "        df_CS.loc[str_model, p] = np.min(hist_lstm_m1.history['val_cosine_similarity'])\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# LSTM with autoencoder         #\n",
    "#################################\n",
    "\n",
    "# Number of features\n",
    "n_features = X_lstm_m2.shape[2]\n",
    "\n",
    "# Define the model\n",
    "def lstm_m2_gen():\n",
    "    lstm_m2 = Sequential()\n",
    "    lstm_m2.add(LSTM(50, activation = act_fun, \n",
    "                     return_sequences=True, \n",
    "                     kernel_initializer = tf.keras.initializers.LecunNormal,\n",
    "                     input_shape=(n_lags_lstm, n_features)))\n",
    "    lstm_m2.add(LSTM(50, activation = act_fun, \n",
    "                     kernel_initializer = tf.keras.initializers.LecunNormal))\n",
    "    lstm_m2.add(Dense(1))\n",
    "    lstm_m2.compile(optimizer='Nadam', loss='mse', \n",
    "                    metrics = ['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), \n",
    "                               tf.keras.metrics.MeanAbsolutePercentageError(), 'cosine_similarity'])\n",
    "    return lstm_m2\n",
    "\n",
    "lstm_m2 = lstm_m2_gen()\n",
    "\n",
    "# Fit model\n",
    "hist_lstm_m2 = lstm_m2.fit(X_lstm_m2_train, y_lstm_m2_train, \n",
    "                           epochs=num_epochs,\n",
    "                           validation_split = 0.1,\n",
    "                           verbose=False)\n",
    "\n",
    "# Fitted values\n",
    "y_fit_lstm_m2 = lstm_m2.predict(X_lstm_m2_train)[:,0]\n",
    "y_hat_lstm_m2 = lstm_m2.predict(X_lstm_m2_test)[:,0]\n",
    "df_y_fit_lstm_m2['p0'][-len(y_lstm_m2_train):] = y_fit_lstm_m2\n",
    "df_y_hat_lstm_m2['p0'][-len(y_lstm_m2_test):] = y_hat_lstm_m2\n",
    "\n",
    "# Plots fitted values x observed values - training sample\n",
    "y_fit_lstm_m2 = y_fit_lstm_m2.reshape(y_fit_lstm_m2.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m2_train, y_fit_lstm_m2])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plots fitted values x observed values - test sample\n",
    "y_hat_lstm_m2 = y_hat_lstm_m2.reshape(y_hat_lstm_m2.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m2_test, y_hat_lstm_m2])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot our loss \n",
    "plt.plot(hist_lstm_m2.history['loss'])\n",
    "plt.plot(hist_lstm_m2.history['val_loss'])\n",
    "plt.title('Loss (MSE)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Saves performance metrics\n",
    "df_MSE.loc['LSTM_M2','p0'] = np.min(hist_lstm_m2.history['val_mse'])\n",
    "df_MAE.loc['LSTM_M2','p0'] = np.min(hist_lstm_m2.history['val_mae'])\n",
    "df_RMSE.loc['LSTM_M2','p0'] = np.min(hist_lstm_m2.history['val_root_mean_squared_error'])\n",
    "df_MAPE.loc['LSTM_M2','p0'] = np.min(hist_lstm_m2.history['val_mean_absolute_percentage_error'])\n",
    "df_CS.loc['LSTM_M2','p0'] = np.min(hist_lstm_m2.history['val_cosine_similarity'])\n",
    "\n",
    "# Fits models for every period\n",
    "k = 0\n",
    "str_model = 'LSTM_M2'\n",
    "for p in col_names:\n",
    "    \n",
    "    if k > 0: # starts at p1 because p0 has been already executed\n",
    "        \n",
    "        shift = periods[k]\n",
    "        lstm_m2 = lstm_m2_gen()\n",
    "        \n",
    "        # Split data into samples (input, output)\n",
    "        X_lstm_m2, y_lstm_m2 = split_sequence_mult(data_lstm_m2, n_lags_lstm, per_ahead = shift + 1, cum = False)\n",
    "            \n",
    "        # Indexation\n",
    "        len1 = len(y_lstm_m2)\n",
    "        len2 = len(y.index)\n",
    "        i = y.index[(len2-len1):len2]\n",
    "            \n",
    "        y_lstm_m2 = pd.DataFrame(data = y_lstm_m2, index = i)\n",
    "        df_index_adj = pd.DataFrame(data = range(0,len(y_lstm_m2)), index = y_lstm_m2.index)\n",
    "        y_lstm_m2_train = y_lstm_m2.loc[y_lstm_m2.index.intersection(index_train)]\n",
    "        y_lstm_m2_test = y_lstm_m2.loc[y_lstm_m2.index.intersection(index_test)]\n",
    "            \n",
    "        # Converts into array\n",
    "        X_lstm_m2 = np.array(X_lstm_m2)\n",
    "        X_lstm_m2_train = X_lstm_m2[df_index_adj.loc[y_lstm_m2_train.index][0], :, :]\n",
    "        X_lstm_m2_test = X_lstm_m2[df_index_adj.loc[y_lstm_m2_test.index][0], :, :]\n",
    "            \n",
    "        y_lstm_m2 = np.array(y_lstm_m2)[:,0]\n",
    "        y_lstm_m2_train = np.array(y_lstm_m2_train)[:,0]\n",
    "        y_lstm_m2_test = np.array(y_lstm_m2_test)[:,0]\n",
    "        \n",
    "        if p == '3m':\n",
    "            \n",
    "            hist_lstm_m2 = lstm_m2.fit(X_lstm_m2_train[:-shift,:,:], y_lstm_m2_3m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '6m':\n",
    "            hist_lstm_m2 = lstm_m2.fit(X_lstm_m2_train[:-shift,:,:], y_lstm_m2_6m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '12m':\n",
    "            hist_lstm_m2 = lstm_m2.fit(X_lstm_m2_train[:-shift,:,:], y_lstm_m2_12m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        else:\n",
    "            \n",
    "            hist_lstm_m2 = lstm_m2.fit(X_lstm_m2_train[:-shift,:,:], y_lstm_m2_train[shift:], \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        \n",
    "        df_y_fit_lstm_m2[p][(-len(y_lstm_m2_train)+shift):] = lstm_m2.predict(X_lstm_m2_train[:-shift,:,:])[:,0]\n",
    "        df_y_hat_lstm_m2[p][(-len(y_lstm_m2_test)+shift):] = lstm_m2.predict(X_lstm_m2_test[:-shift,:,:])[:,0]\n",
    "        \n",
    "        df_MSE.loc[str_model, p] = np.min(hist_lstm_m2.history['val_mse'])\n",
    "        df_MAE.loc[str_model, p] = np.min(hist_lstm_m2.history['val_mae'])\n",
    "        df_RMSE.loc[str_model, p] = np.min(hist_lstm_m2.history['val_root_mean_squared_error'])\n",
    "        df_MAPE.loc[str_model, p] = np.min(hist_lstm_m2.history['val_mean_absolute_percentage_error'])\n",
    "        df_CS.loc[str_model, p] = np.min(hist_lstm_m2.history['val_cosine_similarity'])\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# LSTM w/o autoencoder          #\n",
    "#################################\n",
    "\n",
    "# Number of variables (features)\n",
    "n_features = X_lstm_m3_train.shape[2]\n",
    "\n",
    "# Define the model\n",
    "def lstm_m3_gen():\n",
    "    lstm_m3 = Sequential()\n",
    "    lstm_m3.add(LSTM(50, activation=act_fun, \n",
    "                     return_sequences=True, \n",
    "                     kernel_initializer = tf.keras.initializers.LecunNormal,\n",
    "                     input_shape=(n_lags_lstm, n_features)))\n",
    "    lstm_m3.add(LSTM(50, activation=act_fun,\n",
    "                     kernel_initializer = tf.keras.initializers.LecunNormal))\n",
    "    lstm_m3.add(Dense(1))\n",
    "    lstm_m3.compile(optimizer='Nadam', loss='mse',\n",
    "                    metrics = ['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), \n",
    "                               tf.keras.metrics.MeanAbsolutePercentageError(), 'cosine_similarity'])\n",
    "    return lstm_m3\n",
    "\n",
    "lstm_m3 = lstm_m3_gen()\n",
    "\n",
    "# Fit model\n",
    "hist_lstm_m3 = lstm_m3.fit(X_lstm_m3_train, y_lstm_m3_train, \n",
    "                           epochs=num_epochs, \n",
    "                           validation_split = 0.1,\n",
    "                           verbose=False)\n",
    "\n",
    "# Fitted values\n",
    "y_fit_lstm_m3 = lstm_m3.predict(X_lstm_m3_train)[:,0]\n",
    "y_hat_lstm_m3 = lstm_m3.predict(X_lstm_m3_test)[:,0]\n",
    "df_y_fit_lstm_m3['p0'][-len(y_lstm_m3_train):] = y_fit_lstm_m3\n",
    "df_y_hat_lstm_m3['p0'][-len(y_lstm_m3_test):] = y_hat_lstm_m3\n",
    "\n",
    "# Plots fitted values x observed values - training sample\n",
    "y_fit_lstm_m3 = y_fit_lstm_m3.reshape(y_fit_lstm_m3.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m3_train, y_fit_lstm_m3])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plots fitted values x observed values - test sample\n",
    "y_hat_lstm_m3 = y_hat_lstm_m3.reshape(y_hat_lstm_m3.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m3_test, y_hat_lstm_m3])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot our loss \n",
    "plt.plot(hist_lstm_m3.history['loss'])\n",
    "plt.plot(hist_lstm_m3.history['val_loss'])\n",
    "plt.title('Loss (MSE)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Saves performance metrics\n",
    "df_MSE.loc['LSTM_M3','p0'] = np.min(hist_lstm_m3.history['val_mse'])\n",
    "df_MAE.loc['LSTM_M3','p0'] = np.min(hist_lstm_m3.history['val_mae'])\n",
    "df_RMSE.loc['LSTM_M3','p0'] = np.min(hist_lstm_m3.history['val_root_mean_squared_error'])\n",
    "df_MAPE.loc['LSTM_M3','p0'] = np.min(hist_lstm_m3.history['val_mean_absolute_percentage_error'])\n",
    "df_CS.loc['LSTM_M3','p0'] = np.min(hist_lstm_m3.history['val_cosine_similarity'])\n",
    "\n",
    "# Fits models for every period\n",
    "k = 0\n",
    "str_model = 'LSTM_M3'\n",
    "for p in col_names:\n",
    "    \n",
    "    if k > 0: # starts at p1 because p0 has been already executed\n",
    "        \n",
    "        shift = periods[k]\n",
    "        lstm_m3 = lstm_m3_gen()\n",
    "        \n",
    "        # Split data into samples (input, output)\n",
    "        X_lstm_m3, y_lstm_m3 = split_sequence_mult(data_lstm_m3, n_lags_lstm, per_ahead = 1, cum = False)\n",
    "            \n",
    "        # Indexation\n",
    "        len1 = len(y_lstm_m3)\n",
    "        len2 = len(y.index)\n",
    "        i = y.index[(len2-len1):len2]\n",
    "            \n",
    "        y_lstm_m3 = pd.DataFrame(data = y_lstm_m3, index = i)\n",
    "        df_index_adj = pd.DataFrame(data = range(0,len(y_lstm_m3)), index = y_lstm_m3.index)\n",
    "        y_lstm_m3_train = y_lstm_m3.loc[y_lstm_m3.index.intersection(index_train)]\n",
    "        y_lstm_m3_test = y_lstm_m3.loc[y_lstm_m3.index.intersection(index_test)]\n",
    "            \n",
    "        # Converts into array\n",
    "        X_lstm_m3 = np.array(X_lstm_m3)\n",
    "        X_lstm_m3_train = X_lstm_m3[df_index_adj.loc[y_lstm_m3_train.index][0], :, :]\n",
    "        X_lstm_m3_test = X_lstm_m3[df_index_adj.loc[y_lstm_m3_test.index][0], :, :]\n",
    "        \n",
    "        if p == '3m':\n",
    "            hist_lstm_m3 = lstm_m3.fit(X_lstm_m3_train[:-shift,:,:], y_lstm_m3_3m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '6m':\n",
    "            hist_lstm_m3 = lstm_m3.fit(X_lstm_m3_train[:-shift,:,:], y_lstm_m3_6m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '12m':\n",
    "            hist_lstm_m3 = lstm_m3.fit(X_lstm_m3_train[:-shift,:,:], y_lstm_m3_12m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        else:\n",
    "\n",
    "            hist_lstm_m3 = lstm_m3.fit(X_lstm_m3_train[:-shift,:,:], y_lstm_m3_train[shift:], \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        \n",
    "        df_y_fit_lstm_m3[p][(-len(y_lstm_m3_train)+shift):] = lstm_m3.predict(X_lstm_m3_train[:-shift,:,:])[:,0]\n",
    "        df_y_hat_lstm_m3[p][(-len(y_lstm_m3_test)+shift):] = lstm_m3.predict(X_lstm_m3_test[:-shift,:,:])[:,0]\n",
    "        \n",
    "        df_MSE.loc[str_model, p] = np.min(hist_lstm_m3.history['val_mse'])\n",
    "        df_MAE.loc[str_model, p] = np.min(hist_lstm_m3.history['val_mae'])\n",
    "        df_RMSE.loc[str_model, p] = np.min(hist_lstm_m3.history['val_root_mean_squared_error'])\n",
    "        df_MAPE.loc[str_model, p] = np.min(hist_lstm_m3.history['val_mean_absolute_percentage_error'])\n",
    "        df_CS.loc[str_model, p] = np.min(hist_lstm_m3.history['val_cosine_similarity'])\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# ConvLSTM + Autoencoder        #\n",
    "#################################\n",
    "\n",
    "# Number of variables (features)\n",
    "n_features = X_lstm_m4_train.shape[4]\n",
    "\n",
    "# Define the model\n",
    "def lstm_m4_gen():\n",
    "    lstm_m4 = Sequential()\n",
    "    lstm_m4.add(ConvLSTM2D(name='ConvLSTM_Layer1',\n",
    "                       filters=8, \n",
    "                       kernel_size=(1,4), \n",
    "                       activation=act_fun,\n",
    "                       padding='same',\n",
    "                       kernel_initializer = tf.keras.initializers.LecunNormal,\n",
    "                       input_shape=(n_seq_conv, 1, n_steps_conv, n_features),\n",
    "                       dropout=0.0,\n",
    "                       return_sequences = True))\n",
    "    lstm_m4.add(BatchNormalization(name = 'Batch_Norm_1'))\n",
    "    lstm_m4.add(ConvLSTM2D(name='ConvLSTM_Layer2',\n",
    "                           filters=8, \n",
    "                           kernel_size=(1,4), \n",
    "                           activation=act_fun, \n",
    "                           padding='same',\n",
    "                           kernel_initializer = tf.keras.initializers.LecunNormal,\n",
    "                           dropout=0.0,\n",
    "                           return_sequences = True))\n",
    "    lstm_m4.add(BatchNormalization(name = 'Batch_Norm_2'))\n",
    "    lstm_m4.add(Flatten(name = 'Flatten_1'))\n",
    "    lstm_m4.add(RepeatVector(name = 'Repeat_Vector_1', n = 1))\n",
    "    lstm_m4.add(LSTM(name = 'LSTM_Layer_1', units = 50, activation=act_fun, \n",
    "                     kernel_initializer = tf.keras.initializers.LecunNormal, \n",
    "                     return_sequences=True))\n",
    "    lstm_m4.add(TimeDistributed(name = 'Time_Dist_1', \n",
    "                                layer = Dense(name = 'Dense_Layer_1', units = 32, \n",
    "                                              activation = act_fun, \n",
    "                                              kernel_initializer = tf.keras.initializers.LecunNormal)))\n",
    "    lstm_m4.add(TimeDistributed(name = 'Time_Dist_2', \n",
    "                                layer = Dense(name = 'Dense_Layer_2', units = 16, \n",
    "                                              activation = act_fun, \n",
    "                                              kernel_initializer = tf.keras.initializers.LecunNormal)))\n",
    "    lstm_m4.add(Dense(name = 'Output', units = 1))\n",
    "    lstm_m4.compile(loss='mse', optimizer='Nadam',\n",
    "                    metrics = ['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), \n",
    "                               tf.keras.metrics.MeanAbsolutePercentageError(), 'cosine_similarity'])\n",
    "\n",
    "    return lstm_m4\n",
    "\n",
    "lstm_m4 = lstm_m4_gen()\n",
    "\n",
    "# Fit model\n",
    "hist_lstm_m4 = lstm_m4.fit(X_lstm_m4_train, y_lstm_m4_train, \n",
    "                           epochs=num_epochs, \n",
    "                           validation_split = 0.1,\n",
    "                           verbose=0)\n",
    "\n",
    "# Fitted values\n",
    "y_fit_lstm_m4 = lstm_m4.predict(X_lstm_m4_train)[:,0][:,0]\n",
    "y_hat_lstm_m4 = lstm_m4.predict(X_lstm_m4_test)[:,0][:,0]\n",
    "df_y_fit_lstm_m4['p0'][-len(y_lstm_m4_train):] = y_fit_lstm_m4\n",
    "df_y_hat_lstm_m4['p0'][-len(y_lstm_m4_test):] = y_hat_lstm_m4\n",
    "\n",
    "# Plots fitted values x observed values - training sample\n",
    "y_fit_lstm_m4 = y_fit_lstm_m4.reshape(y_fit_lstm_m4.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m4_train, y_fit_lstm_m4])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.title('Conv + Autoencoder - Training Sample')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plots fitted values x observed values - test sample\n",
    "y_hat_lstm_m4 = y_hat_lstm_m4.reshape(y_hat_lstm_m4.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m4_test, y_hat_lstm_m4])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.title('Conv + Autoencoder - Test Sample')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot our loss \n",
    "plt.plot(hist_lstm_m4.history['loss'])\n",
    "plt.plot(hist_lstm_m4.history['val_loss'])\n",
    "plt.title('Conv + Autoencoder - Loss (MSE)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Saves performance metrics\n",
    "df_MSE.loc['ConvLSTM_M1','p0'] = np.min(hist_lstm_m4.history['val_mse'])\n",
    "df_MAE.loc['ConvLSTM_M1','p0'] = np.min(hist_lstm_m4.history['val_mae'])\n",
    "df_RMSE.loc['ConvLSTM_M1','p0'] = np.min(hist_lstm_m4.history['val_root_mean_squared_error'])\n",
    "df_MAPE.loc['ConvLSTM_M1','p0'] = np.min(hist_lstm_m4.history['val_mean_absolute_percentage_error'])\n",
    "df_CS.loc['ConvLSTM_M1','p0'] = np.min(hist_lstm_m4.history['val_cosine_similarity'])\n",
    "\n",
    "# Fits models for every period\n",
    "k = 0\n",
    "str_model = 'ConvLSTM_M1'\n",
    "for p in col_names:\n",
    "    \n",
    "    if k > 0: # starts at p1 because p0 has been already executed\n",
    "        \n",
    "        shift = periods[k]\n",
    "        lstm_m4 = lstm_m4_gen()\n",
    "        \n",
    "        # Split data into samples (input, output)\n",
    "        X_lstm_m4, y_lstm_m4 = split_sequence_mult(data_lstm_m4, n_lags_lstm, per_ahead = shift + 1, cum = False)\n",
    "            \n",
    "        # Indexation\n",
    "        len1 = len(y_lstm_m4)\n",
    "        len2 = len(y.index)\n",
    "        i = y.index[(len2-len1):len2]\n",
    "            \n",
    "        y_lstm_m4 = pd.DataFrame(data = y_lstm_m4, index = i)\n",
    "        df_index_adj = pd.DataFrame(data = range(0,len(y_lstm_m4)), index = y_lstm_m4.index)\n",
    "        y_lstm_m4_train = y_lstm_m4.loc[y_lstm_m4.index.intersection(index_train)]\n",
    "        y_lstm_m4_test = y_lstm_m4.loc[y_lstm_m4.index.intersection(index_test)]\n",
    "            \n",
    "        # Reshapes and converts into array\n",
    "        X_lstm_m4 = np.array(X_lstm_m4)\n",
    "        X_lstm_m4 = X_lstm_m4.reshape(X_lstm_m4.shape[0], n_seq_conv, 1, n_steps_conv, X_lstm_m4.shape[2])\n",
    "        X_lstm_m4_train = X_lstm_m4[df_index_adj.loc[y_lstm_m4_train.index][0], :, :, :, :]\n",
    "        X_lstm_m4_test = X_lstm_m4[df_index_adj.loc[y_lstm_m4_test.index][0], :, :, :, :]\n",
    "        \n",
    "        y_lstm_m4 = np.array(y_lstm_m4)[:,0]\n",
    "        y_lstm_m4_train = np.array(y_lstm_m4_train)[:,0]\n",
    "        y_lstm_m4_test = np.array(y_lstm_m4_test)[:,0]\n",
    "        \n",
    "        if p == '3m':\n",
    "            hist_lstm_m4 = lstm_m4.fit(X_lstm_m4_train[:-shift,:,:,:,:], y_lstm_m4_3m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '6m':\n",
    "            hist_lstm_m4 = lstm_m4.fit(X_lstm_m4_train[:-shift,:,:,:,:], y_lstm_m4_6m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '12m':\n",
    "            hist_lstm_m4 = lstm_m4.fit(X_lstm_m4_train[:-shift,:,:,:,:], y_lstm_m4_12m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        else:\n",
    "            \n",
    "            hist_lstm_m4 = lstm_m4.fit(X_lstm_m4_train, y_lstm_m4_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        \n",
    "        df_y_fit_lstm_m4[p][(-len(y_lstm_m4_train)+shift):] = lstm_m4.predict(X_lstm_m4_train[:-shift,:,:,:,:])[:,0][:,0]\n",
    "        df_y_hat_lstm_m4[p][(-len(y_lstm_m4_test)+shift):] = lstm_m4.predict(X_lstm_m4_test[:-shift,:,:,:,:])[:,0][:,0]\n",
    "        \n",
    "        df_MSE.loc[str_model, p] = np.min(hist_lstm_m4.history['val_mse'])\n",
    "        df_MAE.loc[str_model, p] = np.min(hist_lstm_m4.history['val_mae'])\n",
    "        df_RMSE.loc[str_model, p] = np.min(hist_lstm_m4.history['val_root_mean_squared_error'])\n",
    "        df_MAPE.loc[str_model, p] = np.min(hist_lstm_m4.history['val_mean_absolute_percentage_error'])\n",
    "        df_CS.loc[str_model, p] = np.min(hist_lstm_m4.history['val_cosine_similarity'])\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# ConvLSTM + VAE                #\n",
    "#################################\n",
    "\n",
    "# Number of variables (features)\n",
    "n_features = X_lstm_m5_train.shape[4]\n",
    "\n",
    "def lstm_m5_gen():\n",
    "    lstm_m5 = Sequential()\n",
    "    lstm_m5.add(ConvLSTM2D(name='ConvLSTM_Layer1',\n",
    "                           filters=16, \n",
    "                           kernel_size=(3,3), \n",
    "                           activation=act_fun,\n",
    "                           padding='same',\n",
    "                           kernel_initializer = tf.keras.initializers.LecunNormal,\n",
    "                           input_shape=(n_seq_conv, 1, n_steps_conv, n_features),\n",
    "                           dropout=0.2,\n",
    "                           return_sequences = True))\n",
    "    lstm_m5.add(BatchNormalization(name = 'Batch_Norm_1'))\n",
    "    # lstm_m5.add(MaxPooling3D(name = 'Max_Pooling_3D_1', pool_size=(2,2,2), padding = 'same'))\n",
    "    lstm_m5.add(ConvLSTM2D(name='ConvLSTM_Layer2',\n",
    "                           filters=16, \n",
    "                           kernel_size=(3,3), \n",
    "                           activation=act_fun, \n",
    "                           padding='same',\n",
    "                           kernel_initializer = tf.keras.initializers.LecunNormal,\n",
    "                           dropout=0.2,\n",
    "                           return_sequences = True))\n",
    "    lstm_m5.add(BatchNormalization(name = 'Batch_Norm_2'))\n",
    "    lstm_m5.add(MaxPooling3D(name = 'Max_Pooling_3D_1', pool_size=(2,2,2), padding = 'same'))\n",
    "    lstm_m5.add(Flatten(name = 'Flatten_1'))\n",
    "    lstm_m5.add(RepeatVector(name = 'Repeat_Vector_1', n = 1))\n",
    "    lstm_m5.add(LSTM(name = 'LSTM_Layer_1', units = 100, activation=act_fun, \n",
    "                     kernel_initializer = tf.keras.initializers.LecunNormal,\n",
    "                     dropout = 0.2,\n",
    "                     return_sequences=True))\n",
    "    lstm_m5.add(LSTM(name = 'LSTM_Layer_2', units = 100, activation=act_fun, \n",
    "                     kernel_initializer = tf.keras.initializers.LecunNormal, \n",
    "                     dropout = 0.2,\n",
    "                     return_sequences=True))\n",
    "    lstm_m5.add(TimeDistributed(name = 'Time_Dist_1', \n",
    "                                layer = Dense(name = 'Dense_Layer_1', units = 64, \n",
    "                                              activation = act_fun, \n",
    "                                              kernel_initializer = tf.keras.initializers.LecunNormal)))\n",
    "    lstm_m5.add(TimeDistributed(name = 'Time_Dist_2', \n",
    "                                layer = Dense(name = 'Dense_Layer_2', units = 32, \n",
    "                                              activation = act_fun, \n",
    "                                              kernel_initializer = tf.keras.initializers.LecunNormal)))\n",
    "    lstm_m5.add(Dense(name = 'Output', units = 1))\n",
    "    lstm_m5.compile(loss='mse', optimizer='Nadam',\n",
    "                    metrics = ['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), \n",
    "                               tf.keras.metrics.MeanAbsolutePercentageError(), 'cosine_similarity'])\n",
    "    return lstm_m5\n",
    "\n",
    "lstm_m5 = lstm_m5_gen()\n",
    "\n",
    "hist_lstm_m5 = lstm_m5.fit(X_lstm_m5, y_lstm_m5, \n",
    "                           epochs=num_epochs,\n",
    "                           validation_split = 0.1,\n",
    "                           shuffle=True,\n",
    "                           verbose=False)\n",
    "\n",
    "# Fitted values\n",
    "y_fit_lstm_m5 = lstm_m5.predict(X_lstm_m5_train)[:,0][:,0]\n",
    "y_hat_lstm_m5 = lstm_m5.predict(X_lstm_m5_test)[:,0][:,0]\n",
    "df_y_fit_lstm_m5['p0'][-len(y_lstm_m5_train):] = y_fit_lstm_m5\n",
    "df_y_hat_lstm_m5['p0'][-len(y_lstm_m5_test):] = y_hat_lstm_m5\n",
    "\n",
    "plt.rc('text', usetex = True)\n",
    "plt.rc('font', family = 'serif')\n",
    "str_Dir_Plots = 'C:/Users/alext/Desktop/Dissertação/Alexandre/v4/'\n",
    "y_full = y.iloc[12:]\n",
    "y_full.index = pd.to_datetime(y_full.index)\n",
    "y_full_lstm_m5 = pd.DataFrame(lstm_m5.predict(X_lstm_m5)[:,0][:,0])\n",
    "y_full_lstm_m5.index = pd.to_datetime(y_full.index)\n",
    "data_y = pd.concat([y_full, y_full_lstm_m5], axis = 1)\n",
    "sns.lineplot(data = data_y)\n",
    "plt.close()\n",
    "y_sq_err = (y_full.iloc[:,0] - y_full_lstm_m5.iloc[:,0])**2\n",
    "MSE_roll = y_sq_err.rolling(12).mean()\n",
    "ax = sns.lineplot(data=MSE_roll)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "plt.savefig(fname = str_Dir_Plots + 'ConvLSTM_MSE_Roll' + '.pdf')\n",
    "plt.close()\n",
    "vol_roll = y_full.rolling(12).std()\n",
    "ax = sns.lineplot(data=vol_roll)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Volatility\")\n",
    "ax.get_legend().remove()\n",
    "plt.savefig(fname = str_Dir_Plots + 'CPI_Vol_Roll' + '.pdf')\n",
    "\n",
    "# Plots fitted values x observed values - training sample\n",
    "y_fit_lstm_m5 = y_fit_lstm_m5.reshape(y_fit_lstm_m5.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m5_train, y_fit_lstm_m5])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plots fitted values x observed values - test sample\n",
    "y_hat_lstm_m5 = y_hat_lstm_m5.reshape(y_hat_lstm_m5.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m5_test, y_hat_lstm_m5])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot our loss \n",
    "plt.plot(hist_lstm_m5.history['loss'])\n",
    "plt.plot(hist_lstm_m5.history['val_loss'])\n",
    "plt.title('ConvLSTM + VAE - Loss (MSE)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Saves performance metrics\n",
    "df_MSE.loc['ConvLSTM_M2','p0'] = np.min(hist_lstm_m5.history['val_mse'])\n",
    "df_MAE.loc['ConvLSTM_M2','p0'] = np.min(hist_lstm_m5.history['val_mae'])\n",
    "df_RMSE.loc['ConvLSTM_M2','p0'] = np.min(hist_lstm_m5.history['val_root_mean_squared_error'])\n",
    "df_MAPE.loc['ConvLSTM_M2','p0'] = np.min(hist_lstm_m5.history['val_mean_absolute_percentage_error'])\n",
    "df_CS.loc['ConvLSTM_M2','p0'] = np.min(hist_lstm_m5.history['val_cosine_similarity'])\n",
    "\n",
    "# Fits models for every period\n",
    "k = 0\n",
    "str_model = 'ConvLSTM_M2'\n",
    "for p in col_names:\n",
    "    \n",
    "    if k > 0: # starts at p1 because p0 has been already executed\n",
    "        \n",
    "        shift = periods[k]\n",
    "        lstm_m5 = lstm_m5_gen()\n",
    "        \n",
    "        # Split data into samples (input, output)\n",
    "        X_lstm_m5, y_lstm_m5 = split_sequence_mult(data_lstm_m5, n_lags_lstm, per_ahead = shift + 1, cum = False)\n",
    "            \n",
    "        # Indexation\n",
    "        len1 = len(y_lstm_m5)\n",
    "        len2 = len(y.index)\n",
    "        i = y.index[(len2-len1):len2]\n",
    "            \n",
    "        y_lstm_m5 = pd.DataFrame(data = y_lstm_m5, index = i)\n",
    "        df_index_adj = pd.DataFrame(data = range(0,len(y_lstm_m5)), index = y_lstm_m5.index)\n",
    "        y_lstm_m5_train = y_lstm_m5.loc[y_lstm_m5.index.intersection(index_train)]\n",
    "        y_lstm_m5_test = y_lstm_m5.loc[y_lstm_m5.index.intersection(index_test)]\n",
    "            \n",
    "        # Reshapes and converts into array\n",
    "        X_lstm_m5 = np.array(X_lstm_m5)\n",
    "        X_lstm_m5 = X_lstm_m5.reshape(X_lstm_m5.shape[0], n_seq_conv, 1, n_steps_conv, X_lstm_m5.shape[2])\n",
    "        X_lstm_m5_train = X_lstm_m5[df_index_adj.loc[y_lstm_m5_train.index][0], :, :, :, :]\n",
    "        X_lstm_m5_test = X_lstm_m5[df_index_adj.loc[y_lstm_m5_test.index][0], :, :, :, :]\n",
    "            \n",
    "        y_lstm_m5 = np.array(y_lstm_m5)[:,0]\n",
    "        y_lstm_m5_train = np.array(y_lstm_m5_train)[:,0]\n",
    "        y_lstm_m5_test = np.array(y_lstm_m5_test)[:,0]\n",
    "        \n",
    "        if p == '3m':\n",
    "            hist_lstm_m5 = lstm_m5.fit(X_lstm_m5_train[:-shift,:,:,:,:], y_lstm_m5_3m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '6m':\n",
    "            hist_lstm_m5 = lstm_m5.fit(X_lstm_m5_train[:-shift,:,:,:,:], y_lstm_m5_6m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '12m':\n",
    "            hist_lstm_m5 = lstm_m5.fit(X_lstm_m5_train[:-shift,:,:,:,:], y_lstm_m5_12m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        else:\n",
    "            \n",
    "            hist_lstm_m5 = lstm_m5.fit(X_lstm_m5, y_lstm_m5, \n",
    "                           epochs=num_epochs,\n",
    "                           validation_split = 0.1,\n",
    "                           verbose=False)\n",
    "        \n",
    "        df_y_fit_lstm_m5[p][(-len(y_lstm_m5_train)+shift):] = lstm_m5.predict(X_lstm_m5_train[:-shift,:,:,:,:])[:,0][:,0]\n",
    "        df_y_hat_lstm_m5[p][(-len(y_lstm_m5_test)+shift):] = lstm_m5.predict(X_lstm_m5_test[:-shift,:,:,:,:])[:,0][:,0]\n",
    "        \n",
    "        df_MSE.loc[str_model, p] = np.min(hist_lstm_m5.history['val_mse'])\n",
    "        df_MAE.loc[str_model, p] = np.min(hist_lstm_m5.history['val_mae'])\n",
    "        df_RMSE.loc[str_model, p] = np.min(hist_lstm_m5.history['val_root_mean_squared_error'])\n",
    "        df_MAPE.loc[str_model, p] = np.min(hist_lstm_m5.history['val_mean_absolute_percentage_error'])\n",
    "        df_CS.loc[str_model, p] = np.min(hist_lstm_m5.history['val_cosine_similarity'])\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# ConvLSTM w/ past inflation    #\n",
    "#################################\n",
    "\n",
    "# Number of variables (features)\n",
    "n_features = X_lstm_m6_train.shape[4]\n",
    "\n",
    "def lstm_m6_gen():\n",
    "    lstm_m6 = Sequential()\n",
    "    lstm_m6.add(ConvLSTM2D(name='ConvLSTM_Layer1',\n",
    "                           filters=8, \n",
    "                           kernel_size=(1,4), \n",
    "                           activation=act_fun,\n",
    "                           padding='same',\n",
    "                           kernel_initializer = tf.keras.initializers.LecunNormal,\n",
    "                           input_shape=(n_seq_conv, 1, n_steps_conv, n_features),\n",
    "                           dropout=0.1,\n",
    "                           return_sequences = True))\n",
    "    lstm_m6.add(BatchNormalization(name = 'Batch_Norm_1'))\n",
    "    lstm_m6.add(ConvLSTM2D(name='ConvLSTM_Layer2',\n",
    "                           filters=8, \n",
    "                           kernel_size=(1,4), \n",
    "                           activation=act_fun, \n",
    "                           padding='same',\n",
    "                           kernel_initializer = tf.keras.initializers.LecunNormal,\n",
    "                           dropout=0.1,\n",
    "                           return_sequences = True))\n",
    "    lstm_m6.add(BatchNormalization(name = 'Batch_Norm_2'))\n",
    "    lstm_m6.add(Flatten(name = 'Flatten_1'))\n",
    "    lstm_m6.add(RepeatVector(name = 'Repeat_Vector_1', n = 1))\n",
    "    lstm_m6.add(LSTM(name = 'LSTM_Layer_1', units = 50, activation=act_fun, \n",
    "                     kernel_initializer = tf.keras.initializers.LecunNormal, \n",
    "                     return_sequences=True))\n",
    "    lstm_m6.add(TimeDistributed(name = 'Time_Dist_1', \n",
    "                                layer = Dense(name = 'Dense_Layer_1', units = 32, \n",
    "                                              activation = act_fun, \n",
    "                                              kernel_initializer = tf.keras.initializers.LecunNormal)))\n",
    "    lstm_m6.add(TimeDistributed(name = 'Time_Dist_2', \n",
    "                                layer = Dense(name = 'Dense_Layer_2', units = 16, \n",
    "                                              activation = act_fun, \n",
    "                                              kernel_initializer = tf.keras.initializers.LecunNormal)))\n",
    "    lstm_m6.add(Dense(name = 'Output', units = 1))\n",
    "    lstm_m6.compile(loss='mse', optimizer='Nadam',\n",
    "                    metrics = ['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), \n",
    "                               tf.keras.metrics.MeanAbsolutePercentageError(), 'cosine_similarity'])\n",
    "    return lstm_m6\n",
    "\n",
    "lstm_m6 = lstm_m6_gen()\n",
    "\n",
    "hist_lstm_m6 = lstm_m6.fit(X_lstm_m6, y_lstm_m6, \n",
    "                           epochs=num_epochs,\n",
    "                           validation_split = 0.1,\n",
    "                           verbose=False)\n",
    "\n",
    "# Fitted values\n",
    "y_fit_lstm_m6 = lstm_m6.predict(X_lstm_m6_train)[:,0][:,0]\n",
    "y_hat_lstm_m6 = lstm_m6.predict(X_lstm_m6_test)[:,0][:,0]\n",
    "df_y_fit_lstm_m6['p0'][-len(y_lstm_m6_train):] = y_fit_lstm_m6\n",
    "df_y_hat_lstm_m6['p0'][-len(y_lstm_m6_test):] = y_hat_lstm_m6\n",
    "\n",
    "# Plots fitted values x observed values - training sample\n",
    "y_fit_lstm_m6 = y_fit_lstm_m6.reshape(y_fit_lstm_m6.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m6_train, y_fit_lstm_m6])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plots fitted values x observed values - test sample\n",
    "y_hat_lstm_m6 = y_hat_lstm_m6.reshape(y_hat_lstm_m6.shape[0])\n",
    "sns.lineplot(data=[y_lstm_m6_test, y_hat_lstm_m6])\n",
    "plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot our loss \n",
    "plt.plot(hist_lstm_m6.history['loss'])\n",
    "plt.plot(hist_lstm_m6.history['val_loss'])\n",
    "plt.title('ConvLSTM + Past Inflation - Loss (MSE)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Saves performance metrics\n",
    "df_MSE.loc['ConvLSTM_M3','p0'] = np.min(hist_lstm_m6.history['val_mse'])\n",
    "df_MAE.loc['ConvLSTM_M3','p0'] = np.min(hist_lstm_m6.history['val_mae'])\n",
    "df_RMSE.loc['ConvLSTM_M3','p0'] = np.min(hist_lstm_m6.history['val_root_mean_squared_error'])\n",
    "df_MAPE.loc['ConvLSTM_M3','p0'] = np.min(hist_lstm_m6.history['val_mean_absolute_percentage_error'])\n",
    "df_CS.loc['ConvLSTM_M3','p0'] = np.min(hist_lstm_m6.history['val_cosine_similarity'])\n",
    "\n",
    "# Fits models for every period\n",
    "k = 0\n",
    "str_model = 'ConvLSTM_M3'\n",
    "for p in col_names:\n",
    "    \n",
    "    if k > 0: # starts at p1 because p0 has been already executed\n",
    "        \n",
    "        shift = periods[k]\n",
    "        lstm_m6 = lstm_m6_gen()\n",
    "        \n",
    "        if p == '3m':\n",
    "            hist_lstm_m6 = lstm_m6.fit(X_lstm_m6_train[:-shift,:,:,:,:], y_lstm_m6_3m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '6m':\n",
    "            hist_lstm_m6 = lstm_m6.fit(X_lstm_m6_train[:-shift,:,:,:,:], y_lstm_m6_6m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        elif p == '12m':\n",
    "            hist_lstm_m6 = lstm_m6.fit(X_lstm_m6_train[:-shift,:,:,:,:], y_lstm_m6_12m_train, \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        else:\n",
    "            hist_lstm_m6 = lstm_m6.fit(X_lstm_m6_train[:-shift,:,:,:,:], y_lstm_m6_train[shift:], \n",
    "                                       epochs=num_epochs,\n",
    "                                       validation_split = 0.1,\n",
    "                                       verbose=False)\n",
    "        \n",
    "        df_y_fit_lstm_m6[p][(-len(y_lstm_m6_train)+shift):] = lstm_m6.predict(X_lstm_m6_train[:-shift,:,:,:,:])[:,0][:,0]\n",
    "        df_y_hat_lstm_m6[p][(-len(y_lstm_m6_test)+shift):] = lstm_m6.predict(X_lstm_m6_test[:-shift,:,:,:,:])[:,0][:,0]\n",
    "        \n",
    "        df_MSE.loc[str_model, p] = np.min(hist_lstm_m6.history['val_mse'])\n",
    "        df_MAE.loc[str_model, p] = np.min(hist_lstm_m6.history['val_mae'])\n",
    "        df_RMSE.loc[str_model, p] = np.min(hist_lstm_m6.history['val_root_mean_squared_error'])\n",
    "        df_MAPE.loc[str_model, p] = np.min(hist_lstm_m6.history['val_mean_absolute_percentage_error'])\n",
    "        df_CS.loc[str_model, p] = np.min(hist_lstm_m6.history['val_cosine_similarity'])\n",
    "    \n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# MLP                           #\n",
    "#################################\n",
    "\n",
    "# Model\n",
    "str_model = 'MLP'\n",
    "\n",
    "# Data\n",
    "X_mlp_train = pd.concat([X_OI_L1_train_pca, X_LM_L1_train_pca, X_H_L1_train_pca, X_COI_L1_train_pca, \n",
    "                                X_MC_L1_train_pca, X_INTFX_L1_train_pca, X_P_L1_train_pca, X_S_L1_train_pca, \n",
    "                                X_OI_L2_train_pca, X_LM_L2_train_pca, X_H_L2_train_pca, X_COI_L2_train_pca, \n",
    "                                X_MC_L2_train_pca, X_INTFX_L2_train_pca, X_P_L2_train_pca, X_S_L2_train_pca, \n",
    "                                X_OI_L3_train_pca, X_LM_L3_train_pca, X_H_L3_train_pca, X_COI_L3_train_pca, \n",
    "                                X_MC_L3_train_pca, X_INTFX_L3_train_pca, X_P_L3_train_pca, X_S_L3_train_pca, \n",
    "                                X_OI_L4_train_pca, X_LM_L4_train_pca, X_H_L4_train_pca, X_COI_L4_train_pca, \n",
    "                                X_MC_L4_train_pca, X_INTFX_L4_train_pca, X_P_L4_train_pca, X_S_L4_train_pca, \n",
    "                                X_OI_L12_train_pca, X_LM_L12_train_pca, X_H_L12_train_pca, X_COI_L12_train_pca, \n",
    "                                X_MC_L12_train_pca, X_INTFX_L12_train_pca, X_P_L12_train_pca, \n",
    "                                X_S_L12_train_pca], axis = 1).dropna()\n",
    "X_mlp_train = np.array(X_mlp_train)\n",
    "y_mlp_train = y_train\n",
    "y_mlp_train_3m = scale(np.array(y_mlp_train.rolling(3).mean().dropna())[:,0])\n",
    "y_mlp_train_6m = scale(np.array(y_mlp_train.rolling(6).mean().dropna())[:,0])\n",
    "y_mlp_train_12m = scale(np.array(y_mlp_train.rolling(12).mean().dropna())[:,0])\n",
    "y_mlp_train = np.array(y_train)[:,0]\n",
    "\n",
    "X_mlp_test = pd.concat([X_OI_L1_test_pca, X_LM_L1_test_pca, X_H_L1_test_pca, X_COI_L1_test_pca, \n",
    "                                X_MC_L1_test_pca, X_INTFX_L1_test_pca, X_P_L1_test_pca, X_S_L1_test_pca, \n",
    "                                X_OI_L2_test_pca, X_LM_L2_test_pca, X_H_L2_test_pca, X_COI_L2_test_pca, \n",
    "                                X_MC_L2_test_pca, X_INTFX_L2_test_pca, X_P_L2_test_pca, X_S_L2_test_pca, \n",
    "                                X_OI_L3_test_pca, X_LM_L3_test_pca, X_H_L3_test_pca, X_COI_L3_test_pca, \n",
    "                                X_MC_L3_test_pca, X_INTFX_L3_test_pca, X_P_L3_test_pca, X_S_L3_test_pca, \n",
    "                                X_OI_L4_test_pca, X_LM_L4_test_pca, X_H_L4_test_pca, X_COI_L4_test_pca, \n",
    "                                X_MC_L4_test_pca, X_INTFX_L4_test_pca, X_P_L4_test_pca, X_S_L4_test_pca, \n",
    "                                X_OI_L12_test_pca, X_LM_L12_test_pca, X_H_L12_test_pca, X_COI_L12_test_pca, \n",
    "                                X_MC_L12_test_pca, X_INTFX_L12_test_pca, X_P_L12_test_pca, \n",
    "                                X_S_L12_test_pca], axis = 1).dropna()\n",
    "X_mlp_test = np.array(X_mlp_test)\n",
    "y_mlp_test = y_test\n",
    "y_mlp_test_3m = scale(np.array(y_mlp_test.rolling(3).mean().dropna())[:,0])\n",
    "y_mlp_test_6m = scale(np.array(y_mlp_test.rolling(6).mean().dropna())[:,0])\n",
    "y_mlp_test_12m = scale(np.array(y_mlp_test.rolling(12).mean().dropna())[:,0])\n",
    "y_mlp_test = np.array(y_test)[:,0]\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    # MLP Model\n",
    "    mlp_model = MLPRegressor(hidden_layer_sizes = (200, 200, 200, 200, 200, 200), activation = 'relu', solver = 'adam', \n",
    "                             batch_size = 'auto', validation_fraction = share_validation_size,\n",
    "                             max_iter = 1000, random_state = rnd_state) \n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_mlp_train_3m\n",
    "        y_test_local = y_mlp_test_3m\n",
    "        X_train_local = X_mlp_train[:-shift,:]\n",
    "        X_test_local = X_mlp_test[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_mlp_train_6m\n",
    "        y_test_local = y_mlp_test_6m\n",
    "        X_train_local = X_mlp_train[:-shift,:]\n",
    "        X_test_local = X_mlp_test[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_mlp_train_12m\n",
    "        y_test_local = y_mlp_test_12m\n",
    "        X_train_local = X_mlp_train[:-shift,:]\n",
    "        X_test_local = X_mlp_test[:-shift,:]\n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_mlp_train\n",
    "            X_test_local = X_mlp_test\n",
    "            y_train_local = y_mlp_train\n",
    "            y_test_local = y_mlp_test\n",
    "        else:\n",
    "            X_train_local = X_mlp_train[:-shift,:]\n",
    "            X_test_local = X_mlp_test[:-shift,:]\n",
    "            y_train_local = y_mlp_train[shift:]\n",
    "            y_test_local = y_mlp_test[shift:]\n",
    "    \n",
    "    mlp_model_fit = mlp_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = mlp_model.predict(X_train_local)\n",
    "    y_hat_local = mlp_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('MLP - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('MLP - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_mlp[p][(-len(y_mlp_train)+shift):] = y_fit_local\n",
    "    df_y_hat_mlp[p][(-len(y_mlp_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "    \n",
    "# Model\n",
    "str_model = 'MLP2'\n",
    "\n",
    "# Data\n",
    "X_mlp_train = pd.concat([X_OI_L1_train_pca, X_LM_L1_train_pca, X_H_L1_train_pca, X_COI_L1_train_pca, \n",
    "                                X_MC_L1_train_pca, X_INTFX_L1_train_pca, X_P_L1_train_pca, X_S_L1_train_pca, \n",
    "                                X_OI_L2_train_pca, X_LM_L2_train_pca, X_H_L2_train_pca, X_COI_L2_train_pca, \n",
    "                                X_MC_L2_train_pca, X_INTFX_L2_train_pca, X_P_L2_train_pca, X_S_L2_train_pca, \n",
    "                                X_OI_L3_train_pca, X_LM_L3_train_pca, X_H_L3_train_pca, X_COI_L3_train_pca, \n",
    "                                X_MC_L3_train_pca, X_INTFX_L3_train_pca, X_P_L3_train_pca, X_S_L3_train_pca, \n",
    "                                X_OI_L4_train_pca, X_LM_L4_train_pca, X_H_L4_train_pca, X_COI_L4_train_pca, \n",
    "                                X_MC_L4_train_pca, X_INTFX_L4_train_pca, X_P_L4_train_pca, X_S_L4_train_pca, \n",
    "                                X_OI_L12_train_pca, X_LM_L12_train_pca, X_H_L12_train_pca, X_COI_L12_train_pca, \n",
    "                                X_MC_L12_train_pca, X_INTFX_L12_train_pca, X_P_L12_train_pca, \n",
    "                                X_S_L12_train_pca], axis = 1).dropna()\n",
    "X_mlp_train = np.array(X_mlp_train)\n",
    "y_mlp_train = y_train\n",
    "y_mlp_train_3m = scale(np.array(y_mlp_train.rolling(3).mean().dropna())[:,0])\n",
    "y_mlp_train_6m = scale(np.array(y_mlp_train.rolling(6).mean().dropna())[:,0])\n",
    "y_mlp_train_12m = scale(np.array(y_mlp_train.rolling(12).mean().dropna())[:,0])\n",
    "y_mlp_train = np.array(y_train)[:,0]\n",
    "\n",
    "X_mlp_test = pd.concat([X_OI_L1_test_pca, X_LM_L1_test_pca, X_H_L1_test_pca, X_COI_L1_test_pca, \n",
    "                                X_MC_L1_test_pca, X_INTFX_L1_test_pca, X_P_L1_test_pca, X_S_L1_test_pca, \n",
    "                                X_OI_L2_test_pca, X_LM_L2_test_pca, X_H_L2_test_pca, X_COI_L2_test_pca, \n",
    "                                X_MC_L2_test_pca, X_INTFX_L2_test_pca, X_P_L2_test_pca, X_S_L2_test_pca, \n",
    "                                X_OI_L3_test_pca, X_LM_L3_test_pca, X_H_L3_test_pca, X_COI_L3_test_pca, \n",
    "                                X_MC_L3_test_pca, X_INTFX_L3_test_pca, X_P_L3_test_pca, X_S_L3_test_pca, \n",
    "                                X_OI_L4_test_pca, X_LM_L4_test_pca, X_H_L4_test_pca, X_COI_L4_test_pca, \n",
    "                                X_MC_L4_test_pca, X_INTFX_L4_test_pca, X_P_L4_test_pca, X_S_L4_test_pca, \n",
    "                                X_OI_L12_test_pca, X_LM_L12_test_pca, X_H_L12_test_pca, X_COI_L12_test_pca, \n",
    "                                X_MC_L12_test_pca, X_INTFX_L12_test_pca, X_P_L12_test_pca, \n",
    "                                X_S_L12_test_pca], axis = 1).dropna()\n",
    "X_mlp_test = np.array(X_mlp_test)\n",
    "y_mlp_test = y_test\n",
    "y_mlp_test_3m = scale(np.array(y_mlp_test.rolling(3).mean().dropna())[:,0])\n",
    "y_mlp_test_6m = scale(np.array(y_mlp_test.rolling(6).mean().dropna())[:,0])\n",
    "y_mlp_test_12m = scale(np.array(y_mlp_test.rolling(12).mean().dropna())[:,0])\n",
    "y_mlp_test = np.array(y_test)[:,0]\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    # MLP Model\n",
    "    mlp_model = MLPRegressor(hidden_layer_sizes = (200), activation = 'relu', solver = 'adam', \n",
    "                             batch_size = 'auto', validation_fraction = share_validation_size,\n",
    "                             max_iter = 1000, random_state = rnd_state) \n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_mlp_train_3m\n",
    "        y_test_local = y_mlp_test_3m\n",
    "        X_train_local = X_mlp_train[:-shift,:]\n",
    "        X_test_local = X_mlp_test[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_mlp_train_6m\n",
    "        y_test_local = y_mlp_test_6m\n",
    "        X_train_local = X_mlp_train[:-shift,:]\n",
    "        X_test_local = X_mlp_test[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_mlp_train_12m\n",
    "        y_test_local = y_mlp_test_12m\n",
    "        X_train_local = X_mlp_train[:-shift,:]\n",
    "        X_test_local = X_mlp_test[:-shift,:]\n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_mlp_train\n",
    "            X_test_local = X_mlp_test\n",
    "            y_train_local = y_mlp_train\n",
    "            y_test_local = y_mlp_test\n",
    "        else:\n",
    "            X_train_local = X_mlp_train[:-shift,:]\n",
    "            X_test_local = X_mlp_test[:-shift,:]\n",
    "            y_train_local = y_mlp_train[shift:]\n",
    "            y_test_local = y_mlp_test[shift:]\n",
    "    \n",
    "    ridge_model_fit = mlp_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = mlp_model.predict(X_train_local)\n",
    "    y_hat_local = mlp_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('MLP - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('MLP - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_mlp2[p][(-len(y_mlp_train)+shift):] = y_fit_local\n",
    "    df_y_hat_mlp2[p][(-len(y_mlp_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Random Walk                   #\n",
    "#################################\n",
    "\n",
    "# Model\n",
    "str_model = 'RW'\n",
    "\n",
    "X_rw_train = y_train.shift(1).dropna()\n",
    "y_rw_train = y_train.loc[X_rw_train.index]\n",
    "y_rw_train_3m = y_rw_train.rolling(3).mean().dropna()\n",
    "y_rw_train_6m = y_rw_train.rolling(6).mean().dropna()\n",
    "y_rw_train_12m = y_rw_train.rolling(12).mean().dropna()\n",
    "X_rw_train = np.array(X_rw_train)[:,0]\n",
    "y_rw_train = np.array(y_rw_train)[:,0]\n",
    "y_rw_train_3m = scale(np.array(y_rw_train_3m)[:,0])\n",
    "y_rw_train_6m = scale(np.array(y_rw_train_6m)[:,0])\n",
    "y_rw_train_12m = scale(np.array(y_rw_train_12m)[:,0])\n",
    "\n",
    "X_rw_test = y_test.shift(1).dropna()\n",
    "y_rw_test = y_test.loc[X_rw_test.index]\n",
    "y_rw_test_3m = y_rw_test.rolling(3).mean().dropna()\n",
    "y_rw_test_6m = y_rw_test.rolling(6).mean().dropna()\n",
    "y_rw_test_12m = y_rw_test.rolling(12).mean().dropna()\n",
    "X_rw_test = np.array(X_rw_test)[:,0]\n",
    "y_rw_test = np.array(y_rw_test)[:,0]\n",
    "y_rw_test_3m = scale(np.array(y_rw_test_3m)[:,0])\n",
    "y_rw_test_6m = scale(np.array(y_rw_test_6m)[:,0])\n",
    "y_rw_test_12m = scale(np.array(y_rw_test_12m)[:,0])\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    if shift == 0:\n",
    "        y_fit_rw = X_rw_train\n",
    "        y_hat_rw = X_rw_test\n",
    "    else:\n",
    "        y_fit_rw = X_rw_train[:-shift]\n",
    "        y_hat_rw = X_rw_test[:-shift]\n",
    "    \n",
    "    if p == '3m':\n",
    "            \n",
    "        df_y_fit_rw[p][(-len(y_rw_train)+shift):] = y_fit_rw\n",
    "        df_y_hat_rw[p][(-len(y_rw_test)+shift):] = y_hat_rw\n",
    "        \n",
    "        sns.lineplot(data = [y_rw_train_3m, y_fit_rw])\n",
    "        plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        sns.lineplot(data = [y_rw_test_3m, y_hat_rw])\n",
    "        plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        df_MSE.loc[str_model, p] = MSE(y_rw_test_3m, y_hat_rw)\n",
    "        df_MAE.loc[str_model, p] = MAE(y_rw_test_3m, y_hat_rw)\n",
    "        df_RMSE.loc[str_model, p] = RMSE(y_rw_test_3m, y_hat_rw)\n",
    "        df_MAPE.loc[str_model, p] = MAPE(y_rw_test_3m, y_hat_rw)\n",
    "        df_CS.loc[str_model, p] = cos_sim(y_rw_test_3m, y_hat_rw)\n",
    "\n",
    "    elif p == '6m':\n",
    "            \n",
    "        df_y_fit_rw[p][(-len(y_rw_train)+shift):] = y_fit_rw\n",
    "        df_y_hat_rw[p][(-len(y_rw_test)+shift):] = y_hat_rw\n",
    "        \n",
    "        sns.lineplot(data = [y_rw_train_6m, y_fit_rw])\n",
    "        plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        sns.lineplot(data = [y_rw_test_6m, y_hat_rw])\n",
    "        plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        df_MSE.loc[str_model, p] = MSE(y_rw_test_6m, y_hat_rw)\n",
    "        df_MAE.loc[str_model, p] = MAE(y_rw_test_6m, y_hat_rw)\n",
    "        df_RMSE.loc[str_model, p] = RMSE(y_rw_test_6m, y_hat_rw)\n",
    "        df_MAPE.loc[str_model, p] = MAPE(y_rw_test_6m, y_hat_rw)\n",
    "        df_CS.loc[str_model, p] = cos_sim(y_rw_test_6m, y_hat_rw)\n",
    "\n",
    "    elif p == '12m':\n",
    "            \n",
    "        df_y_fit_rw[p][(-len(y_rw_train)+shift):] = y_fit_rw\n",
    "        df_y_hat_rw[p][(-len(y_rw_test)+shift):] = y_hat_rw\n",
    "        \n",
    "        sns.lineplot(data = [y_rw_train_12m, y_fit_rw])\n",
    "        plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        sns.lineplot(data = [y_rw_test_12m, y_hat_rw])\n",
    "        plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        df_MSE.loc[str_model, p] = MSE(y_rw_test_12m, y_hat_rw)\n",
    "        df_MAE.loc[str_model, p] = MAE(y_rw_test_12m, y_hat_rw)\n",
    "        df_RMSE.loc[str_model, p] = RMSE(y_rw_test_12m, y_hat_rw)\n",
    "        df_MAPE.loc[str_model, p] = MAPE(y_rw_test_12m, y_hat_rw)\n",
    "        df_CS.loc[str_model, p] = cos_sim(y_rw_test_12m, y_hat_rw)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        df_y_fit_rw[p][(-len(y_rw_train)+shift):] = y_fit_rw\n",
    "        df_y_hat_rw[p][(-len(y_rw_test)+shift):] = y_hat_rw\n",
    "    \n",
    "        sns.lineplot(data = [y_rw_train[shift:], y_fit_rw])\n",
    "        plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        sns.lineplot(data = [y_rw_test[shift:], y_hat_rw])\n",
    "        plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "        df_MSE.loc[str_model, p] = MSE(y_rw_test[shift:], y_hat_rw)\n",
    "        df_MAE.loc[str_model, p] = MAE(y_rw_test[shift:], y_hat_rw)\n",
    "        df_RMSE.loc[str_model, p] = RMSE(y_rw_test[shift:], y_hat_rw)\n",
    "        df_MAPE.loc[str_model, p] = MAPE(y_rw_test[shift:], y_hat_rw)\n",
    "        df_CS.loc[str_model, p] = cos_sim(y_rw_test[shift:], y_hat_rw)\n",
    "        \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Ridge Regression              #\n",
    "#################################\n",
    "\n",
    "# Model\n",
    "str_model = 'Ridge'\n",
    "\n",
    "X_ridge_train = pd.concat([X_L1_train, X_L2_train, X_L3_train, X_L4_train, X_L12_train], axis = 1)\n",
    "X_ridge_train = X_ridge_train.dropna()\n",
    "y_ridge_train = y_train.iloc[:,0]\n",
    "y_ridge_train_3m = scale(y_ridge_train.rolling(3).mean().dropna())\n",
    "y_ridge_train_6m = scale(y_ridge_train.rolling(6).mean().dropna())\n",
    "y_ridge_train_12m = scale(y_ridge_train.rolling(12).mean().dropna())\n",
    "X_ridge_test = pd.concat([X_L1_test, X_L2_test, X_L3_test, X_L4_test, X_L12_test], axis = 1)\n",
    "X_ridge_test = X_ridge_test.dropna()\n",
    "y_ridge_test = y_test.iloc[:,0]\n",
    "y_ridge_test_3m = scale(y_ridge_test.rolling(3).mean().dropna())\n",
    "y_ridge_test_6m = scale(y_ridge_test.rolling(6).mean().dropna())\n",
    "y_ridge_test_12m = scale(y_ridge_test.rolling(12).mean().dropna())\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    ridge_model = Ridge(alpha = 1.0)    \n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_ridge_train_3m\n",
    "        y_test_local = y_ridge_test_3m\n",
    "        X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "        X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_ridge_train_6m\n",
    "        y_test_local = y_ridge_test_6m\n",
    "        X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "        X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_ridge_train_12m\n",
    "        y_test_local = y_ridge_test_12m\n",
    "        X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "        X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_ridge_train\n",
    "            X_test_local = X_ridge_test\n",
    "            y_train_local = y_ridge_train\n",
    "            y_test_local = y_ridge_test\n",
    "        else:\n",
    "            X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "            X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "            y_train_local = y_ridge_train.iloc[shift:]\n",
    "            y_test_local = y_ridge_test.iloc[shift:]\n",
    "    \n",
    "    ridge_model_fit = ridge_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = ridge_model.predict(X_train_local)\n",
    "    y_hat_local = ridge_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Ridge Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Ridge Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_ridge[p][(-len(y_ridge_train)+shift):] = y_fit_local\n",
    "    df_y_hat_ridge[p][(-len(y_ridge_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Ridge Regression with CV      #\n",
    "#################################\n",
    "\n",
    "str_model = \"Ridge_CV\"\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    ridge_model_cv = RidgeCV(alphas=[1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000])\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_ridge_train_3m\n",
    "        y_test_local = y_ridge_test_3m\n",
    "        X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "        X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_ridge_train_6m\n",
    "        y_test_local = y_ridge_test_6m\n",
    "        X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "        X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_ridge_train_12m\n",
    "        y_test_local = y_ridge_test_12m\n",
    "        X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "        X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_ridge_train\n",
    "            X_test_local = X_ridge_test\n",
    "            y_train_local = y_ridge_train\n",
    "            y_test_local = y_ridge_test\n",
    "        else:\n",
    "            X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "            X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "            y_train_local = y_ridge_train.iloc[shift:]\n",
    "            y_test_local = y_ridge_test.iloc[shift:]\n",
    "    \n",
    "    ridge_model_cv_fit = ridge_model_cv.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = ridge_model_cv.predict(X_train_local)\n",
    "    y_hat_local = ridge_model_cv.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Ridge CV Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Ridge CV Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_ridge_cv[p][(-len(y_ridge_train)+shift):] = y_fit_local\n",
    "    df_y_hat_ridge_cv[p][(-len(y_ridge_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Bayesian Ridge                #\n",
    "#################################\n",
    "\n",
    "str_model = 'Bayesian_Ridge'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    bridge_model = BayesianRidge(n_iter=1000, tol=1e-05, alpha_1=1e-02, alpha_2=1e-02, lambda_1=1e-02, lambda_2=1e-02, \n",
    "                             alpha_init=None, lambda_init=None, \n",
    "                             compute_score=False, fit_intercept=True, \n",
    "                             normalize=False, copy_X=True, verbose=False)\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_ridge_train_3m\n",
    "        y_test_local = y_ridge_test_3m\n",
    "        X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "        X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_ridge_train_6m\n",
    "        y_test_local = y_ridge_test_6m\n",
    "        X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "        X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_ridge_train_12m\n",
    "        y_test_local = y_ridge_test_12m\n",
    "        X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "        X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_ridge_train\n",
    "            X_test_local = X_ridge_test\n",
    "            y_train_local = y_ridge_train\n",
    "            y_test_local = y_ridge_test\n",
    "        else:\n",
    "            X_train_local = X_ridge_train.iloc[:-shift,:]\n",
    "            X_test_local = X_ridge_test.iloc[:-shift,:]\n",
    "            y_train_local = y_ridge_train.iloc[shift:]\n",
    "            y_test_local = y_ridge_test.iloc[shift:]\n",
    "    \n",
    "    bridge_model_fit = bridge_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = bridge_model.predict(X_train_local)\n",
    "    y_hat_local = bridge_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Bayesian Ridge - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Bayesian Ridge - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_bridge[p][(-len(y_ridge_train)+shift):] = y_fit_local\n",
    "    df_y_hat_bridge[p][(-len(y_ridge_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# LASSO                         #\n",
    "#################################\n",
    "\n",
    "str_model = 'Lasso'\n",
    "\n",
    "X_lasso_train = pd.concat([X_L1_train, X_L2_train, X_L3_train, X_L4_train, X_L5_train, X_L6_train, X_L7_train,\n",
    "                           X_L8_train, X_L9_train, X_L10_train, X_L11_train, X_L12_train], axis = 1)\n",
    "X_lasso_train = X_lasso_train.dropna()\n",
    "y_lasso_train = y_train.iloc[:,0]\n",
    "y_lasso_train_3m = scale(y_lasso_train.rolling(3).mean().dropna())\n",
    "y_lasso_train_6m = scale(y_lasso_train.rolling(6).mean().dropna())\n",
    "y_lasso_train_12m = scale(y_lasso_train.rolling(12).mean().dropna())\n",
    "X_lasso_test = pd.concat([X_L1_test, X_L2_test, X_L3_test, X_L4_test, X_L5_test, X_L6_test, X_L7_test,\n",
    "                           X_L8_test, X_L9_test, X_L10_test, X_L11_test, X_L12_test], axis = 1)\n",
    "X_lasso_test = X_lasso_test.dropna()\n",
    "y_lasso_test = y_test.iloc[:,0]\n",
    "y_lasso_test_3m = scale(y_lasso_test.rolling(3).mean().dropna())\n",
    "y_lasso_test_6m = scale(y_lasso_test.rolling(6).mean().dropna())\n",
    "y_lasso_test_12m = scale(y_lasso_test.rolling(12).mean().dropna())\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    lasso_model = Lasso(alpha = 1e-3)\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_lasso_train_3m\n",
    "        y_test_local = y_lasso_test_3m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_lasso_train_6m\n",
    "        y_test_local = y_lasso_test_6m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_lasso_train_12m\n",
    "        y_test_local = y_lasso_test_12m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_lasso_train\n",
    "            X_test_local = X_lasso_test\n",
    "            y_train_local = y_lasso_train\n",
    "            y_test_local = y_lasso_test\n",
    "        else:\n",
    "            X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "            X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "            y_train_local = y_lasso_train.iloc[shift:]\n",
    "            y_test_local = y_lasso_test.iloc[shift:]\n",
    "    \n",
    "    lasso_model_fit = lasso_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = lasso_model.predict(X_train_local)\n",
    "    y_hat_local = lasso_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('LASSO Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('LASSO Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_lasso[p][(-len(y_lasso_train)+shift):] = y_fit_local\n",
    "    df_y_hat_lasso[p][(-len(y_lasso_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# LASSO with CV                 #\n",
    "#################################\n",
    "\n",
    "str_model = 'Lasso_CV'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    lasso_cv_model = LassoCV(alphas=[1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000])\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_lasso_train_3m\n",
    "        y_test_local = y_lasso_test_3m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_lasso_train_6m\n",
    "        y_test_local = y_lasso_test_6m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_lasso_train_12m\n",
    "        y_test_local = y_lasso_test_12m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_lasso_train\n",
    "            X_test_local = X_lasso_test\n",
    "            y_train_local = y_lasso_train\n",
    "            y_test_local = y_lasso_test\n",
    "        else:\n",
    "            X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "            X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "            y_train_local = y_lasso_train.iloc[shift:]\n",
    "            y_test_local = y_lasso_test.iloc[shift:]\n",
    "    \n",
    "    lasso_cv_model_fit = lasso_cv_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = lasso_cv_model.predict(X_train_local)\n",
    "    y_hat_local = lasso_cv_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('LASSO CV Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('LASSO CV Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_lasso_cv[p][(-len(y_lasso_train)+shift):] = y_fit_local\n",
    "    df_y_hat_lasso_cv[p][(-len(y_lasso_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Bayesian Lasso                #\n",
    "#################################\n",
    "\n",
    "str_model = 'Bayesian_Lasso'\n",
    "importr('monomvn')\n",
    "\n",
    "X_blasso_train = pd.concat([X_OI_L1_train_pca, X_LM_L1_train_pca, X_H_L1_train_pca, X_COI_L1_train_pca, \n",
    "                                X_MC_L1_train_pca, X_INTFX_L1_train_pca, X_P_L1_train_pca, \n",
    "                                X_S_L1_train_pca], axis = 1).dropna()\n",
    "X_blasso_train = X_blasso_train.dropna()\n",
    "y_blasso_train = y_train.iloc[:,0]\n",
    "y_blasso_train_3m = scale(y_blasso_train.rolling(3).mean().dropna())\n",
    "y_blasso_train_6m = scale(y_blasso_train.rolling(6).mean().dropna())\n",
    "y_blasso_train_12m = scale(y_blasso_train.rolling(12).mean().dropna())\n",
    "X_blasso_test = X_L1_test_pca\n",
    "X_blasso_test = pd.concat([X_OI_L1_test_pca, X_LM_L1_test_pca, X_H_L1_test_pca, X_COI_L1_test_pca, \n",
    "                                X_MC_L1_test_pca, X_INTFX_L1_test_pca, X_P_L1_test_pca, \n",
    "                                X_S_L1_test_pca], axis = 1).dropna()\n",
    "y_blasso_test = y_test.iloc[:,0]\n",
    "y_blasso_test_3m = scale(y_blasso_test.rolling(3).mean().dropna())\n",
    "y_blasso_test_6m = scale(y_blasso_test.rolling(6).mean().dropna())\n",
    "y_blasso_test_12m = scale(y_blasso_test.rolling(12).mean().dropna())\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_blasso_train_3m\n",
    "        y_test_local = y_blasso_test_3m\n",
    "        X_train_local = X_blasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_blasso_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_blasso_train_6m\n",
    "        y_test_local = y_blasso_test_6m\n",
    "        X_train_local = X_blasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_blasso_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_blasso_train_12m\n",
    "        y_test_local = y_blasso_test_12m\n",
    "        X_train_local = X_blasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_blasso_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_blasso_train\n",
    "            X_test_local = X_blasso_test\n",
    "            y_train_local = y_blasso_train\n",
    "            y_test_local = y_blasso_test\n",
    "        else:\n",
    "            X_train_local = X_blasso_train.iloc[:-shift,:]\n",
    "            X_test_local = X_blasso_test.iloc[:-shift,:]\n",
    "            y_train_local = y_blasso_train.iloc[shift:]\n",
    "            y_test_local = y_blasso_test.iloc[shift:]\n",
    "    \n",
    "    blasso_model_fit = r['blasso'](X = X_train_local, y = y_train_local, T = 400)\n",
    "    betas = np.mean(pd.DataFrame(blasso_model_fit.rx2('beta')))\n",
    "    y_fit_local = np.dot(betas, X_train_local.T)\n",
    "    y_hat_local = np.dot(betas, X_test_local.T)\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Bayesian LASSO - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Bayesian LASSO - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_blasso[p][(-len(y_blasso_train)+shift):] = y_fit_local\n",
    "    df_y_hat_blasso[p][(-len(y_blasso_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "    \n",
    "#################################\n",
    "# Elastic Net with CV           #\n",
    "#################################\n",
    "\n",
    "str_model = 'ENet'\n",
    "\n",
    "# Note that a good choice of list of values for l1_ratio is often to put \n",
    "# more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge)\n",
    "\n",
    "importr('glmnet')\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_lasso_train_3m\n",
    "        y_test_local = y_lasso_test_3m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_lasso_train_6m\n",
    "        y_test_local = y_lasso_test_6m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_lasso_train_12m\n",
    "        y_test_local = y_lasso_test_12m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_lasso_train\n",
    "            X_test_local = X_lasso_test\n",
    "            y_train_local = y_lasso_train\n",
    "            y_test_local = y_lasso_test\n",
    "        else:\n",
    "            X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "            X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "            y_train_local = y_lasso_train.iloc[shift:]\n",
    "            y_test_local = y_lasso_test.iloc[shift:]\n",
    "    \n",
    "    mse = np.inf\n",
    "    for alpha in [.01, .1, .3, .5, .7, .9, .95, .99]:\n",
    "        enet_model_fit_aux = r['glmnet'](np.array(X_train_local), np.array(y_train_local), alpha = alpha, nlambda = 100)\n",
    "        mse_aux = np.inf\n",
    "        y_fit_local = r['predict'](enet_model_fit_aux, newx = np.array(X_train_local))\n",
    "        y_hat_local = r['predict'](enet_model_fit_aux, newx = np.array(X_test_local))\n",
    "        for j in range(0, y_fit_local.shape[1]):\n",
    "            mse_temp = MSE(y_hat_local[:,j], y_test_local)\n",
    "            if mse_temp < mse_aux:\n",
    "                j_aux = j\n",
    "                mse_aux = mse_temp\n",
    "        if mse_aux < mse:\n",
    "            mse = mse_aux\n",
    "            alpha_min = alpha\n",
    "            j_min = j_aux\n",
    "            enet_model_fit = enet_model_fit_aux\n",
    "    y_fit_local = r['predict'](enet_model_fit, newx = np.array(X_train_local))[:,j_min]\n",
    "    y_hat_local = r['predict'](enet_model_fit, newx = np.array(X_test_local))[:,j_min]\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('ENet CV Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('ENet CV Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_enet[p][(-len(y_lasso_train)+shift):] = y_fit_local\n",
    "    df_y_hat_enet[p][(-len(y_lasso_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Support Vector Regression     #\n",
    "#################################\n",
    "\n",
    "# C is the regularization parameter. \n",
    "# The strength of the regularization is inversely proportional to C.\n",
    "# Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "\n",
    "# Epsilon: Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which\n",
    "# no penalty is associated in the training loss function with points predicted within \n",
    "# a distance epsilon from the actual value.\n",
    "\n",
    "# Kernels: it seems that 'rbf' and 'linear' are the most appropriate for our data\n",
    "    \n",
    "str_model = 'SVR'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    svr_model = make_pipeline(StandardScaler(), SVR(C=1, epsilon=0.1, gamma = 'scale', kernel = 'rbf'))\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_lasso_train_3m\n",
    "        y_test_local = y_lasso_test_3m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_lasso_train_6m\n",
    "        y_test_local = y_lasso_test_6m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_lasso_train_12m\n",
    "        y_test_local = y_lasso_test_12m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_lasso_train\n",
    "            X_test_local = X_lasso_test\n",
    "            y_train_local = y_lasso_train\n",
    "            y_test_local = y_lasso_test\n",
    "        else:\n",
    "            X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "            X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "            y_train_local = y_lasso_train.iloc[shift:]\n",
    "            y_test_local = y_lasso_test.iloc[shift:]\n",
    "    \n",
    "    svr_model_fit = svr_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = svr_model.predict(X_train_local)\n",
    "    y_hat_local = svr_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('SVR Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('SVR Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_svr[p][(-len(y_lasso_train)+shift):] = y_fit_local\n",
    "    df_y_hat_svr[p][(-len(y_lasso_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "    \n",
    "#################################\n",
    "# Random Forest                 #\n",
    "#################################\n",
    "\n",
    "str_model = 'Random_Forest'\n",
    "\n",
    "# Data\n",
    "X_rf_train = pd.concat([X_OI_L1_train_pca, X_LM_L1_train_pca, X_H_L1_train_pca, X_COI_L1_train_pca, \n",
    "                                X_MC_L1_train_pca, X_INTFX_L1_train_pca, X_P_L1_train_pca, X_S_L1_train_pca, \n",
    "                                X_OI_L2_train_pca, X_LM_L2_train_pca, X_H_L2_train_pca, X_COI_L2_train_pca, \n",
    "                                X_MC_L2_train_pca, X_INTFX_L2_train_pca, X_P_L2_train_pca, X_S_L2_train_pca, \n",
    "                                X_OI_L3_train_pca, X_LM_L3_train_pca, X_H_L3_train_pca, X_COI_L3_train_pca, \n",
    "                                X_MC_L3_train_pca, X_INTFX_L3_train_pca, X_P_L3_train_pca, X_S_L3_train_pca, \n",
    "                                X_OI_L4_train_pca, X_LM_L4_train_pca, X_H_L4_train_pca, X_COI_L4_train_pca, \n",
    "                                X_MC_L4_train_pca, X_INTFX_L4_train_pca, X_P_L4_train_pca, X_S_L4_train_pca, \n",
    "                                X_OI_L12_train_pca, X_LM_L12_train_pca, X_H_L12_train_pca, X_COI_L12_train_pca, \n",
    "                                X_MC_L12_train_pca, X_INTFX_L12_train_pca, \n",
    "                                X_P_L12_train_pca, X_S_L12_train_pca], axis = 1)\n",
    "X_rf_train = X_rf_train.dropna()\n",
    "y_rf_train = y_train.iloc[:,0]\n",
    "y_rf_train_3m = scale(y_rf_train.rolling(3).mean().dropna())\n",
    "y_rf_train_6m = scale(y_rf_train.rolling(6).mean().dropna())\n",
    "y_rf_train_12m = scale(y_rf_train.rolling(12).mean().dropna())\n",
    "\n",
    "X_rf_test = pd.concat([X_OI_L1_test_pca, X_LM_L1_test_pca, X_H_L1_test_pca, X_COI_L1_test_pca, \n",
    "                                X_MC_L1_test_pca, X_INTFX_L1_test_pca, X_P_L1_test_pca, X_S_L1_test_pca, \n",
    "                                X_OI_L2_test_pca, X_LM_L2_test_pca, X_H_L2_test_pca, X_COI_L2_test_pca, \n",
    "                                X_MC_L2_test_pca, X_INTFX_L2_test_pca, X_P_L2_test_pca, X_S_L2_test_pca, \n",
    "                                X_OI_L3_test_pca, X_LM_L3_test_pca, X_H_L3_test_pca, X_COI_L3_test_pca, \n",
    "                                X_MC_L3_test_pca, X_INTFX_L3_test_pca, X_P_L3_test_pca, X_S_L3_test_pca, \n",
    "                                X_OI_L4_test_pca, X_LM_L4_test_pca, X_H_L4_test_pca, X_COI_L4_test_pca, \n",
    "                                X_MC_L4_test_pca, X_INTFX_L4_test_pca, X_P_L4_test_pca, X_S_L4_test_pca, \n",
    "                                X_OI_L12_test_pca, X_LM_L12_test_pca, X_H_L12_test_pca, X_COI_L12_test_pca, \n",
    "                                X_MC_L12_test_pca, X_INTFX_L12_test_pca, X_P_L12_test_pca, X_S_L12_test_pca], axis = 1)\n",
    "X_rf_test = X_rf_test.dropna()\n",
    "y_rf_test = y_test.iloc[:,0]\n",
    "y_rf_test_3m = scale(y_rf_test.rolling(3).mean().dropna())\n",
    "y_rf_test_6m = scale(y_rf_test.rolling(6).mean().dropna())\n",
    "y_rf_test_12m = scale(y_rf_test.rolling(12).mean().dropna())\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    rf_model = RandomForestRegressor(n_estimators = 20, \n",
    "                           max_depth = None,\n",
    "                           min_samples_split = 4,\n",
    "                           random_state = rnd_state)\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_rf_train_3m\n",
    "        y_test_local = y_rf_test_3m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_rf_train_6m\n",
    "        y_test_local = y_rf_test_6m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_rf_train_12m\n",
    "        y_test_local = y_rf_test_12m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_rf_train\n",
    "            X_test_local = X_rf_test\n",
    "            y_train_local = y_rf_train\n",
    "            y_test_local = y_rf_test\n",
    "        else:\n",
    "            X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "            X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "            y_train_local = y_rf_train.iloc[shift:]\n",
    "            y_test_local = y_rf_test.iloc[shift:]\n",
    "    \n",
    "    rf_model_fit = rf_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = rf_model.predict(X_train_local)\n",
    "    y_hat_local = rf_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('RF Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('RF Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_rf[p][(-len(y_rf_train)+shift):] = y_fit_local\n",
    "    df_y_hat_rf[p][(-len(y_rf_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "    \n",
    "#################################\n",
    "# kNN                           #\n",
    "#################################\n",
    "\n",
    "str_model = 'kNN'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=25)\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_rf_train_3m\n",
    "        y_test_local = y_rf_test_3m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_rf_train_6m\n",
    "        y_test_local = y_rf_test_6m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_rf_train_12m\n",
    "        y_test_local = y_rf_test_12m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_rf_train\n",
    "            X_test_local = X_rf_test\n",
    "            y_train_local = y_rf_train\n",
    "            y_test_local = y_rf_test\n",
    "        else:\n",
    "            X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "            X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "            y_train_local = y_rf_train.iloc[shift:]\n",
    "            y_test_local = y_rf_test.iloc[shift:]\n",
    "    \n",
    "    knn_model_fit = knn_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = knn_model.predict(X_train_local)\n",
    "    y_hat_local = knn_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('RF Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('RF Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_knn[p][(-len(y_rf_train)+shift):] = y_fit_local\n",
    "    df_y_hat_knn[p][(-len(y_rf_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "    \n",
    "#################################\n",
    "# BART                          #\n",
    "#################################\n",
    "\n",
    "str_model = 'BART'\n",
    "\n",
    "# See also 'BART' package!\n",
    "importr('BayesTree')\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    print(p)\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = pd.Series(y_rf_train_3m)\n",
    "        y_test_local = pd.Series(y_rf_test_3m)\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = pd.Series(y_rf_train_6m)\n",
    "        y_test_local = pd.Series(y_rf_test_6m)\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = pd.Series(y_rf_train_12m)\n",
    "        y_test_local = pd.Series(y_rf_test_12m)\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_rf_train\n",
    "            X_test_local = X_rf_test\n",
    "            y_train_local = y_rf_train\n",
    "            y_test_local = y_rf_test\n",
    "        else:\n",
    "            X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "            X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "            y_train_local = y_rf_train.iloc[shift:]\n",
    "            y_test_local = y_rf_test.iloc[shift:]\n",
    "    \n",
    "    rb = r['bart'](X_train_local, y_train_local, X_test_local)\n",
    "\n",
    "    y_fit_local = rb.rx2['yhat.train.mean']\n",
    "    y_hat_local = rb.rx2['yhat.test.mean']\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('BART - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('BART - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_bart[p][(-len(y_rf_train)+shift):] = y_fit_local\n",
    "    df_y_hat_bart[p][(-len(y_rf_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Bagging                       #\n",
    "#################################\n",
    "\n",
    "str_model = 'BAGGING'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    bg_model = BaggingRegressor()\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_rf_train_3m\n",
    "        y_test_local = y_rf_test_3m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_rf_train_6m\n",
    "        y_test_local = y_rf_test_6m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_rf_train_12m\n",
    "        y_test_local = y_rf_test_12m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_rf_train\n",
    "            X_test_local = X_rf_test\n",
    "            y_train_local = y_rf_train\n",
    "            y_test_local = y_rf_test\n",
    "        else:\n",
    "            X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "            X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "            y_train_local = y_rf_train.iloc[shift:]\n",
    "            y_test_local = y_rf_test.iloc[shift:]\n",
    "    \n",
    "    bg_model.fit(X_train_local, y_train_local)\n",
    "    \n",
    "    y_fit_local = bg_model.predict(X_train_local)\n",
    "    y_hat_local = bg_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Bagging Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Bagging Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_bagging[p][(-len(y_rf_train)+shift):] = y_fit_local\n",
    "    df_y_hat_bagging[p][(-len(y_rf_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Robust Regression             #\n",
    "#################################\n",
    "\n",
    "str_model = 'Huber'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    huber_model = HuberRegressor()\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_lasso_train_3m\n",
    "        y_test_local = y_lasso_test_3m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_lasso_train_6m\n",
    "        y_test_local = y_lasso_test_6m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_lasso_train_12m\n",
    "        y_test_local = y_lasso_test_12m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_lasso_train\n",
    "            X_test_local = X_lasso_test\n",
    "            y_train_local = y_lasso_train\n",
    "            y_test_local = y_lasso_test\n",
    "        else:\n",
    "            X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "            X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "            y_train_local = y_lasso_train.iloc[shift:]\n",
    "            y_test_local = y_lasso_test.iloc[shift:]\n",
    "    \n",
    "    huber_model_fit = huber_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = huber_model.predict(X_train_local)\n",
    "    y_hat_local = huber_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Huber Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Huber Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_huber[p][(-len(y_lasso_train)+shift):] = y_fit_local\n",
    "    df_y_hat_huber[p][(-len(y_lasso_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Theil-Sen Regression          #\n",
    "#################################\n",
    "\n",
    "str_model = 'Theil_Sen'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    ts_model = TheilSenRegressor()\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_lasso_train_3m\n",
    "        y_test_local = y_lasso_test_3m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_lasso_train_6m\n",
    "        y_test_local = y_lasso_test_6m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_lasso_train_12m\n",
    "        y_test_local = y_lasso_test_12m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_lasso_train\n",
    "            X_test_local = X_lasso_test\n",
    "            y_train_local = y_lasso_train\n",
    "            y_test_local = y_lasso_test\n",
    "        else:\n",
    "            X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "            X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "            y_train_local = y_lasso_train.iloc[shift:]\n",
    "            y_test_local = y_lasso_test.iloc[shift:]\n",
    "    \n",
    "    ts_model_fit = ts_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = ts_model.predict(X_train_local)\n",
    "    y_hat_local = ts_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Theil-Sen Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Theil-Sen Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_ts[p][(-len(y_lasso_train)+shift):] = y_fit_local\n",
    "    df_y_hat_ts[p][(-len(y_lasso_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Factor analysis               #\n",
    "#################################\n",
    "\n",
    "str_model = 'Factor'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_rf_train_3m\n",
    "        y_test_local = y_rf_test_3m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_rf_train_6m\n",
    "        y_test_local = y_rf_test_6m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_rf_train_12m\n",
    "        y_test_local = y_rf_test_12m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_rf_train\n",
    "            X_test_local = X_rf_test\n",
    "            y_train_local = y_rf_train\n",
    "            y_test_local = y_rf_test\n",
    "        else:\n",
    "            X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "            X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "            y_train_local = y_rf_train.iloc[shift:]\n",
    "            y_test_local = y_rf_test.iloc[shift:]\n",
    "    \n",
    "    factor_model = sm.OLS(np.array(y_train_local), np.array(X_train_local))\n",
    "    factor_model_fit = factor_model.fit()\n",
    "    X_train_local_adj = pd.DataFrame()\n",
    "    X_test_local_adj = pd.DataFrame()\n",
    "    for j in range(0,len(factor_model_fit.pvalues)):\n",
    "        if factor_model_fit.pvalues[j] < 0.05:\n",
    "            X_train_local_adj = pd.concat([X_train_local_adj, X_train_local.iloc[:,j]], axis = 1)\n",
    "            X_test_local_adj = pd.concat([X_test_local_adj, X_test_local.iloc[:,j]], axis = 1)\n",
    "    factor_model = sm.OLS(np.array(y_train_local), np.array(X_train_local_adj))\n",
    "    factor_model_fit = factor_model.fit()\n",
    "    X_train_local_adj2 = pd.DataFrame()\n",
    "    X_test_local_adj2 = pd.DataFrame()\n",
    "    for j in range(0,len(factor_model_fit.pvalues)):\n",
    "        if factor_model_fit.pvalues[j] < 0.05:\n",
    "            X_train_local_adj2 = pd.concat([X_train_local_adj, X_train_local.iloc[:,j]], axis = 1)\n",
    "            X_test_local_adj2 = pd.concat([X_test_local_adj, X_test_local.iloc[:,j]], axis = 1)\n",
    "    y_fit_local = factor_model_fit.predict(np.array(X_train_local_adj))\n",
    "    y_hat_local = factor_model_fit.predict(np.array(X_test_local_adj))\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Factor Model - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Factor Model - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_factor[p][(-len(y_rf_train)+shift):] = y_fit_local\n",
    "    df_y_hat_factor[p][(-len(y_rf_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# GARCH                         #\n",
    "#################################\n",
    "\n",
    "str_model = 'GARCH'\n",
    "\n",
    "importr('rugarch')\n",
    "\n",
    "n_train = len(index_train)\n",
    "n_test = len(index_test)\n",
    "y_fit_local = np.zeros(n_train+n_test)\n",
    "y_sim_local = np.zeros(n_train+n_test)\n",
    "y_hat_local = np.zeros(n_test)\n",
    "y_hat_local_3m = np.zeros(n_test)\n",
    "y_hat_local_6m = np.zeros(n_test)\n",
    "y_hat_local_12m = np.zeros(n_test)\n",
    "\n",
    "spec = r['ugarchspec']() # GARCH(1,1)\n",
    "fit = r['ugarchfit'](spec = spec, data = y)\n",
    "sim = r['ugarchsim'](fit, n_train + n_test)\n",
    "y_sim_local = scale(r['fitted'](sim))\n",
    "        \n",
    "for i in range(0,n_test+1):\n",
    "    print(i)\n",
    "    y_window = np.array(y_sim_local[i:(i+n_train)])\n",
    "    try:\n",
    "        spec = r['ugarchspec']() # GARCH(1,1)\n",
    "        fit = r['ugarchfit'](spec = spec, data = y_window)\n",
    "        forc = r['ugarchforecast'](fit, 'NULL', 12)\n",
    "        y_fit_local[i:(i+n_train)] = r['fitted'](fit)[:,0]\n",
    "        aux = r['fitted'](forc)[:,0]\n",
    "        if i <= n_test-1:\n",
    "            y_hat_local[i] = aux[0]\n",
    "            y_hat_local_3m[i] = np.mean(aux[0:2])\n",
    "            y_hat_local_6m[i] = np.mean(aux[0:5])\n",
    "            y_hat_local_12m[i] = np.mean(aux[0:11])\n",
    "    except:\n",
    "        if i <= n_test-1:\n",
    "            y_hat_local[i] = y_hat_local[i-1]\n",
    "            y_hat_local_3m[i] = y_hat_local_3m[i-1]\n",
    "            y_hat_local_6m[i] = y_hat_local_6m[i-1]\n",
    "            y_hat_local_12m[i] = y_hat_local_12m[i-1]\n",
    "\n",
    "y_fit_local = np.array(y_fit_local[0:n_train])\n",
    "y_fit_local = pd.DataFrame(y_fit_local, columns = ['Train'])\n",
    "y_hat_local = np.array(y_hat_local)\n",
    "y_hat_local = pd.DataFrame(y_hat_local, columns = ['Test'])\n",
    "y_hat_local_3m = np.array(y_hat_local_3m[:-2])\n",
    "y_hat_local_3m = pd.DataFrame(y_hat_local_3m, columns = ['Test'])\n",
    "y_hat_local_6m = np.array(y_hat_local_6m[:-5])\n",
    "y_hat_local_6m = pd.DataFrame(y_hat_local_6m, columns = ['Test'])\n",
    "y_hat_local_12m = np.array(y_hat_local_12m[:-11])\n",
    "y_hat_local_12m = pd.DataFrame(y_hat_local_12m, columns = ['Test'])\n",
    "y_true = np.array(y_sim_local[n_train:(n_train+n_test)])\n",
    "y_true = pd.DataFrame(y_true, columns = ['Observed'])\n",
    "\n",
    "sns.lineplot(data=pd.concat([y_true, y_hat_local],axis=1))\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    print(p)\n",
    "    \n",
    "    if p == '3m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(3).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(3).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_3m)\n",
    "        \n",
    "    elif p == '6m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(6).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(6).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_6m)\n",
    "        \n",
    "    elif p == '12m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(12).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(12).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_12m)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if p == 'p0':\n",
    "            y_local = y_true\n",
    "            y_fit = y_fit_local\n",
    "            y_hat = y_hat_local\n",
    "        else:\n",
    "            y_local = y_true[shift:]\n",
    "            y_fit = y_fit_local[:-shift]\n",
    "            y_hat = y_hat_local[:-shift]\n",
    "    \n",
    "    y_local = np.array(y_local)[:,0]\n",
    "    y_fit = np.array(y_fit)[:,0]\n",
    "    y_hat = np.array(y_hat)[:,0]\n",
    "    \n",
    "    df_y_fit_garch[p][(-n_train+shift):] = y_fit\n",
    "    df_y_hat_garch[p][(-n_test+shift):] = y_hat\n",
    "    \n",
    "    df_MSE.loc[str_model, p] = MSE(y_local, y_hat)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_local, y_hat)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_local, y_hat)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_local, y_hat)\n",
    "    df_CS.loc[str_model, p] = cos_sim2(y_local, y_hat)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# VECM                          #\n",
    "#################################\n",
    "\n",
    "str_model = 'VECM'\n",
    "\n",
    "data_vecm = pd.concat([X['CPIAUCSL'], X['TB3MS'], X['M1SL'], X['OILPRICEx'], X['TWEXAFEGSMTHx'], X['INDPRO']], axis = 1)\n",
    "\n",
    "importr('tsDyn')\n",
    "stats = importr('stats')\n",
    "\n",
    "n_train = len(index_train)\n",
    "n_test = len(index_test)\n",
    "y_fit_local = np.zeros(n_train+n_test)\n",
    "y_hat_local = np.zeros(n_test)\n",
    "y_hat_local_3m = np.zeros(n_test)\n",
    "y_hat_local_6m = np.zeros(n_test)\n",
    "y_hat_local_12m = np.zeros(n_test)\n",
    "\n",
    "vecm_fit = r['VECM'](data_vecm, 12)\n",
    "data_vecm_sim = pd.DataFrame(scale(r['VECM.boot'](vecm_fit)))\n",
    "\n",
    "for i in range(0,n_test+1):\n",
    "    print(i)\n",
    "    data_vecm_window = np.array(data_vecm_sim.iloc[i:(i+n_train),:])\n",
    "    try:\n",
    "        vecm_fit = r['VECM'](data_vecm_window, 12)\n",
    "        aux = stats.predict(object = vecm_fit, **{'n.ahead':12})[:,0]\n",
    "        y_fit_local[(i+13):(i+n_train)] = r['fitted'](vecm_fit)[:,0]\n",
    "        if i <= n_test-1:\n",
    "            y_hat_local[i] = aux[0]\n",
    "            y_hat_local_3m[i] = np.mean(aux[0:2])\n",
    "            y_hat_local_6m[i] = np.mean(aux[0:5])\n",
    "            y_hat_local_12m[i] = np.mean(aux[0:11])\n",
    "    except:\n",
    "        if i <= n_test-1:\n",
    "            y_hat_local[i] = y_hat_local[i-1]\n",
    "            y_hat_local_3m[i] = y_hat_local_3m[i-1]\n",
    "            y_hat_local_6m[i] = y_hat_local_6m[i-1]\n",
    "            y_hat_local_12m[i] = y_hat_local_12m[i-1]\n",
    "\n",
    "y_fit_local = np.array(y_fit_local[0:n_train])\n",
    "y_fit_local = pd.DataFrame(y_fit_local, columns = ['Train'])\n",
    "y_hat_local = np.array(y_hat_local)\n",
    "y_hat_local = pd.DataFrame(y_hat_local, columns = ['Test'])\n",
    "y_hat_local_3m = np.array(y_hat_local_3m[:-2])\n",
    "y_hat_local_3m = pd.DataFrame(y_hat_local_3m, columns = ['Test'])\n",
    "y_hat_local_6m = np.array(y_hat_local_6m[:-5])\n",
    "y_hat_local_6m = pd.DataFrame(y_hat_local_6m, columns = ['Test'])\n",
    "y_hat_local_12m = np.array(y_hat_local_12m[:-11])\n",
    "y_hat_local_12m = pd.DataFrame(y_hat_local_12m, columns = ['Test'])\n",
    "y_true = np.array(data_vecm_sim.iloc[n_train:(n_train+n_test),0])\n",
    "y_true = pd.DataFrame(y_true, columns = ['Observed'])\n",
    "\n",
    "sns.lineplot(data=pd.concat([y_true, y_hat_local],axis=1))\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    print(p)\n",
    "    \n",
    "    if p == '3m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(3).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(3).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_3m)\n",
    "        \n",
    "    elif p == '6m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(6).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(6).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_6m)\n",
    "        \n",
    "    elif p == '12m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(12).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(12).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_12m)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if p == 'p0':\n",
    "            y_local = y_true\n",
    "            y_fit = y_fit_local\n",
    "            y_hat = y_hat_local\n",
    "        else:\n",
    "            y_local = y_true[shift:]\n",
    "            y_fit = y_fit_local[:-shift]\n",
    "            y_hat = y_hat_local[:-shift]\n",
    "    \n",
    "    y_local = np.array(y_local)[:,0]\n",
    "    y_fit = np.array(y_fit)[:,0]\n",
    "    y_hat = np.array(y_hat)[:,0]\n",
    "    \n",
    "    df_y_fit_vecm[p][(-n_train+shift):] = y_fit\n",
    "    df_y_hat_vecm[p][(-n_test+shift):] = y_hat\n",
    "    \n",
    "    df_MSE.loc[str_model, p] = MSE(y_local, y_hat)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_local, y_hat)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_local, y_hat)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_local, y_hat)\n",
    "    df_CS.loc[str_model, p] = cos_sim2(y_local, y_hat)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# SETAR                         #\n",
    "#################################\n",
    "\n",
    "str_model = 'SETAR'\n",
    "\n",
    "data_setar = y\n",
    "\n",
    "importr('tsDyn')\n",
    "stats = importr('stats')\n",
    "\n",
    "n_train = len(index_train)\n",
    "n_test = len(index_test)\n",
    "y_fit_local = np.zeros(n_train+n_test)\n",
    "y_hat_local = np.zeros(n_test)\n",
    "y_hat_local_3m = np.zeros(n_test)\n",
    "y_hat_local_6m = np.zeros(n_test)\n",
    "y_hat_local_12m = np.zeros(n_test)\n",
    "\n",
    "setar_fit = r['setar'](x = np.array(data_setar), m = 2, d = 1, nthresh = 1, thDelay = 1, th = 0)\n",
    "data_setar_sim = r['setar.sim'](setarObject = setar_fit, type = 'boot')\n",
    "data_setar_sim = pd.DataFrame(scale(data_setar_sim.rx2('serie')))\n",
    "        \n",
    "for i in range(0,n_test+1):\n",
    "    print(i)\n",
    "    data_setar_window = np.array(data_setar_sim.iloc[i:(i+n_train),:])\n",
    "    try:\n",
    "        setar_fit = r['setar'](x = data_setar_window, m = 2, d = 1, nthresh = 1, thDelay = 1, th = 0)\n",
    "        aux = stats.predict(object = setar_fit, **{'n.ahead':12})\n",
    "        fitted_values = r['fitted.values'](setar_fit)\n",
    "        fitted_values = fitted_values[~np.isnan(fitted_values)]\n",
    "        y_fit_local[(i+2):(i+n_train)] = fitted_values\n",
    "        if i <= n_test-1:\n",
    "            y_hat_local[i] = aux[0]\n",
    "            y_hat_local_3m[i] = np.mean(aux[0:2])\n",
    "            y_hat_local_6m[i] = np.mean(aux[0:5])\n",
    "            y_hat_local_12m[i] = np.mean(aux[0:11])\n",
    "    except:\n",
    "        if i <= n_test-1:\n",
    "            y_hat_local[i] = y_hat_local[i-1]\n",
    "            y_hat_local_3m[i] = y_hat_local_3m[i-1]\n",
    "            y_hat_local_6m[i] = y_hat_local_6m[i-1]\n",
    "            y_hat_local_12m[i] = y_hat_local_12m[i-1]\n",
    "\n",
    "y_fit_local = np.array(y_fit_local[0:n_train])\n",
    "y_fit_local = pd.DataFrame(y_fit_local, columns = ['Train'])\n",
    "y_hat_local = np.array(y_hat_local)\n",
    "y_hat_local = pd.DataFrame(y_hat_local, columns = ['Test'])\n",
    "y_hat_local_3m = np.array(y_hat_local_3m[:-2])\n",
    "y_hat_local_3m = pd.DataFrame(y_hat_local_3m, columns = ['Test'])\n",
    "y_hat_local_6m = np.array(y_hat_local_6m[:-5])\n",
    "y_hat_local_6m = pd.DataFrame(y_hat_local_6m, columns = ['Test'])\n",
    "y_hat_local_12m = np.array(y_hat_local_12m[:-11])\n",
    "y_hat_local_12m = pd.DataFrame(y_hat_local_12m, columns = ['Test'])\n",
    "y_true = np.array(data_setar_sim[n_train:(n_train+n_test)])\n",
    "y_true = pd.DataFrame(y_true, columns = ['Observed'])\n",
    "\n",
    "sns.lineplot(data=pd.concat([y_true, y_hat_local],axis=1))\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    print(p)\n",
    "    \n",
    "    if p == '3m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(3).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(3).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_3m)\n",
    "        \n",
    "    elif p == '6m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(6).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(6).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_6m)\n",
    "        \n",
    "    elif p == '12m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(12).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(12).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_12m)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if p == 'p0':\n",
    "            y_local = y_true\n",
    "            y_fit = y_fit_local\n",
    "            y_hat = y_hat_local\n",
    "        else:\n",
    "            y_local = y_true[shift:]\n",
    "            y_fit = y_fit_local[:-shift]\n",
    "            y_hat = y_hat_local[:-shift]\n",
    "    \n",
    "    y_local = np.array(y_local)[:,0]\n",
    "    y_fit = np.array(y_fit)[:,0]\n",
    "    y_hat = np.array(y_hat)[:,0]\n",
    "    \n",
    "    df_y_fit_setar[p][(-n_train+shift):] = y_fit\n",
    "    df_y_hat_setar[p][(-n_test+shift):] = y_hat\n",
    "    \n",
    "    df_MSE.loc[str_model, p] = MSE(y_local, y_hat)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_local, y_hat)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_local, y_hat)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_local, y_hat)\n",
    "    df_CS.loc[str_model, p] = cos_sim2(y_local, y_hat)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# AO (2001) Moving Average      #\n",
    "#################################\n",
    "\n",
    "# Model\n",
    "str_model = 'MA'\n",
    "\n",
    "X_ma = y.rolling(12).mean().dropna()\n",
    "y_ma = y\n",
    "y_ma_3m = y.rolling(3).mean().dropna()\n",
    "y_ma_6m = y.rolling(6).mean().dropna()\n",
    "y_ma_12m = y.rolling(12).mean().dropna()\n",
    "y_ma = y_ma.loc[X_ma.index]\n",
    "y_ma_3m = y_ma_3m.loc[X_ma.index]\n",
    "y_ma_6m = y_ma_6m.loc[X_ma.index]\n",
    "y_ma_12m = y_ma_12m.loc[X_ma.index]\n",
    "X_ma = np.array(X_ma)[:,0]\n",
    "y_ma = np.array(y_ma)[:,0]\n",
    "y_ma_3m = np.array(y_ma_3m)[:,0]\n",
    "y_ma_6m = np.array(y_ma_6m)[:,0]\n",
    "y_ma_12m = np.array(y_ma_12m)[:,0]\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    # Moving Average Model\n",
    "    \n",
    "    if shift == 0:\n",
    "        y_fit_local = X_ma\n",
    "    else:\n",
    "        y_fit_local = X_ma[:-shift]\n",
    "    \n",
    "    sns.lineplot(data = [y_ma[shift:], y_fit_local])\n",
    "    plt.legend(['Observed', 'Fitted'], loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # df_y_fit_ma[p][(-len(y_fit_local)):] = y_fit_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_ma[shift:], y_fit_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_ma[shift:], y_fit_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_ma[shift:], y_fit_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_ma[shift:], y_fit_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_ma[shift:], y_fit_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# SARIMA                        #\n",
    "#################################\n",
    "\n",
    "# Model\n",
    "str_model = 'SARIMA'\n",
    "\n",
    "importr('stats')\n",
    "importr('forecast')\n",
    "\n",
    "data_arima = y\n",
    "\n",
    "n_train = len(index_train)\n",
    "n_test = len(index_test)\n",
    "y_fit_local = np.zeros(n_train+n_test)\n",
    "y_hat_local = np.zeros(n_test)\n",
    "y_hat_local_3m = np.zeros(n_test)\n",
    "y_hat_local_6m = np.zeros(n_test)\n",
    "y_hat_local_12m = np.zeros(n_test)\n",
    "\n",
    "arima_fit = r['arima'](x = data_arima, order = FloatVector([1,0,1]))\n",
    "data_arima_sim = pd.DataFrame(r['simulate'](object = arima_fit))\n",
    "\n",
    "for i in range(0,n_test+1):\n",
    "    print(i)\n",
    "    data_arima_window = np.array(data_arima_sim.iloc[i:(i+n_train),:])\n",
    "    try:\n",
    "        arima_fit = r['arima'](x = data_arima_window, order = FloatVector([1,0,1]))\n",
    "        aux = r['predict'](arima_fit, **{'n.ahead':13})\n",
    "        aux = aux.rx2('pred')\n",
    "        fitted_values = r['fitted'](arima_fit)[:,0]\n",
    "        fitted_values = fitted_values[~np.isnan(fitted_values)]\n",
    "        y_fit_local[i:(i+n_train)] = fitted_values\n",
    "        if i <= n_test-1:\n",
    "            y_hat_local[i] = aux[0]\n",
    "            y_hat_local_3m[i] = np.mean(aux[0:2])\n",
    "            y_hat_local_6m[i] = np.mean(aux[0:5])\n",
    "            y_hat_local_12m[i] = np.mean(aux[0:11])\n",
    "    except:\n",
    "        if i <= n_test-1:\n",
    "            y_hat_local[i] = y_hat_local[i-1]\n",
    "            y_hat_local_3m[i] = y_hat_local_3m[i-1]\n",
    "            y_hat_local_6m[i] = y_hat_local_6m[i-1]\n",
    "            y_hat_local_12m[i] = y_hat_local_12m[i-1]\n",
    "\n",
    "y_fit_local = np.array(y_fit_local[0:n_train])\n",
    "y_fit_local = pd.DataFrame(y_fit_local, columns = ['Train'])\n",
    "y_hat_local = np.array(y_hat_local)\n",
    "y_hat_local = pd.DataFrame(y_hat_local, columns = ['Test'])\n",
    "y_hat_local_3m = np.array(y_hat_local_3m[:-2])\n",
    "y_hat_local_3m = pd.DataFrame(y_hat_local_3m, columns = ['Test'])\n",
    "y_hat_local_6m = np.array(y_hat_local_6m[:-5])\n",
    "y_hat_local_6m = pd.DataFrame(y_hat_local_6m, columns = ['Test'])\n",
    "y_hat_local_12m = np.array(y_hat_local_12m[:-11])\n",
    "y_hat_local_12m = pd.DataFrame(y_hat_local_12m, columns = ['Test'])\n",
    "y_true = np.array(data_arima_sim[n_train:(n_train+n_test)])\n",
    "y_true = pd.DataFrame(y_true, columns = ['Observed'])\n",
    "\n",
    "sns.lineplot(data=pd.concat([y_true, y_hat_local],axis=1))\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    print(p)\n",
    "    \n",
    "    if p == '3m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(3).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(3).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_3m)\n",
    "        \n",
    "    elif p == '6m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(6).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(6).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_6m)\n",
    "        \n",
    "    elif p == '12m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(12).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(12).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_12m)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if p == 'p0':\n",
    "            y_local = y_true\n",
    "            y_fit = y_fit_local\n",
    "            y_hat = y_hat_local\n",
    "        else:\n",
    "            y_local = y_true[shift:]\n",
    "            y_fit = y_fit_local[:-shift]\n",
    "            y_hat = y_hat_local[:-shift]\n",
    "    \n",
    "    y_local = np.array(y_local)[:,0]\n",
    "    y_fit = np.array(y_fit)[:,0]\n",
    "    y_hat = np.array(y_hat)[:,0]\n",
    "    \n",
    "    df_y_fit_sarima[p][(-n_train+shift):] = y_fit\n",
    "    df_y_hat_sarima[p][(-n_test+shift):] = y_hat\n",
    "    \n",
    "    df_MSE.loc[str_model, p] = MSE(y_local, y_hat)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_local, y_hat)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_local, y_hat)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_local, y_hat)\n",
    "    df_CS.loc[str_model, p] = cos_sim2(y_local, y_hat)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# ARFIMA                        #\n",
    "#################################\n",
    "\n",
    "# Model\n",
    "str_model = 'ARFIMA'\n",
    "\n",
    "importr('stats')\n",
    "importr('forecast')\n",
    "\n",
    "data_arfima = y\n",
    "\n",
    "n_train = len(index_train)\n",
    "n_test = len(index_test)\n",
    "y_fit_local = np.zeros(n_train+n_test)\n",
    "y_hat_local = np.zeros(n_test)\n",
    "y_hat_local_3m = np.zeros(n_test)\n",
    "y_hat_local_6m = np.zeros(n_test)\n",
    "y_hat_local_12m = np.zeros(n_test)\n",
    "\n",
    "arfima_fit = r['arfima'](FloatVector(np.array(data_arfima)))\n",
    "data_arfima_sim = pd.DataFrame(r['simulate'](object = arfima_fit))\n",
    "\n",
    "for i in range(0,n_test+1):\n",
    "    print(i)\n",
    "    data_arfima_window = np.array(data_arfima_sim.iloc[i:(i+n_train),:])\n",
    "    try:\n",
    "        arfima_fit = r['arfima'](FloatVector(data_arfima_window))\n",
    "        aux = r['forecast'](arfima_fit, h=12)\n",
    "        aux = aux.rx2('mean')\n",
    "        fitted_values = arfima_fit.rx2('fitted')\n",
    "        fitted_values = fitted_values[~np.isnan(fitted_values)]\n",
    "        y_fit_local[i:(i+n_train)] = fitted_values\n",
    "        if i <= n_test-1:\n",
    "            y_hat_local[i] = aux[0]\n",
    "            y_hat_local_3m[i] = np.mean(aux[0:2])\n",
    "            y_hat_local_6m[i] = np.mean(aux[0:5])\n",
    "            y_hat_local_12m[i] = np.mean(aux[0:11])\n",
    "    except:\n",
    "        if i <= n_test-1:\n",
    "            y_hat_local[i] = y_hat_local[i-1]\n",
    "            y_hat_local_3m[i] = y_hat_local_3m[i-1]\n",
    "            y_hat_local_6m[i] = y_hat_local_6m[i-1]\n",
    "            y_hat_local_12m[i] = y_hat_local_12m[i-1]\n",
    "\n",
    "y_fit_local = np.array(y_fit_local[0:n_train])\n",
    "y_fit_local = pd.DataFrame(y_fit_local, columns = ['Train'])\n",
    "y_hat_local = np.array(y_hat_local)\n",
    "y_hat_local = pd.DataFrame(y_hat_local, columns = ['Test'])\n",
    "y_hat_local_3m = np.array(y_hat_local_3m[:-2])\n",
    "y_hat_local_3m = pd.DataFrame(y_hat_local_3m, columns = ['Test'])\n",
    "y_hat_local_6m = np.array(y_hat_local_6m[:-5])\n",
    "y_hat_local_6m = pd.DataFrame(y_hat_local_6m, columns = ['Test'])\n",
    "y_hat_local_12m = np.array(y_hat_local_12m[:-11])\n",
    "y_hat_local_12m = pd.DataFrame(y_hat_local_12m, columns = ['Test'])\n",
    "y_true = np.array(data_arfima_sim[n_train:(n_train+n_test)])\n",
    "y_true = pd.DataFrame(y_true, columns = ['Observed'])\n",
    "\n",
    "sns.lineplot(data=pd.concat([y_true, y_hat_local],axis=1))\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "    \n",
    "    print(p)\n",
    "    \n",
    "    if p == '3m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(3).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(3).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_3m)\n",
    "        \n",
    "    elif p == '6m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(6).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(6).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_6m)\n",
    "        \n",
    "    elif p == '12m':\n",
    "        \n",
    "        y_local = scale(y_true.rolling(12).mean().dropna())\n",
    "        y_fit = scale(y_fit_local.rolling(12).mean().dropna())\n",
    "        y_hat = scale(y_hat_local_12m)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if p == 'p0':\n",
    "            y_local = y_true\n",
    "            y_fit = y_fit_local\n",
    "            y_hat = y_hat_local\n",
    "        else:\n",
    "            y_local = y_true[shift:]\n",
    "            y_fit = y_fit_local[:-shift]\n",
    "            y_hat = y_hat_local[:-shift]\n",
    "    \n",
    "    y_local = np.array(y_local)[:,0]\n",
    "    y_fit = np.array(y_fit)[:,0]\n",
    "    y_hat = np.array(y_hat)[:,0]\n",
    "    \n",
    "    df_y_fit_arfima[p][(-n_train+shift):] = y_fit\n",
    "    df_y_hat_arfima[p][(-n_test+shift):] = y_hat\n",
    "    \n",
    "    df_MSE.loc[str_model, p] = MSE(y_local, y_hat)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_local, y_hat)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_local, y_hat)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_local, y_hat)\n",
    "    df_CS.loc[str_model, p] = cos_sim2(y_local, y_hat)\n",
    "    \n",
    "    k = k + 1\n",
    "   \n",
    "#################################\n",
    "# Gradient Boosting             #\n",
    "################################# \n",
    "\n",
    "str_model = 'GradBoost'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    gradboost_model = GradientBoostingRegressor(random_state = rnd_state)\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_lasso_train_3m\n",
    "        y_test_local = y_lasso_test_3m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_lasso_train_6m\n",
    "        y_test_local = y_lasso_test_6m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_lasso_train_12m\n",
    "        y_test_local = y_lasso_test_12m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_lasso_train\n",
    "            X_test_local = X_lasso_test\n",
    "            y_train_local = y_lasso_train\n",
    "            y_test_local = y_lasso_test\n",
    "        else:\n",
    "            X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "            X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "            y_train_local = y_lasso_train.iloc[shift:]\n",
    "            y_test_local = y_lasso_test.iloc[shift:]\n",
    "    \n",
    "    gradboost_model_fit = gradboost_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = gradboost_model.predict(X_train_local)\n",
    "    y_hat_local = gradboost_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('GradBoost Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('GradBoost Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_gradboost[p][(-len(y_lasso_train)+shift):] = y_fit_local\n",
    "    df_y_hat_gradboost[p][(-len(y_lasso_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# AdaBoost                      #\n",
    "#################################\n",
    "   \n",
    "str_model = 'AdaBoost'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    adaboost_model = AdaBoostRegressor(random_state = rnd_state)\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_lasso_train_3m\n",
    "        y_test_local = y_lasso_test_3m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_lasso_train_6m\n",
    "        y_test_local = y_lasso_test_6m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_lasso_train_12m\n",
    "        y_test_local = y_lasso_test_12m\n",
    "        X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "        X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_lasso_train\n",
    "            X_test_local = X_lasso_test\n",
    "            y_train_local = y_lasso_train\n",
    "            y_test_local = y_lasso_test\n",
    "        else:\n",
    "            X_train_local = X_lasso_train.iloc[:-shift,:]\n",
    "            X_test_local = X_lasso_test.iloc[:-shift,:]\n",
    "            y_train_local = y_lasso_train.iloc[shift:]\n",
    "            y_test_local = y_lasso_test.iloc[shift:]\n",
    "    \n",
    "    adaboost_model_fit = adaboost_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = adaboost_model.predict(X_train_local)\n",
    "    y_hat_local = adaboost_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('GradBoost Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('GradBoost Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_adaboost[p][(-len(y_lasso_train)+shift):] = y_fit_local\n",
    "    df_y_hat_adaboost[p][(-len(y_lasso_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#################################\n",
    "# Bayesian ARDR                 #\n",
    "#################################\n",
    "\n",
    "str_model = 'BayesRegression'\n",
    "\n",
    "k = 0\n",
    "for p in col_names:\n",
    "    \n",
    "    shift = periods[k]\n",
    "\n",
    "    bayesreg_model = ARDRegression()\n",
    "    \n",
    "    if p == '3m':\n",
    "        y_train_local = y_rf_train_3m\n",
    "        y_test_local = y_rf_test_3m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "        \n",
    "    elif p == '6m':\n",
    "        y_train_local = y_rf_train_6m\n",
    "        y_test_local = y_rf_test_6m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "\n",
    "    elif p == '12m':\n",
    "        y_train_local = y_rf_train_12m\n",
    "        y_test_local = y_rf_test_12m\n",
    "        X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "        X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "    \n",
    "    else:\n",
    "        if shift == 0:\n",
    "            X_train_local = X_rf_train\n",
    "            X_test_local = X_rf_test\n",
    "            y_train_local = y_rf_train\n",
    "            y_test_local = y_rf_test\n",
    "        else:\n",
    "            X_train_local = X_rf_train.iloc[:-shift,:]\n",
    "            X_test_local = X_rf_test.iloc[:-shift,:]\n",
    "            y_train_local = y_rf_train.iloc[shift:]\n",
    "            y_test_local = y_rf_test.iloc[shift:]\n",
    "    \n",
    "    bayesreg_model_fit = bayesreg_model.fit(X_train_local, y_train_local)\n",
    "    y_fit_local = bayesreg_model.predict(X_train_local)\n",
    "    y_hat_local = bayesreg_model.predict(X_test_local)    \n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_train_local)),\n",
    "                                 pd.Series(np.array(y_fit_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Bayes Regression - Training Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=pd.concat([pd.Series(np.array(y_test_local)),\n",
    "                                 pd.Series(np.array(y_hat_local))], \n",
    "                                 axis = 1))\n",
    "    plt.legend(['Observed', 'Fitted'])\n",
    "    plt.title('Bayes Regression - Test Sample')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    df_y_fit_bayesreg[p][(-len(y_rf_train)+shift):] = y_fit_local\n",
    "    df_y_hat_bayesreg[p][(-len(y_rf_test)+shift):] = y_hat_local\n",
    "        \n",
    "    df_MSE.loc[str_model, p] = MSE(y_test_local, y_hat_local)\n",
    "    df_MAE.loc[str_model, p] = MAE(y_test_local, y_hat_local)\n",
    "    df_RMSE.loc[str_model, p] = RMSE(y_test_local, y_hat_local)\n",
    "    df_MAPE.loc[str_model, p] = MAPE(y_test_local, y_hat_local)\n",
    "    df_CS.loc[str_model, p] = cos_sim(y_test_local, y_hat_local)\n",
    "    \n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Out-of-Sample Performance Comparison                                                                              #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# Saves matrices\n",
    "\n",
    "df_MSE.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' MSE.csv')\n",
    "df_MAE.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' MAE.csv')\n",
    "df_RMSE.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' RMSE.csv')\n",
    "df_MAPE.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' MAPE.csv')\n",
    "df_CS.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' CS.csv')\n",
    "\n",
    "df_y_fit_lstm_m1.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_lstm_m1.csv')\n",
    "df_y_hat_lstm_m1.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_lstm_m1.csv')\n",
    "\n",
    "df_y_fit_lstm_m2.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_lstm_m2.csv')\n",
    "df_y_hat_lstm_m2.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_lstm_m2.csv')\n",
    "\n",
    "df_y_fit_lstm_m3.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_lstm_m3.csv')\n",
    "df_y_hat_lstm_m3.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_lstm_m3.csv')\n",
    "\n",
    "df_y_fit_lstm_m4.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_lstm_m4.csv')\n",
    "df_y_hat_lstm_m4.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_lstm_m4.csv')\n",
    "\n",
    "df_y_fit_lstm_m5.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_lstm_m5.csv')\n",
    "df_y_hat_lstm_m5.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_lstm_m5.csv')\n",
    "\n",
    "df_y_fit_lstm_m6.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_lstm_m6.csv')\n",
    "df_y_hat_lstm_m6.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_lstm_m6.csv')\n",
    "\n",
    "df_y_fit_mlp.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_mlp.csv')\n",
    "df_y_hat_mlp.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_mlp.csv')\n",
    "\n",
    "df_y_fit_mlp2.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_mlp2.csv')\n",
    "df_y_hat_mlp2.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_mlp2.csv')\n",
    "\n",
    "df_y_fit_rw.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_rw.csv')\n",
    "df_y_hat_rw.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_rw.csv')\n",
    "\n",
    "df_y_fit_ridge.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_ridge.csv')\n",
    "df_y_hat_ridge.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_ridge.csv')\n",
    "\n",
    "df_y_fit_ridge_cv.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_ridge_cv.csv')\n",
    "df_y_hat_ridge_cv.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_ridge_cv.csv')\n",
    "\n",
    "df_y_fit_bridge.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_bridge.csv')\n",
    "df_y_hat_bridge.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_bridge.csv')\n",
    "\n",
    "df_y_fit_lasso.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_lasso.csv')\n",
    "df_y_hat_lasso.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_lasso.csv')\n",
    "\n",
    "df_y_fit_lasso_cv.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_lasso_cv.csv')\n",
    "df_y_hat_lasso_cv.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_lasso_cv.csv')\n",
    "\n",
    "df_y_fit_blasso.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_blasso.csv')\n",
    "df_y_hat_blasso.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_blasso.csv')\n",
    "\n",
    "df_y_fit_enet.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_enet.csv')\n",
    "df_y_hat_enet.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_enet.csv')\n",
    "\n",
    "df_y_fit_svr.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_svr.csv')\n",
    "df_y_hat_svr.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_svr.csv')\n",
    "\n",
    "df_y_fit_rf.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_rf.csv')\n",
    "df_y_hat_rf.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_rf.csv')\n",
    "\n",
    "df_y_fit_bart.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_bart.csv')\n",
    "df_y_hat_bart.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_bart.csv')\n",
    "\n",
    "df_y_fit_bagging.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_bagging.csv')\n",
    "df_y_hat_bagging.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_bagging.csv')\n",
    "\n",
    "df_y_fit_ma.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_ma.csv')\n",
    "df_y_hat_ma.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_ma.csv')\n",
    "\n",
    "df_y_fit_sarima.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_sarima.csv')\n",
    "df_y_hat_sarima.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_sarima.csv')\n",
    "\n",
    "df_y_fit_huber.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_huber.csv')\n",
    "df_y_hat_huber.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_huber.csv')\n",
    "\n",
    "df_y_fit_ts.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_ts.csv')\n",
    "df_y_hat_ts.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_ts.csv')\n",
    "\n",
    "df_y_fit_factor.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_factor.csv')\n",
    "df_y_hat_factor.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_factor.csv')\n",
    "\n",
    "df_y_fit_gradboost.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_gradboost.csv')\n",
    "df_y_hat_gradboost.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_gradboost.csv')\n",
    "\n",
    "df_y_fit_adaboost.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_adaboost.csv')\n",
    "df_y_hat_adaboost.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_adaboost.csv')\n",
    "\n",
    "df_y_fit_arfima.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_arfima.csv')\n",
    "df_y_hat_arfima.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_arfima.csv')\n",
    "\n",
    "df_y_fit_garch.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_garch.csv')\n",
    "df_y_hat_garch.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_garch.csv')\n",
    "\n",
    "df_y_fit_setar.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_setar.csv')\n",
    "df_y_hat_setar.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_setar.csv')\n",
    "\n",
    "df_y_fit_bayesreg.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_bayesreg.csv')\n",
    "df_y_hat_bayesreg.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_bayesreg.csv')\n",
    "\n",
    "df_y_fit_knn.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_fit_knn.csv')\n",
    "df_y_hat_knn.to_csv(path_or_buf = str_Dir_Results + str(rnd_state) + ' y_hat_knn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
